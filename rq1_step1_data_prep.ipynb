{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1 Data Preparation\n",
    "\n",
    "[Add description]\n",
    "\n",
    "Steps:\n",
    "1. **Manually** download all datasets (5 forest definitions and Natura 2000) \n",
    "2. Filter Natura 2000 for areas in Germany\n",
    "3. Mosaic data which comes in tiles (Hansen & JAXA)\n",
    "4. Threshold & update (Hansen)\n",
    "5. Extract layer from netcdf (ESA)\n",
    "6. Reproject to the most common projection - EPSG 3035\n",
    "7. Rasterise or Upsample (WITHOUT INTERPOLATION) to 5m \n",
    "8. Convert all datasets to FNF\n",
    "9. ~~Clip data to Germany~~ (SKIPPED FOR NOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup\n",
    "\n",
    "Used this for help with directory setup: \n",
    "https://www.freecodecamp.org/news/creating-a-directory-in-python-how-to-create-a-folder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ./rawdata already exists\n",
      "Folder ./processing already exists\n",
      "Folder ./outputs already exists\n"
     ]
    }
   ],
   "source": [
    "# 0: SETUP\n",
    "\n",
    "# Import packages\n",
    "import os\n",
    "import warnings\n",
    "import glob\n",
    "import math\n",
    "import subprocess\n",
    "\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.crs import CRS \n",
    "import xarray as xr \n",
    "import rioxarray as rio\n",
    "\n",
    "#from osgeo import gdal \n",
    "\n",
    "# Create required directories if they don't already exist\n",
    "# Note: these directories are ignored in git\n",
    "path_list = (\"./rawdata\", \"./processing\", \"./outputs\")\n",
    "\n",
    "for path in path_list:\n",
    "  if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print(\"Folder %s created!\" % path)\n",
    "  else:\n",
    "    print(\"Folder %s already exists\" % path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Manually download datasets\n",
    "\n",
    "As several of the datasets require login credentials and are not available through an API, I decided to manually download all the required data. I have stored everything in the \"rawdata\" folder. This folder is set to be ignored by git because the files are too big to push onto the GitHub repo.\n",
    "\n",
    "**So for the first step: manually download all datasets using the notes below and save to the \"rawdata\" folder.** \n",
    "\n",
    "Note: For the forest definiton layers I have downloaded the 2018 datasets as this is the most recent data available across all datasets. \n",
    "\n",
    "**1. UMD (Hansen) / Global Forest Watch**\n",
    "- Download from: https://storage.googleapis.com/earthenginepartners-hansen/GFC-2023-v1.11/download.html\n",
    "    - Using the map interface, download the treecover2000, gain & lossyear layers for the 4 granules with top-left corner at: (60N, 0E), (60N, 10E), (50N, 0E) and (50N, 10E). \n",
    "    - The files will be used in combination with each other to generate a dataset that corresponds (roughly) to forest cover in 2018.\n",
    "- My info:\n",
    "    - Download date: 15 Jan 2025\n",
    "    - File: rawdata/Hansen_GFC-2023-v1.11 (folder contains 12 tifs - 4 each for cover, gain and loss)\n",
    "\n",
    "**2. ESA Land Cover**\n",
    "- Download from: https://cds.climate.copernicus.eu/datasets/satellite-land-cover?tab=download \n",
    "    - Login credentials required (there is a prompt in the page to sign up/login).\n",
    "    - Select 2018 map and v2.1.1.\n",
    "    - Only download the sub-region for ~Germany bounding box (N:56, W:1, E:16, S:46).\n",
    "- My Info:\n",
    "    - Download date: 14 Jan 2025\n",
    "    - File: rawdata/C3S-LC-L4-LCCS-Map-300m-P1Y-2018-v2.1.1.area-subset.56.1.46.16.nc\n",
    "\n",
    "**3. JAXA FNF** \n",
    "- Download from: https://www.eorc.jaxa.jp/ALOS/en/palsar_fnf/data/index.htm\n",
    "    - Login credentials required (To register, go here: https://www.eorc.jaxa.jp/ALOS/en/palsar_fnf/registration.htm).\n",
    "    - Under the heading \"PALSAR/PALSAR-2 mosaic and forest/non-forest (FNF) map\", select the 2018 data.\n",
    "    - Use the map interface to click through until you can download tiles. I opted to download four 5 x 5 tiles using the link above the map (for N55E005, N55E010, N50E005, N50E010), but also had to supplement with some individual tiles. I used QGIS to sort through the tiles and figure out which ones were needed. In total, 73 tiles are needed for the Germany Natura areas - a list of the required tile names is available in: other/jaxa_tile_list.txt\n",
    "- My Info:\n",
    "    - Download date: 15 Jan 2025\n",
    "    - File: rawdata/jaxa_2018_fnf_ger (folder contains 73 tifs)  \n",
    "\n",
    "**4. CORINE Land Use** \n",
    "- Download from: https://land.copernicus.eu/en/products/corine-land-cover/clc2018#download\n",
    "    - Login credentials required (there is a prompt in the page to sign up/login).\n",
    "    - Click on “Go to download by area”. Then select the CORINE land cover 2018 layer, use the area selection tool to click on Germany and then click on the download icon beside the layer name. \n",
    "    - From the cart, select the dataset and chose \"vector\" and \"shapefile\". I opted for vector so that I can rasterise at a common resolution that makes sense with the other data. Click the \"Process Download Request\" button.\n",
    "    - NOTE: At this point the download request enters a queue which can take a long time. When it is ready to download, an email will be sent so you don't have to keep checking it.\n",
    "- My Info:\n",
    "    - Download date: 16 Jan 2025 (request date - ready for download on 18 Jan 2025)\n",
    "    - File: U2018_CLC2018_V2020_20u1.zip  (contains: 1 shp & its components)\n",
    "\n",
    "**5. German Land Use**\n",
    "- Download from: https://gdz.bkg.bund.de/index.php/default/corine-land-cover-5-ha-stand-2018-clc5-2018.html  \n",
    "    - Click on the “Direktdownload” tab, and then click on \"Georeferenzierung: UTM32s, Format: Shape (ZIP, 1,24 GB)\". This will download 5 shapefiles which represent the 5 main land cover classes (also used in CORINE) - individual features within these files have their more precise class as an attribute. Class 3 contains the classes related to forests and natural features, but Class 4 (marshlands, etc) is also required for producing the FAO map. So these 2 are retained for now and Class 1 (urban), Class 2 (agriculture) and Class 5 (water) are discarded.\n",
    "- My Info:\n",
    "    - Download date: 14 Jan 2025\n",
    "    - Files: rawdata/clc5_class3xx.zip, rawdata/clc5_class4xx.zip (each contains: 1 shp & its components)\n",
    "\n",
    "**6. Natura 2000 protected areas**\n",
    "- Download from: https://www.eea.europa.eu/en/datahub/datahubitem-view/6fc8ad2d-195d-40f4-bdec-576e7d1268e4\n",
    "    - Download the most recent date available (in my case: 2022 - direct link: https://sdi.eea.europa.eu/data/95e717d4-81dc-415d-a8f0-fecdf7e686b0).\n",
    "- My Info:\n",
    "    - Download date: 15 Jan 2025\n",
    "    - File: rawdata/Natura2000_end2022_epsg3035.zip (contains: 1 shp & its components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Filter Natura 2000 \n",
    "\n",
    "Use the attributes of the Natura shapefile to filter the \"MS\" field (i.e. \"Member States\") to only include \"DE\" (i.e. Germany). \n",
    "\n",
    "I also save the results as a shapefile in the outputs folder as this maybe be useful for visualisations at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\core.py:35: RuntimeWarning: Could not detect GDAL data files.  Set GDAL_DATA environment variable to the correct path.\n",
      "  _init_gdal_data()\n"
     ]
    }
   ],
   "source": [
    "# 2: FILTER NATURA2000\n",
    "\n",
    "# Load the Natura 2000 shapefile as a geodataframe\n",
    "# You can do this directly from the zipped file\n",
    "natura_gdf = gpd.read_file(\"./rawdata/Natura2000_end2022_epsg3035.zip\")\n",
    "\n",
    "#print(natura_gdf[1:20])\n",
    "\n",
    "# Extract only the German areas\n",
    "natura_de_gdf = natura_gdf.loc[natura_gdf[\"MS\"] == \"DE\"]\n",
    "\n",
    "# Check - there should be 5200 areas\n",
    "natura_de_gdf.count()\n",
    "\n",
    "# Save the file to outputs folder (turned off warnings which is about a datetime column)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    natura_de_gdf.to_file('./outputs/natura2000_3035_DE.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Mosaic tiled data\n",
    "\n",
    "JAXA and Hansen\n",
    "\n",
    "Help with mosaicing using rasterio: https://automating-gis-processes.github.io/CSC18/lessons/L6/raster-mosaic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: MOSAIC TILES\n",
    "\n",
    "# Store paths for tiles in list\n",
    "jaxa_paths = glob.glob('./rawdata/jaxa_2018_fnf_ger/*.tif')\n",
    "hansen_cover_paths = glob.glob('./rawdata/Hansen_GFC-2023-v1.11/Hansen_GFC-2023-v1.11_tree*.tif')\n",
    "hansen_gain_paths = glob.glob('./rawdata/Hansen_GFC-2023-v1.11/Hansen_GFC-2023-v1.11_gain*.tif')\n",
    "hansen_loss_paths = glob.glob('./rawdata/Hansen_GFC-2023-v1.11/Hansen_GFC-2023-v1.11_loss*.tif')\n",
    "\n",
    "# Store the path for the output mosaics\n",
    "jaxa_mosaic = \"./processing/jaxa_FNF_4326_DE.tif\"\n",
    "hansen_cover_mosaic = \"./processing/hansen_treecover2000_4326_DE.tif\"\n",
    "hansen_gain_mosaic = \"./processing/hansen_gain_4326_DE.tif\"\n",
    "hansen_loss_mosaic = \"./processing/hansen_lossyear_4326_DE.tif\"\n",
    "\n",
    "# Create a function which mosaics the tiles\n",
    "def mosaic_rasters(input_paths, output_path):\n",
    "    # Create an empty list to store the opened tiles\n",
    "    tiles_to_mosaic = []\n",
    "    # Iterate through the tile paths to open them and store the opened tiles a list\n",
    "    for path in input_paths:\n",
    "        tile = rasterio.open(path)\n",
    "        tiles_to_mosaic.append(tile)\n",
    "    # Create the mosaic and store the transform information\n",
    "    mosaic, transform = merge(tiles_to_mosaic)\n",
    "    # Copy the metadata for the mosaic from a tile\n",
    "    mosaic_meta = tile.meta.copy()\n",
    "    # Update the metadata with the new information for the mosaic\n",
    "    mosaic_meta.update({\"driver\": \"GTiff\",\n",
    "                        \"height\": mosaic.shape[1],\n",
    "                        \"width\": mosaic.shape[2],\n",
    "                        \"transform\": transform,\n",
    "                        # crs is not included, as it is copied from the tiles\n",
    "                        }\n",
    "                        )\n",
    "        # Write the mosaic with its metata to a tif file\n",
    "    with rasterio.open(output_path, \"w\", **mosaic_meta, compress=\"LZW\") as dest:\n",
    "        dest.write(mosaic)\n",
    "\n",
    "# Run the function to mosaic the tiles\n",
    "# If mosaic already exists, make sure it's not open in QGIS :) you'll get permission error if so!\n",
    "mosaic_rasters(jaxa_paths, jaxa_mosaic)\n",
    "mosaic_rasters(hansen_cover_paths, hansen_cover_mosaic)\n",
    "mosaic_rasters(hansen_gain_paths, hansen_gain_mosaic)\n",
    "mosaic_rasters(hansen_loss_paths, hansen_loss_mosaic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Threshold & update for gain/loss \n",
    "\n",
    "Hansen data only\n",
    "\n",
    ">60% cover - provides a good range with the other datasets (which are lower), and it's also the threshold used by the International Geosphere-Biosphere Programme (IGBP) definition\n",
    "\n",
    "reclassify 1-100 values so that 61-100 are forest, everything else is non-forest\n",
    "\n",
    "reclassify loss so that 1-18 has a value of 1, everything else is 0\n",
    "\n",
    "subtract/add loss and gain from treecover layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Extract layer from netcdf\n",
    "\n",
    "This is required for the ESA dataset only. The netcdf file includes several different layers of information - the one I want to use is called \"lccs_class\". The aim here is to simply extract the single layer and save it as a geotiff.\n",
    "\n",
    "Help with saving netcdf layer as geotiff: https://help.marine.copernicus.eu/en/articles/5029956-how-to-convert-netcdf-to-geotiff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: EXTRACT & CONVERT NETCDF DATA\n",
    "\n",
    "# Open the ESA netcdf for conversion\n",
    "esa_netcdf = xr.open_dataset(\"./rawdata/C3S-LC-L4-LCCS-Map-300m-P1Y-2018-v2.1.1.area-subset.56.1.46.16.nc\", engine=\"netcdf4\")\n",
    "#esa_netcdf\n",
    "\n",
    "# Extract the lccs_class variable\n",
    "esa_lccs_class = esa_netcdf['lccs_class']\n",
    "\n",
    "# Provide spatial axis & define the CRS\n",
    "esa_lccs_class = esa_lccs_class.rio.set_spatial_dims(x_dim='lon', y_dim='lat')\n",
    "esa_lccs_class.rio.crs\n",
    "esa_lccs_class.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "# Save the geotiff\n",
    "esa_lccs_class.rio.to_raster(\"./processing/esa_lccs_class_4326_DE.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Reproject\n",
    "\n",
    "The data needed to be projected to work in units of meters for calculating area. Three datasets are not in a projected CRS (they are WGS 1984 / EPSG:4326). The most common projection is ETRS89-extended / LAEA Europe (EPSG: 3035) which is used by the Natura 2000 areas and the CORINE dataset. \n",
    "\n",
    "So for this step, all datasets which are not already in this projection, will be (re)projected to 3035.\n",
    "\n",
    "Help with reprojecting using rioxarray: https://www.earthdatascience.org/courses/use-data-open-source-python/intro-raster-data-python/raster-data-processing/reproject-raster/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: REPROJECT RASTERS\n",
    "\n",
    "# A quick function for reprojecting individual rasters to EPSG:3035\n",
    "def reproject_raster_3035(input_path, output_path):\n",
    "    # Open the raster (NOTE: need to use rio.open_rasterio() here!)\n",
    "    input = rio.open_rasterio(input_path)\n",
    "    # Run the reprojection\n",
    "    output = input.rio.reproject(\"EPSG:3035\")\n",
    "    # Write the reprojected output raster\n",
    "    output.rio.to_raster(output_path, compress = \"LZW\")\n",
    "\n",
    "# Run this per file \n",
    "reproject_raster_3035(\"./processing/jaxa_FNF_4326_DE.tif\", \"./processing/jaxa_FNF_3035_DE.tif\")\n",
    "reproject_raster_3035(\"./processing/esa_lccs_class_4326_DE.tif\", \"./processing/esa_lccs_class_3035_DE.tif\")\n",
    "\n",
    "# TO DO: HANSEN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: REPROJECT SHPS\n",
    "\n",
    "# Store paths for shp zips in list\n",
    "ger_lulc_paths = glob.glob('./rawdata/clc5_class*.zip')\n",
    "\n",
    "# Create a function which reprojects the shp to 3035 (and saves to processing folder)\n",
    "def reproj_shp_3035(input_paths):\n",
    "    # Iterate through the shp paths \n",
    "    for path in input_paths:\n",
    "        # Open the shp for each path (excludes extra cols as they cause problems & are not needed)\n",
    "        shp = gpd.read_file(path, columns = [\"CLC18\"])\n",
    "        \n",
    "        # Reprojects to 3035\n",
    "        shp_3035  = shp.to_crs(\"EPSG:3035\")\n",
    "\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(path)[1] \n",
    "        # For output file naming: remove extension from input file name \n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # For output file naming: create the new name for reprojected shp\n",
    "        new_name = name_wo_ext + \"_3035_DE.shp\"\n",
    "\n",
    "        # Write the reprojected shp to the processing folder\n",
    "        shp_3035.to_file('./processing/' + new_name)\n",
    "\n",
    "# Run the function for the German LULC zipped shps\n",
    "reproj_shp_3035(ger_lulc_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: CLEAN UP\n",
    "\n",
    "# Create a list of the data paths for deletion (files with \"4326\" in their name)\n",
    "old_data =  glob.glob('./processing/*4326*')\n",
    "\n",
    "# Create a function which deletes the input paths\n",
    "def clean_up(input_paths):\n",
    "    for path in input_paths:\n",
    "        # Check that the paths exist\n",
    "        if os.path.exists(path):  \n",
    "           os.remove(path)\n",
    "        else:\n",
    "            print(\"Nothing to clean!\") \n",
    "\n",
    "# Run the function to remove any data with \"4326\" in the file name\n",
    "clean_up(old_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Rasterise or Upsample\n",
    "\n",
    "In this step, I rasterise the vector files (German LULC - Class 3 only & CORINE) and upsample the already exisiting rasters to 5m. \n",
    "\n",
    "5m was selected as is the commonly divisible unit across all datasets; so all pixels can be approximately divided by 5, meaning there is as little transformation as possible. It also means that a lot of the detail of the shapefiles can be retained during rasterisation. \n",
    "\n",
    "Importantly, the upsampling needs to happen WITHOUT INTERPOLATION so that no \"new\" information is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\core.py:35: RuntimeWarning: Could not detect GDAL data files.  Set GDAL_DATA environment variable to the correct path.\n",
      "  _init_gdal_data()\n"
     ]
    }
   ],
   "source": [
    "# 7: RASTERISE VECTORS (WITH ATTRIBUTE VALUE)\n",
    "# This is only required for GER LULC Class3 and CORINE\n",
    "\n",
    "# First some prep is needed for GER LULC Class 3\n",
    "# Read in shp\n",
    "ger_lulc_class3xx = gpd.read_file(\"./processing/clc5_class3xx_3035_DE.shp\")\n",
    "\n",
    "# Convert the CLC18 column to integer (stored as string in original file)\n",
    "ger_lulc_class3xx['CLC18'] = ger_lulc_class3xx['CLC18'].astype('int')\n",
    "\n",
    "# Save the output\n",
    "ger_lulc_class3xx.to_file('./processing/clc5_class3xx_3035_DE_int.shp')\n",
    "\n",
    "# And we need to unzip the CORINE data\n",
    "# Read in shp, Code_18 column only \n",
    "corine_shp = gpd.read_file(\"./rawdata/U2018_CLC2018_V2020_20u1.zip\", columns = [\"Code_18\"])\n",
    "\n",
    "# Save the output\n",
    "corine_shp.to_file('./processing/U2018_CLC2018_V2020_DE.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Input file size is 127980, 173478\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7: RASTERISE VECTORS (WITH ATTRIBUTE VALUE) - CONTINUED\n",
    "\n",
    "# TAKES ABOUT 35 MIN TO RUN\n",
    "# Runs batch script which runs gdal_rasterize for GER LULC Class 3 & CORINE - outputs 5m tifs\n",
    "rasterise_5m = subprocess.run([\"rq1_step1_sub1_rasterise.bat\"], \n",
    "                                    capture_output=True, \n",
    "                                    text=True)\n",
    "\n",
    "print(rasterise_5m.stdout)\n",
    "print(rasterise_5m.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file size is 4942, 4884\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Input file size is 42602, 51782\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7: UPSAMPLE RASTERS \n",
    "\n",
    "# TAKES ABOUT 30 MIN TO RUN\n",
    "# Runs batch script which runs gdal_translate to resample rasters (ESA & JAXA) to 5m - outputs tifs\n",
    "upsample_5m = subprocess.run([\"rq1_step1_sub2_upsample.bat\"], \n",
    "                             capture_output=True, \n",
    "                             text=True)\n",
    "\n",
    "print(upsample_5m.stdout)\n",
    "print(upsample_5m.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Convert all datasets to FNF\n",
    "\n",
    "To prepare the datasets for turning into a presence/absence consensus map, and also to prepare the JAXA and GER LULC maps for use in the workflow to create the FAO-aligned map, they need to be converted to Forest-Nonforest maps - i.e. maps where there are only two classes, 0 = Nonforest and 1 = Forest.\n",
    "\n",
    "As each dataset has its own set of classes, this conversion needs to be customisted for each map.\n",
    "\n",
    "Help with reclassifying (use gdal_calc): https://gis.stackexchange.com/questions/245170/reclassifying-raster-using-gdal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8.1: JAXA Reclassify\n",
    "\n",
    "For JAXA, the reclassification to true FNF is as follows:\n",
    "\n",
    "| Original Value | Original Label               | New Value | New Label  |\n",
    "| -------------- | ---------------------------- | --------- | ---------- |\n",
    "| 0              | NoData                       | -9999     | NoData     |\n",
    "| 1              | Forest (>90% canopy cover)   | 1         | Forest     |\n",
    "| 2              | Forest (10-90% canopy cover) | 1         | Forest     |\n",
    "| 3              | Non-Forest                   | 0         | Non-Forest |\n",
    "| 4              | Water                        | 0         | Non-Forest |\n",
    "\n",
    "Both forest categories are converted to forest here as this fits with the FAO canopy cover thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8.1: RECLASSIFY JAXA \n",
    "\n",
    "# Store path to 5m Jaxa dataset as the intput file\n",
    "jaxa_input = \"./processing/jaxa_FNF_3035_DE_5m.tif\"\n",
    "\n",
    "# Store the path to where gdal_calc.py is (for some reason this is different than where the .exe scripts are)\n",
    "gdal_calc = \"./thesis_env_conda/Lib/site-packages/GDAL-3.10.1-py3.12-win-amd64.egg-info/scripts/gdal_calc.py\"\n",
    "\n",
    "# TAKES ABOUT 30 MIN TO RUN\n",
    "# Runs gdal_calc.py in order to reclassify JAXA 5m raster \n",
    "reclass_jaxa = subprocess.run(['python', gdal_calc, '-A', jaxa_input, '--outfile=./processing/jaxa_FNF_3035_DE_5m_reclass.tif', '--calc=-9999*(A==0)+1*(A==1)+1*(A==2)+0*(A>=3)', '--co=COMPRESS=LZW', '--co=BIGTIFF=YES', '--NoDataValue=-9999'],\n",
    "                              capture_output=True, \n",
    "                              text=True)\n",
    "\n",
    "print(reclass_jaxa.stdout)\n",
    "print(reclass_jaxa.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8.2: CORINE Reclassify\n",
    "\n",
    "For CORINE, the reclassification to FNF is as follows:\n",
    "\n",
    "| Original Value | Original Label                 | New Value | New Label  |\n",
    "| -------------- | ------------------------------ | --------- | ---------- |\n",
    "| 0              | NoData                         | -9999     | NoData     |\n",
    "| 1xx            | Urban classes                  | 0         | Non-Forest |\n",
    "| 2xx            | Agricultural classes           | 0         | Non-Forest |\n",
    "| 311            | Broad-leaved forest            | 1         | Forest     |\n",
    "| 312            | Coniferous forest              | 1         | Forest     |\n",
    "| 313            | Mixed forest                   | 1         | Forest     |\n",
    "| 321            | Natural grasslands             | 0         | Non-Forest |\n",
    "| 322            | Moors and heathland            | 0         | Non-Forest |\n",
    "| 323            | Sclerophyllous vegetation      | 1         | Forest     |\n",
    "| 324            | Transitional woodland-shrub    | 1         | Forest     |\n",
    "| 331            | Beaches - dunes - sands        | 0         | Non-Forest |\n",
    "| 332            | Bare rocks                     | 0         | Non-Forest |\n",
    "| 333            | Sparsely vegetated areas       | 0         | Non-Forest |\n",
    "| 334            | Burnt areas                    | 0         | Non-Forest |\n",
    "| 335            | Glaciers and perpetual snow    | 0         | Non-Forest |\n",
    "| 4xx            | Marsh, bog, intertidal classes | 0         | Non-Forest |\n",
    "| 5xx            | Water body classes             | 0         | Non-Forest |\n",
    "| >=600          | NoData                         | -9999     | NoData     |\n",
    "\n",
    "More simply: classes <311 = Non-Forest; classes 311-324 = Forest, classes >325 = Non-Forest.\n",
    "\n",
    "This is based on *Natura 2000 and forests. Part I-II* (European Commission, 2015) which describes how forest area calculations were performed with data from CORINE with \"CLC classes grouped as forests: 311 Broad-leaf forests; 312 Coniferous forests; 313 Mixed forests; 323 Sclerophyllous vegetation; 324 Transitional woodland-shrub.\" \n",
    "\n",
    "** Note the 323 class (Sclerophyllous vegetation) is not present in the Germany dataset.\n",
    "\n",
    "Whether class 324 (Transitional woodland-shrub) is included is a bit uncertain. In the definition above it is included, and in the *State of nature in the EU...* report (EEA, 2020) forest area tends to be reported as \"Forests and transitional woodland shrubbodies\" - so they are also sort of grouped together. \n",
    "\n",
    "For now, I will include class 324 in the definition of forest for CORINE. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8.2: RECLASSIFY CORINE\n",
    "\n",
    "# Store path to 5m Corine dataset as the intput file\n",
    "corine_input = \"./processing/U2018_CLC2018_V2020_3035_DE_5m.tif\"\n",
    "\n",
    "# Store the path to where gdal_calc.py is\n",
    "gdal_calc = \"./thesis_env_conda/Lib/site-packages/GDAL-3.10.1-py3.12-win-amd64.egg-info/scripts/gdal_calc.py\"\n",
    "\n",
    "# TAKES ABOUT 10 MIN TO RUN\n",
    "# Runs gdal_calc.py in order to reclassify CORINE 5m raster \n",
    "reclass_corine = subprocess.run(['python', gdal_calc, '-A', corine_input, '--outfile=./processing/U2018_CLC2018_V2020_3035_DE_5m_reclass.tif', '--calc=-9999*(A==0)+0*(A<=310)+1*((A>=311)*(A<=324))+0*((A>=325)*(A<=599))+-9999*(A>=600)', '--co=COMPRESS=LZW', '--co=BIGTIFF=YES', '--NoDataValue=-9999'],\n",
    "                              capture_output=True, \n",
    "                              text=True)\n",
    "\n",
    "print(reclass_corine.stdout)\n",
    "print(reclass_corine.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8.3: GER LULC Class 3 Reclassify\n",
    "\n",
    "The GER LULC class conversion is essentially the same as the CORINE one; I am applying the same definition of forest to both, but this dataset is a different operationalisation of that definition (and also how they apply the classes appears to be different?). Because the different main classes (1xx, 2xx, 3xx, 4xx and 5xx) come in separate shapefiles, the reclassification is simplier since I'm only dealing with class 3 for forests. \n",
    "\n",
    "For GER LULC Class 3, the reclassification to FNF is as follows:\n",
    "\n",
    "| Original Value | Original Label                 | New Value | New Label  |\n",
    "| -------------- | ------------------------------ | --------- | ---------- |\n",
    "| 0              | NoData                         | 0         | Non-Forest |\n",
    "| 311            | Broad-leaved forest            | 1         | Forest     |\n",
    "| 312            | Coniferous forest              | 1         | Forest     |\n",
    "| 313            | Mixed forest                   | 1         | Forest     |\n",
    "| 321            | Natural grasslands             | 0         | Non-Forest |\n",
    "| 322            | Moors and heathland            | 0         | Non-Forest |\n",
    "| 323            | Sclerophyllous vegetation      | 1         | Forest     |\n",
    "| 324            | Transitional woodland-shrub    | 1         | Forest     |\n",
    "| 331            | Beaches - dunes - sands        | 0         | Non-Forest |\n",
    "| 332            | Bare rocks                     | 0         | Non-Forest |\n",
    "| 333            | Sparsely vegetated areas       | 0         | Non-Forest |\n",
    "| 334            | Burnt areas                    | 0         | Non-Forest |\n",
    "| 335            | Glaciers and perpetual snow    | 0         | Non-Forest |\n",
    "| >=336          | NoData                         | -9999     | NoData     |\n",
    "\n",
    "More simply: classes <310 = Non-Forest; classes 311-324 = Forest, classes >325 = Non-Forest.\n",
    "\n",
    "See the CORINE reclassification for more explantion on the class conversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8.3: RECLASSIFY GER LULC Class 3\n",
    "\n",
    "# Store path to 5m GER LULC Class 3 as the intput file\n",
    "ger_lulc3_input = \"./processing/clc5_class3xx_3035_DE_5m.tif\"\n",
    "\n",
    "# Store the path to where gdal_calc.py is\n",
    "gdal_calc = \"./thesis_env_conda/Lib/site-packages/GDAL-3.10.1-py3.12-win-amd64.egg-info/scripts/gdal_calc.py\"\n",
    "\n",
    "# TAKES ABOUT 15 MIN TO RUN\n",
    "# Runs gdal_calc.py in order to reclassify GER LULC Class 3 5m raster \n",
    "reclass_ger_lulc3 = subprocess.run(['python', gdal_calc, '-A', ger_lulc3_input, '--outfile=./processing/clc5_class3xx_3035_DE_5m_reclass.tif', '--calc=0*(A<=310)+1*((A>=311)*(A<=324))+0*((A>=325)*(A<=335))+-9999*(A>=336)', '--co=COMPRESS=LZW', '--co=BIGTIFF=YES', '--NoDataValue=-9999'],\n",
    "                              capture_output=True, \n",
    "                              text=True)\n",
    "\n",
    "print(reclass_ger_lulc3.stdout)\n",
    "print(reclass_ger_lulc3.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8.4: ESA Reclassify\n",
    "\n",
    "For ESA, the reclassification to FNF is as follows:\n",
    "\n",
    "| Original Value          | Original Label                 | New Value | New Label  |\n",
    "| ----------------------- | ------------------------------ | --------- | ---------- |\n",
    "| 0                       | NoData                         | -9999     | NoData     |\n",
    "| 10, 11, 12, 20, 30, 40  | Agriculture classes            | 0         | Non-Forest |\n",
    "| 50                      | Broadleaf evergreen            | 1         | Forest     |\n",
    "| 60, 61, 62              | Broadleaf deciduous            | 1         | Forest     |\n",
    "| 70, 71, 72              | Needleleaf evergreen           | 1         | Forest     |\n",
    "| 80, 81, 82              | Needleleaf deciduous           | 1         | Forest     |\n",
    "| 90                      | Mixed (broad & needle leaf)    | 1         | Forest     |\n",
    "| 100                     | Mosaic tree & shrub / herb.    | 1         | Forest     |\n",
    "| 110                     | Mosaic herb. / tree & shrub    | 0         | Non-Forest |\n",
    "| 120, 121, 122           | Shrubland                      | 0         | Non-Forest |\n",
    "| 130                     | Grassland                      | 0         | Non-Forest |\n",
    "| 140, 150, 151, 152, 153 | Sparse vegetation              | 0         | Non-Forest |\n",
    "| 160, 170                | Tree cover, flooded            | 1         | Forest     |\n",
    "| 180                     | Wetland                        | 0         | Non-Forest |\n",
    "| 190                     | Urban                          | 0         | Non-Forest |\n",
    "| 200, 201, 202           | Bare Areas                     | 0         | Non-Forest |\n",
    "| 210, 220                | Water / Permanent Snow & Ice   | 0         | Non-Forest |\n",
    "\n",
    "\n",
    "This is based on how the producers of the data align their ESA classes with the IPCC land categories - see page 30 of the *Land Cover CCI Product User Guide*.\n",
    "\n",
    "Note that the classes 160, 170 (which correspond to mangroves and are included as forests) are not present in the Germany dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8.4: RECLASSIFY ESA\n",
    "\n",
    "# Store path to 5m ESA as the intput file\n",
    "esa_input = \"./processing/esa_lccs_class_3035_DE_5m.tif\"\n",
    "\n",
    "# Store the path to where gdal_calc.py is \n",
    "gdal_calc = \"./thesis_env_conda/Lib/site-packages/GDAL-3.10.1-py3.12-win-amd64.egg-info/scripts/gdal_calc.py\"\n",
    "\n",
    "# TAKES ABOUT 60 MIN TO RUN \n",
    "# Runs gdal_calc.py in order to reclassify ESA 5m raster \n",
    "reclass_esa = subprocess.run(['python', gdal_calc, '-A', esa_input, '--outfile=./processing/esa_lccs_class_3035_DE_5m_reclass.tif', '--calc=-9999*(A==0)+0*((A>=10)*(A<=40))+1*((A>=50)*(A<=100))+0*((A>=110)*(A<=153))+1*((A>=160)*(A<=170))+0*(A>=180)', '--co=COMPRESS=LZW', '--co=BIGTIFF=YES', '--NoDataValue=-9999'],\n",
    "                              capture_output=True, \n",
    "                              text=True)\n",
    "\n",
    "print(reclass_esa.stdout)\n",
    "print(reclass_esa.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8.5: Hansen Reclassify\n",
    "\n",
    "This might not be needed (depending on if I use Hansen and also whether I already end up with a FNF for Hansen from Step 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Clip to Germany\n",
    "\n",
    "All the rasters created so far can now be clipped to the area of interest. I originally intended to clip the rasters to the German Natura2000 areas, however this proved to be too slow / create insanely large outputs . I have therefore adjusted my plans to clip to Germany instead - just to eliminate unneeded data from the larger mosaiced rasters and to ensure that there's a common extent.\n",
    "\n",
    "Note that I decided to clip to the footprint of the CORINE data. This does mean that some data on the edges are lost from the GER LULC output. However, it was relatively little data and it makes sense to clip to a common region so that I am comparing the same areas across maps. \n",
    "\n",
    "\n",
    "IMPORTANT: This now ouputs vrts for each input dataset, BUT they are very large and I haven't been able to view them properly in QGIS. For now I will NOT work with these clipped versions and plan to try the following at later stages:\n",
    "- use zonal statistics to extract pixel counts (which can then be summed and multiplied to convert to m2) for the Natura 2000 areas\n",
    "- if I want to have some outputs for Germany as a whole, I could try clipping with the clipper.shp (generated in the batch script called below) at a later stage - i.e. when the maps have been converted to FNF, and maybe therefore be a bit smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9: CLIP RASTERS\n",
    "\n",
    "# Runs batch script which runs gdalwarp for all vrt files ending in \"_5m\" in the processing folder - outputs a vrt for each\n",
    "clip_to_DE = subprocess.run([\"rq1_step1_sub3_clip.bat\"], \n",
    "                             capture_output=True, \n",
    "                             text=True)\n",
    "\n",
    "print(clip_to_DE.stdout)\n",
    "print(clip_to_DE.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
