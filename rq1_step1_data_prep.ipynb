{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1 Data Preparation\n",
    "\n",
    "[Add description]\n",
    "\n",
    "Steps:\n",
    "1. **Manually** download all datasets (5 forest definitions and Natura 2000) \n",
    "2. Filter Natura 2000 for areas in Germany\n",
    "3. Mosaic data which comes in tiles (Hansen & JAXA)\n",
    "4. Threshold & update (Hansen)\n",
    "5. Extract layer from netcdf (ESA)\n",
    "6. Reproject to the most common projection - EPSG 3035\n",
    "7. Rasterise or Upsample (WITHOUT INTERPOLATION) to 5m \n",
    "8. ~~Clip data to Germany~~ (SKIPPED FOR NOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup\n",
    "\n",
    "Used this for help with directory setup: \n",
    "https://www.freecodecamp.org/news/creating-a-directory-in-python-how-to-create-a-folder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ./rawdata already exists\n",
      "Folder ./processing already exists\n",
      "Folder ./outputs already exists\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "\n",
    "# Import packages\n",
    "import os\n",
    "import warnings\n",
    "import glob\n",
    "import math\n",
    "import subprocess\n",
    "\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.crs import CRS \n",
    "import xarray as xr \n",
    "import rioxarray as rio\n",
    "\n",
    "#from osgeo import gdal \n",
    "\n",
    "# Create required directories if they don't already exist\n",
    "# Note: these directories are ignored in git\n",
    "path_list = (\"./rawdata\", \"./processing\", \"./outputs\")\n",
    "\n",
    "for path in path_list:\n",
    "  if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print(\"Folder %s created!\" % path)\n",
    "  else:\n",
    "    print(\"Folder %s already exists\" % path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Manually download datasets\n",
    "\n",
    "As several of the datasets require login credentials and are not available through an API, I decided to manually download all the required data. I have stored everything in the \"rawdata\" folder. This folder is set to be ignored by git because the files are too big to push onto the GitHub repo.\n",
    "\n",
    "**So for the first step: manually download all datasets using the notes below and save to the \"rawdata\" folder.** \n",
    "\n",
    "Note: For the forest definiton layers I have downloaded the 2018 datasets as this is the most recent data available across all datasets. \n",
    "\n",
    "**1. UMD (Hansen) / Global Forest Watch**\n",
    "- Download from: https://storage.googleapis.com/earthenginepartners-hansen/GFC-2023-v1.11/download.html\n",
    "    - Using the map interface, download the treecover2000, gain & lossyear layers for the 4 granules with top-left corner at: (60N, 0E), (60N, 10E), (50N, 0E) and (50N, 10E). \n",
    "    - The files will be used in combination with each other to generate a dataset that corresponds (roughly) to forest cover in 2018.\n",
    "- My info:\n",
    "    - Download date: 15 Jan 2025\n",
    "    - File: rawdata/Hansen_GFC-2023-v1.11 (folder contains 12 tifs - 4 each for cover, gain and loss)\n",
    "\n",
    "**2. ESA Land Cover**\n",
    "- Download from: https://cds.climate.copernicus.eu/datasets/satellite-land-cover?tab=download \n",
    "    - Login credentials required (there is a prompt in the page to sign up/login).\n",
    "    - Select 2018 map and v2.1.1.\n",
    "    - Only download the sub-region for ~Germany bounding box (N:56, W:1, E:16, S:46).\n",
    "- My Info:\n",
    "    - Download date: 14 Jan 2025\n",
    "    - File: rawdata/C3S-LC-L4-LCCS-Map-300m-P1Y-2018-v2.1.1.area-subset.56.1.46.16.nc\n",
    "\n",
    "**3. JAXA FNF** \n",
    "- Download from: https://www.eorc.jaxa.jp/ALOS/en/palsar_fnf/data/index.htm\n",
    "    - Login credentials required (To register, go here: https://www.eorc.jaxa.jp/ALOS/en/palsar_fnf/registration.htm).\n",
    "    - Under the heading \"PALSAR/PALSAR-2 mosaic and forest/non-forest (FNF) map\", select the 2018 data.\n",
    "    - Use the map interface to click through until you can download tiles. I opted to download four 5 x 5 tiles using the link above the map (for N55E005, N55E010, N50E005, N50E010), but also had to supplement with some individual tiles. I used QGIS to sort through the tiles and figure out which ones were needed. In total, 73 tiles are needed for the Germany Natura areas - a list of the required tile names is available in: other/jaxa_tile_list.txt\n",
    "- My Info:\n",
    "    - Download date: 15 Jan 2025\n",
    "    - File: rawdata/jaxa_2018_fnf_ger (folder contains 73 tifs)  \n",
    "\n",
    "**4. CORINE Land Use** \n",
    "- Download from: https://land.copernicus.eu/en/products/corine-land-cover/clc2018#download\n",
    "    - Login credentials required (there is a prompt in the page to sign up/login).\n",
    "    - Click on “Go to download by area”. Then select the CORINE land cover 2018 layer, use the area selection tool to click on Germany and then click on the download icon beside the layer name. \n",
    "    - From the cart, select the dataset and chose \"vector\" and \"shapefile\". I opted for vector so that I can rasterise at a common resolution that makes sense with the other data. Click the \"Process Download Request\" button.\n",
    "    - NOTE: At this point the download request enters a queue which can take a long time. When it is ready to download, an email will be sent so you don't have to keep checking it.\n",
    "- My Info:\n",
    "    - Download date: 16 Jan 2025 (request date - ready for download on 18 Jan 2025)\n",
    "    - File: U2018_CLC2018_V2020_20u1.zip  (contains: 1 shp & its components)\n",
    "\n",
    "**5. German Land Use**\n",
    "- Download from: https://gdz.bkg.bund.de/index.php/default/corine-land-cover-5-ha-stand-2018-clc5-2018.html  \n",
    "    - Click on the “Direktdownload” tab, and then click on \"Georeferenzierung: UTM32s, Format: Shape (ZIP, 1,24 GB)\". This will download 5 shapefiles which represent the 5 main land cover classes (also used in CORINE) - individual features within these files have their more precise class as an attribute. Class 3 contains the classes related to forests, but other classes may be required for producing the FAO map (so all 5 are retained for now).\n",
    "- My Info:\n",
    "    - Download date: 14 Jan 2025\n",
    "    - Files: rawdata/clc5_class1xx.zip, rawdata/clc5_class2xx.zip, rawdata/clc5_class3xx.zip, rawdata/clc5_class4xx.zip, rawdata/clc5_class5xx.zip (each contains: 1 shp & its components)\n",
    "\n",
    "**6. Natura 2000 protected areas**\n",
    "- Download from: https://www.eea.europa.eu/en/datahub/datahubitem-view/6fc8ad2d-195d-40f4-bdec-576e7d1268e4\n",
    "    - Download the most recent date available (in my case: 2022 - direct link: https://sdi.eea.europa.eu/data/95e717d4-81dc-415d-a8f0-fecdf7e686b0).\n",
    "- My Info:\n",
    "    - Download date: 15 Jan 2025\n",
    "    - File: rawdata/Natura2000_end2022_epsg3035.zip (contains: 1 shp & its components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Filter Natura 2000 \n",
    "\n",
    "Use the attributes of the Natura shapefile to filter the \"MS\" field (i.e. \"Member States\") to only include \"DE\" (i.e. Germany). \n",
    "\n",
    "I also save the results as a shapefile in the outputs folder as this maybe be useful for visualisations at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\core.py:35: RuntimeWarning: Could not detect GDAL data files.  Set GDAL_DATA environment variable to the correct path.\n",
      "  _init_gdal_data()\n"
     ]
    }
   ],
   "source": [
    "# FILTER NATURA2000\n",
    "\n",
    "# Load the Natura 2000 shapefile as a geodataframe\n",
    "# You can do this directly from the zipped file\n",
    "natura_gdf = gpd.read_file(\"./rawdata/Natura2000_end2022_epsg3035.zip\")\n",
    "\n",
    "#print(natura_gdf[1:20])\n",
    "\n",
    "# Extract only the German areas\n",
    "natura_de_gdf = natura_gdf.loc[natura_gdf[\"MS\"] == \"DE\"]\n",
    "\n",
    "# Check - there should be 5200 areas\n",
    "natura_de_gdf.count()\n",
    "\n",
    "# Save the file to outputs folder (turned off warnings which is about a datetime column)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    natura_de_gdf.to_file('./outputs/natura2000_3035_DE.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Mosaic tiled data\n",
    "\n",
    "JAXA and Hansen\n",
    "\n",
    "Help with mosaicing using rasterio: https://automating-gis-processes.github.io/CSC18/lessons/L6/raster-mosaic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOSAIC TILES\n",
    "\n",
    "# Store paths for tiles in list\n",
    "jaxa_paths = glob.glob('./rawdata/jaxa_2018_fnf_ger/*.tif')\n",
    "hansen_cover_paths = glob.glob('./rawdata/Hansen_GFC-2023-v1.11/Hansen_GFC-2023-v1.11_tree*.tif')\n",
    "hansen_gain_paths = glob.glob('./rawdata/Hansen_GFC-2023-v1.11/Hansen_GFC-2023-v1.11_gain*.tif')\n",
    "hansen_loss_paths = glob.glob('./rawdata/Hansen_GFC-2023-v1.11/Hansen_GFC-2023-v1.11_loss*.tif')\n",
    "\n",
    "# Store the path for the output mosaics\n",
    "jaxa_mosaic = \"./processing/jaxa_FNF_4326_DE.tif\"\n",
    "hansen_cover_mosaic = \"./processing/hansen_treecover2000_4326_DE.tif\"\n",
    "hansen_gain_mosaic = \"./processing/hansen_gain_4326_DE.tif\"\n",
    "hansen_loss_mosaic = \"./processing/hansen_lossyear_4326_DE.tif\"\n",
    "\n",
    "# Create a function which mosaics the tiles\n",
    "def mosaic_rasters(input_paths, output_path):\n",
    "    # Create an empty list to store the opened tiles\n",
    "    tiles_to_mosaic = []\n",
    "    # Iterate through the tile paths to open them and store the opened tiles a list\n",
    "    for path in input_paths:\n",
    "        tile = rasterio.open(path)\n",
    "        tiles_to_mosaic.append(tile)\n",
    "    # Create the mosaic and store the transform information\n",
    "    mosaic, transform = merge(tiles_to_mosaic)\n",
    "    # Copy the metadata for the mosaic from a tile\n",
    "    mosaic_meta = tile.meta.copy()\n",
    "    # Update the metadata with the new information for the mosaic\n",
    "    mosaic_meta.update({\"driver\": \"GTiff\",\n",
    "                        \"height\": mosaic.shape[1],\n",
    "                        \"width\": mosaic.shape[2],\n",
    "                        \"transform\": transform,\n",
    "                        # crs is not included, as it is copied from the tiles\n",
    "                        }\n",
    "                        )\n",
    "        # Write the mosaic with its metata to a tif file\n",
    "    with rasterio.open(output_path, \"w\", **mosaic_meta, compress=\"LZW\") as dest:\n",
    "        dest.write(mosaic)\n",
    "\n",
    "# Run the function to mosaic the tiles\n",
    "# If mosaic already exists, make sure it's not open in QGIS :) you'll get permission error if so!\n",
    "mosaic_rasters(jaxa_paths, jaxa_mosaic)\n",
    "mosaic_rasters(hansen_cover_paths, hansen_cover_mosaic)\n",
    "mosaic_rasters(hansen_gain_paths, hansen_gain_mosaic)\n",
    "mosaic_rasters(hansen_loss_paths, hansen_loss_mosaic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Threshold & update for gain/loss \n",
    "\n",
    "Hansen data only\n",
    "\n",
    ">60% cover - provides a good range with the other datasets (which are lower), and it's also the threshold used by the International Geosphere-Biosphere Programme (IGBP) definition\n",
    "\n",
    "reclassify 1-100 values so that 61-100 are forest, everything else is non-forest\n",
    "\n",
    "reclassify loss so that 1-18 has a value of 1, everything else is 0\n",
    "\n",
    "subtract/add loss and gain from treecover layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Extract layer from netcdf\n",
    "\n",
    "This is required for the ESA dataset only. The netcdf file includes several different layers of information - the one I want to use is called \"lccs_class\". The aim here is to simply extract the single layer and save it as a geotiff.\n",
    "\n",
    "Help with saving netcdf layer as geotiff: https://help.marine.copernicus.eu/en/articles/5029956-how-to-convert-netcdf-to-geotiff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT & CONVERT NETCDF DATA\n",
    "\n",
    "# Open the ESA netcdf for conversion\n",
    "esa_netcdf = xr.open_dataset(\"./rawdata/C3S-LC-L4-LCCS-Map-300m-P1Y-2018-v2.1.1.area-subset.56.1.46.16.nc\", engine=\"netcdf4\")\n",
    "#esa_netcdf\n",
    "\n",
    "# Extract the lccs_class variable\n",
    "esa_lccs_class = esa_netcdf['lccs_class']\n",
    "\n",
    "# Provide spatial axis & define the CRS\n",
    "esa_lccs_class = esa_lccs_class.rio.set_spatial_dims(x_dim='lon', y_dim='lat')\n",
    "esa_lccs_class.rio.crs\n",
    "esa_lccs_class.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "# Save the geotiff\n",
    "esa_lccs_class.rio.to_raster(\"./processing/esa_lccs_class_4326_DE.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Reproject\n",
    "\n",
    "The data needed to be projected to work in units of meters for calculating area. Three datasets are not in a projected CRS (they are WGS 1984 / EPSG:4326). The most common projection is ETRS89-extended / LAEA Europe (EPSG: 3035) which is used by the Natura 2000 areas and the CORINE dataset. \n",
    "\n",
    "So for this step, all datasets which are not already in this projection, will be (re)projected to 3035.\n",
    "\n",
    "Help with reprojecting using rioxarray: https://www.earthdatascience.org/courses/use-data-open-source-python/intro-raster-data-python/raster-data-processing/reproject-raster/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPROJECT RASTERS\n",
    "\n",
    "# A quick function for reprojecting individual rasters to EPSG:3035\n",
    "def reproject_raster_3035(input_path, output_path):\n",
    "    # Open the raster (NOTE: need to use rio.open_rasterio() here!)\n",
    "    input = rio.open_rasterio(input_path)\n",
    "    # Run the reprojection\n",
    "    output = input.rio.reproject(\"EPSG:3035\")\n",
    "    # Write the reprojected output raster\n",
    "    output.rio.to_raster(output_path, compress = \"LZW\")\n",
    "\n",
    "# Run this per file \n",
    "reproject_raster_3035(\"./processing/jaxa_FNF_4326_DE.tif\", \"./processing/jaxa_FNF_3035_DE.tif\")\n",
    "reproject_raster_3035(\"./processing/esa_lccs_class_4326_DE.tif\", \"./processing/esa_lccs_class_3035_DE.tif\")\n",
    "\n",
    "# TO DO: HANSEN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPROJECT SHPS\n",
    "\n",
    "# Store paths for shp zips in list\n",
    "ger_lulc_paths = glob.glob('./rawdata/clc5_class*.zip')\n",
    "\n",
    "# Create a function which reprojects the shp to 3035 (and saves to processing folder)\n",
    "def reproj_shp_3035(input_paths):\n",
    "    # Iterate through the shp paths \n",
    "    for path in input_paths:\n",
    "        # Open the shp for each path (excludes extra cols as they cause problems & are not needed)\n",
    "        shp = gpd.read_file(path, columns = [\"CLC18\"])\n",
    "        \n",
    "        # Reprojects to 3035\n",
    "        shp_3035  = shp.to_crs(\"EPSG:3035\")\n",
    "\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(path)[1] \n",
    "        # For output file naming: remove extension from input file name \n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # For output file naming: create the new name for reprojected shp\n",
    "        new_name = name_wo_ext + \"_3035_DE.shp\"\n",
    "\n",
    "        # Write the reprojected shp to the processing folder\n",
    "        shp_3035.to_file('./processing/' + new_name)\n",
    "\n",
    "# Run the function for the German LULC zipped shps\n",
    "reproj_shp_3035(ger_lulc_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN UP\n",
    "\n",
    "# Create a list of the data paths for deletion (files with \"4326\" in their name)\n",
    "old_data =  glob.glob('./processing/*4326*')\n",
    "\n",
    "# Create a function which deletes the input paths\n",
    "def clean_up(input_paths):\n",
    "    for path in input_paths:\n",
    "        # Check that the paths exist\n",
    "        if os.path.exists(path):  \n",
    "           os.remove(path)\n",
    "        else:\n",
    "            print(\"Nothing to clean!\") \n",
    "\n",
    "# Run the function to remove any data with \"4326\" in the file name\n",
    "clean_up(old_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Rasterise or Upsample\n",
    "\n",
    "In this step, I rasterise the vector files (German LULC - Class 3 only & CORINE) and upsample the already exisiting rasters to 5m. \n",
    "\n",
    "5m was selected as is the commonly divisible unit across all datasets; so all pixels can be approximately divided by 5, meaning there is as little transformation as possible. It also means that a lot of the detail of the shapefiles can be retained during rasterisation. \n",
    "\n",
    "Importantly, the upsampling needs to happen WITHOUT INTERPOLATION so that no \"new\" information is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\core.py:35: RuntimeWarning: Could not detect GDAL data files.  Set GDAL_DATA environment variable to the correct path.\n",
      "  _init_gdal_data()\n"
     ]
    }
   ],
   "source": [
    "# RASTERISE VECTORS (WITH ATTRIBUTE VALUE)\n",
    "# This is only required for GER LULC Class3 and CORINE\n",
    "\n",
    "# First some prep is needed for GER LULC Class 3\n",
    "# Read in shp\n",
    "ger_lulc_class3xx = gpd.read_file(\"./processing/clc5_class3xx_3035_DE.shp\")\n",
    "\n",
    "# Convert the CLC18 column to integer (stored as string in original file)\n",
    "ger_lulc_class3xx['CLC18'] = ger_lulc_class3xx['CLC18'].astype('int')\n",
    "\n",
    "# Save the output\n",
    "ger_lulc_class3xx.to_file('./processing/clc5_class3xx_3035_DE_int.shp')\n",
    "\n",
    "# And we need to unzip the CORINE data\n",
    "# Read in shp, Code_18 column only \n",
    "corine_shp = gpd.read_file(\"./rawdata/U2018_CLC2018_V2020_20u1.zip\", columns = [\"Code_18\"])\n",
    "\n",
    "# Save the output\n",
    "corine_shp.to_file('./processing/U2018_CLC2018_V2020_DE.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Input file size is 127980, 173478\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RASTERISE VECTORS (WITH ATTRIBUTE VALUE) - CONTINUED\n",
    "\n",
    "# TAKES ABOUT 35 MIN SO RUN WITH CAUTION! \n",
    "# Runs batch script which runs gdal_rasterize for GER LULC Class 3 & CORINE - outputs 5m tifs and vrts for each\n",
    "rasterise_5m = subprocess.run([\"rq1_step1_sub1_rasterise.bat\"], \n",
    "                                    capture_output=True, \n",
    "                                    text=True)\n",
    "\n",
    "print(rasterise_5m.stdout)\n",
    "print(rasterise_5m.stderr)\n",
    "\n",
    "# Use clean_up function (see \"# CLEAN UP\" chunk) to remove tifs - only vrts are kept\n",
    "## REMOVING FOR NOW: I think this actually breaks the vrt?\n",
    "#clean_up([\"./processing/clc5_class3xx_3035_DE_5m.tif\", \"./processing/U2018_CLC2018_V2020_3035_DE_5m.tif\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file size is 4942, 4884\n",
      "Input file size is 42602, 51782\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UPSAMPLE RASTERS \n",
    "\n",
    "# Runs batch script which runs gdal_translate to resample rasters (ESA & JAXA) to 5m (outputs vrt for each)\n",
    "upsample_5m = subprocess.run([\"rq1_step1_sub2_upsample.bat\"], \n",
    "                             capture_output=True, \n",
    "                             text=True)\n",
    "\n",
    "print(upsample_5m.stdout)\n",
    "print(upsample_5m.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Clip to Germany\n",
    "\n",
    "All the rasters created so far can now be clipped to the area of interest. I originally intended to clip the rasters to the German Natura2000 areas, however this proved to be too slow / create insanely large outputs. I have therefore adjusted my plans to clip to Germany instead - just to eliminate unneeded data from the larger mosaiced rasters and to ensure that there's a common extent.\n",
    "\n",
    "Note that I decided to clip to the footprint of the CORINE data. This does mean that some data on the edges are lost from the GER LULC output. However, it was relatively little data and it makes sense to clip to a common region so that I am comparing the same areas across maps. \n",
    "\n",
    "\n",
    "IMPORTANT: This now ouputs vrts for each input dataset, BUT they are very large and I haven't been able to view them properly in QGIS. For now I will NOT work with these clipped versions and plan to try the following at later stages:\n",
    "- use zonal statistics to extract pixel counts (which can then be summed and multiplied to convert to m2) for the Natura 2000 areas\n",
    "- if I want to have some outputs for Germany as a whole, I could try clipping with the clipper.shp (generated in the batch script called below) at a later stage - i.e. when the maps have been converted to FNF, and maybe therefore be a bit smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output file that is 128211P x 173468L.\n",
      "Using internal nodata values (e.g. 0) for image c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\processing\\clc5_class3xx_3035_DE_5m.vrt.\n",
      "Processing c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\processing\\clc5_class3xx_3035_DE_5m.vrt [1/1] : 0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Creating output file that is 128211P x 173468L.\n",
      "Using internal nodata values (e.g. 0) for image c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\processing\\esa_lccs_class_3035_DE_5m.vrt.\n",
      "Processing c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\processing\\esa_lccs_class_3035_DE_5m.vrt [1/1] : 0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Creating output file that is 128211P x 173468L.\n",
      "Using internal nodata values (e.g. 0) for image c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\processing\\jaxa_FNF_3035_DE_5m.vrt.\n",
      "Processing c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\processing\\jaxa_FNF_3035_DE_5m.vrt [1/1] : 0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Creating output file that is 128212P x 173469L.\n",
      "Using internal nodata values (e.g. 0) for image c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\processing\\U2018_CLC2018_V2020_3035_DE_5m.vrt.\n",
      "Processing c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\processing\\U2018_CLC2018_V2020_3035_DE_5m.vrt [1/1] : 0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CLIP RASTERS\n",
    "\n",
    "# Runs batch script which runs gdalwarp for all vrt files ending in \"_5m\" in the processing folder - outputs a vrt for each\n",
    "#clip_to_natura_DE = subprocess.run([\"rq1_step1_sub3_clip.bat\"], \n",
    "#                             capture_output=True, \n",
    "#                             text=True)\n",
    "\n",
    "#print(clip_to_natura_DE.stdout)\n",
    "#print(clip_to_natura_DE.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
