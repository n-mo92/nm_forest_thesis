{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1 Data Preparation\n",
    "\n",
    "[Add description]\n",
    "\n",
    "Steps:\n",
    "1. **Manually** download all datasets (5 forest definitions and Natura 2000) \n",
    "2. Filter Natura 2000 for areas in Germany\n",
    "3. Mosaic data which comes in tiles (Hansen & JAXA)\n",
    "4. Threshold & update (Hansen)\n",
    "    1. Set tree cover threshold\n",
    "    2. Adjust for forest loss & gain\n",
    "5. Extract layer from netcdf (ESA)\n",
    "6. Reproject to the most common projection - EPSG 3035\n",
    "7. Rasterise or Upsample (WITHOUT INTERPOLATION) to 5m \n",
    "8. Convert all datasets to FNF\n",
    "9. Alignment fixes\n",
    "    1. Clip to bbox\n",
    "    2. Warp extents\n",
    "10. Clip data to Germany (CORINE footprint)\n",
    "11. Copy & Rename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup\n",
    "\n",
    "Used this for help with directory setup: \n",
    "https://www.freecodecamp.org/news/creating-a-directory-in-python-how-to-create-a-folder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ./rawdata already exists\n",
      "Folder ./processing already exists\n",
      "Folder ./outputs already exists\n"
     ]
    }
   ],
   "source": [
    "# 0: SETUP\n",
    "\n",
    "# Import packages\n",
    "import os\n",
    "import warnings\n",
    "import glob\n",
    "import math\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.crs import CRS \n",
    "import xarray as xr \n",
    "import rioxarray as rio\n",
    "from shapely.geometry import box\n",
    "\n",
    "# Create required directories if they don't already exist\n",
    "# Note: these directories are ignored in git\n",
    "path_list = (\"./rawdata\", \"./processing\", \"./outputs\")\n",
    "\n",
    "for path in path_list:\n",
    "  if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print(\"Folder %s created!\" % path)\n",
    "  else:\n",
    "    print(\"Folder %s already exists\" % path)\n",
    "\n",
    "# Store gdal.exe paths\n",
    "gdalwarp = \"./thesis_env_conda/Library/bin/gdalwarp.exe\"\n",
    "gdal_footprint = \"./thesis_env_conda/Library/bin/gdal_footprint.exe\"\n",
    "\n",
    "# Store gdal.py paths\n",
    "gdal_calc = \"./thesis_env_conda/Lib/site-packages/GDAL-3.10.1-py3.12-win-amd64.egg-info/scripts/gdal_calc.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Manually download datasets\n",
    "\n",
    "As several of the datasets require login credentials and are not available through an API, I decided to manually download all the required data. I have stored everything in the \"rawdata\" folder. This folder is set to be ignored by git because the files are too big to push onto the GitHub repo.\n",
    "\n",
    "**So for the first step: manually download all datasets using the notes below and save to the \"rawdata\" folder.** \n",
    "\n",
    "Note: For the forest definiton layers I have downloaded the 2018 datasets as this is the most recent data available across all datasets. \n",
    "\n",
    "**1. UMD (Hansen) / Global Forest Watch**\n",
    "- Download from: https://storage.googleapis.com/earthenginepartners-hansen/GFC-2023-v1.11/download.html\n",
    "    - Using the map interface, download the treecover2000, gain & lossyear layers for the 4 granules with top-left corner at: (60N, 0E), (60N, 10E), (50N, 0E) and (50N, 10E). \n",
    "    - The files will be used in combination with each other to generate a dataset that corresponds (roughly) to forest cover in 2018.\n",
    "- My info:\n",
    "    - Download date: 15 Jan 2025\n",
    "    - File: rawdata/Hansen_GFC-2023-v1.11 (folder contains 12 tifs - 4 each for cover, gain and loss)\n",
    "\n",
    "**2. ESA Land Cover**\n",
    "- Download from: https://cds.climate.copernicus.eu/datasets/satellite-land-cover?tab=download \n",
    "    - Login credentials required (there is a prompt in the page to sign up/login).\n",
    "    - Select 2018 map and v2.1.1.\n",
    "    - Only download the sub-region for ~Germany bounding box (N:56, W:1, E:16, S:46).\n",
    "- My Info:\n",
    "    - Download date: 14 Jan 2025\n",
    "    - File: rawdata/C3S-LC-L4-LCCS-Map-300m-P1Y-2018-v2.1.1.area-subset.56.1.46.16.nc\n",
    "\n",
    "**3. JAXA FNF** \n",
    "- Download from: https://www.eorc.jaxa.jp/ALOS/en/palsar_fnf/data/index.htm\n",
    "    - Login credentials required (To register, go here: https://www.eorc.jaxa.jp/ALOS/en/palsar_fnf/registration.htm).\n",
    "    - Under the heading \"PALSAR/PALSAR-2 mosaic and forest/non-forest (FNF) map\", select the 2018 data.\n",
    "    - Use the map interface to click through until you can download tiles. I opted to download four 5 x 5 tiles using the link above the map (for N55E005, N55E010, N50E005, N50E010), but also had to supplement with some individual tiles. I used QGIS to sort through the tiles and figure out which ones were needed. In total, 73 tiles are needed for the Germany Natura areas - a list of the required tile names is available in: other/jaxa_tile_list.txt\n",
    "- My Info:\n",
    "    - Download date: 15 Jan 2025\n",
    "    - File: rawdata/jaxa_2018_fnf_ger (folder contains 73 tifs)  \n",
    "\n",
    "**4. CORINE Land Use** \n",
    "- Download from: https://land.copernicus.eu/en/products/corine-land-cover/clc2018#download\n",
    "    - Login credentials required (there is a prompt in the page to sign up/login).\n",
    "    - Click on “Go to download by area”. Then select the CORINE land cover 2018 layer, use the area selection tool to click on Germany and then click on the download icon beside the layer name. \n",
    "    - From the cart, select the dataset and chose \"vector\" and \"shapefile\". I opted for vector so that I can rasterise at a common resolution that makes sense with the other data. Click the \"Process Download Request\" button.\n",
    "    - NOTE: At this point the download request enters a queue which can take a long time. When it is ready to download, an email will be sent so you don't have to keep checking it.\n",
    "- My Info:\n",
    "    - Download date: 16 Jan 2025 (request date - ready for download on 18 Jan 2025)\n",
    "    - File: U2018_CLC2018_V2020_20u1.zip  (contains: 1 shp & its components)\n",
    "\n",
    "**5. German Land Use**\n",
    "- Download from: https://gdz.bkg.bund.de/index.php/default/corine-land-cover-5-ha-stand-2018-clc5-2018.html  \n",
    "    - Click on the “Direktdownload” tab, and then click on \"Georeferenzierung: UTM32s, Format: Shape (ZIP, 1,24 GB)\". This will download 5 shapefiles which represent the 5 main land cover classes (also used in CORINE) - individual features within these files have their more precise class as an attribute. Class 3 contains the classes related to forests and natural features, but Class 4 (marshlands, etc) is also required for producing the FAO map. So these 2 are retained for now and Class 1 (urban), Class 2 (agriculture) and Class 5 (water) are discarded.\n",
    "- My Info:\n",
    "    - Download date: 14 Jan 2025\n",
    "    - Files: rawdata/clc5_class3xx.zip, rawdata/clc5_class4xx.zip (each contains: 1 shp & its components)\n",
    "\n",
    "**6. Natura 2000 protected areas**\n",
    "- Download from: https://www.eea.europa.eu/en/datahub/datahubitem-view/6fc8ad2d-195d-40f4-bdec-576e7d1268e4\n",
    "    - Download the most recent date available (in my case: 2022 - direct link: https://sdi.eea.europa.eu/data/95e717d4-81dc-415d-a8f0-fecdf7e686b0).\n",
    "- My Info:\n",
    "    - Download date: 15 Jan 2025\n",
    "    - File: rawdata/Natura2000_end2022_epsg3035.zip (contains: 1 shp & its components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Filter Natura 2000 \n",
    "\n",
    "Use the attributes of the Natura shapefile to filter the \"MS\" field (i.e. \"Member States\") to only include \"DE\" (i.e. Germany). \n",
    "\n",
    "I also save the results as a shapefile in the outputs folder as this maybe be useful for visualisations at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: FILTER NATURA2000\n",
    "\n",
    "# Load the Natura 2000 shapefile as a geodataframe\n",
    "# You can do this directly from the zipped file\n",
    "natura_gdf = gpd.read_file(\"./rawdata/Natura2000_end2022_epsg3035.zip\")\n",
    "\n",
    "#print(natura_gdf[1:20])\n",
    "\n",
    "# Extract only the German areas\n",
    "natura_de_gdf = natura_gdf.loc[natura_gdf[\"MS\"] == \"DE\"]\n",
    "\n",
    "# Check - there should be 5200 areas\n",
    "natura_de_gdf.count()\n",
    "\n",
    "# Save the file to outputs folder (turned off warnings which is about a datetime column)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    natura_de_gdf.to_file('./outputs/natura2000_3035_DE.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Mosaic tiled data\n",
    "\n",
    "JAXA and Hansen\n",
    "\n",
    "Help with mosaicing using rasterio: https://automating-gis-processes.github.io/CSC18/lessons/L6/raster-mosaic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: MOSAIC TILES\n",
    "\n",
    "# Store paths for tiles in list\n",
    "jaxa_paths = glob.glob('./rawdata/jaxa_2018_fnf_ger/*.tif')\n",
    "hansen_cover_paths = glob.glob('./rawdata/Hansen_GFC-2023-v1.11/Hansen_GFC-2023-v1.11_tree*.tif')\n",
    "hansen_gain_paths = glob.glob('./rawdata/Hansen_GFC-2023-v1.11/Hansen_GFC-2023-v1.11_gain*.tif')\n",
    "hansen_loss_paths = glob.glob('./rawdata/Hansen_GFC-2023-v1.11/Hansen_GFC-2023-v1.11_loss*.tif')\n",
    "\n",
    "# Store the path for the output mosaics\n",
    "jaxa_mosaic = \"./processing/jaxa_FNF_4326_DE.tif\"\n",
    "hansen_cover_mosaic = \"./processing/hansen_treecover2000_4326_DE.tif\"\n",
    "hansen_gain_mosaic = \"./processing/hansen_gain_4326_DE.tif\"\n",
    "hansen_loss_mosaic = \"./processing/hansen_lossyear_4326_DE.tif\"\n",
    "\n",
    "# Create a function which mosaics the tiles\n",
    "def mosaic_rasters(input_paths, output_path):\n",
    "    # Create an empty list to store the opened tiles\n",
    "    tiles_to_mosaic = []\n",
    "    # Iterate through the tile paths to open them and store the opened tiles a list\n",
    "    for path in input_paths:\n",
    "        tile = rasterio.open(path)\n",
    "        tiles_to_mosaic.append(tile)\n",
    "    # Create the mosaic and store the transform information\n",
    "    mosaic, transform = merge(tiles_to_mosaic)\n",
    "    # Copy the metadata for the mosaic from a tile\n",
    "    mosaic_meta = tile.meta.copy()\n",
    "    # Update the metadata with the new information for the mosaic\n",
    "    mosaic_meta.update({\"driver\": \"GTiff\",\n",
    "                        \"height\": mosaic.shape[1],\n",
    "                        \"width\": mosaic.shape[2],\n",
    "                        \"transform\": transform,\n",
    "                        # crs is not included, as it is copied from the tiles\n",
    "                        }\n",
    "                        )\n",
    "        # Write the mosaic with its metata to a tif file\n",
    "    with rasterio.open(output_path, \"w\", **mosaic_meta, compress=\"LZW\") as dest:\n",
    "        dest.write(mosaic)\n",
    "\n",
    "# Run the function to mosaic the tiles\n",
    "# If mosaic already exists, make sure it's not open in QGIS :) you'll get permission error if so!\n",
    "mosaic_rasters(jaxa_paths, jaxa_mosaic)\n",
    "mosaic_rasters(hansen_cover_paths, hansen_cover_mosaic)\n",
    "mosaic_rasters(hansen_gain_paths, hansen_gain_mosaic)\n",
    "mosaic_rasters(hansen_loss_paths, hansen_loss_mosaic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Threshold & update for gain/loss (Hansen)\n",
    "\n",
    "To prepare the Hansen data, two main steps need to be taken:\n",
    "\n",
    "1. Set the tree cover threshold\n",
    "2. Adjust for forest loss & gain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Set tree cover threshold\n",
    "\n",
    "For the tree cover threshold I've decided to use >60% cover. This provides a good comparison with the other datasets (which are all lower), and it's also the threshold used by the International Geosphere-Biosphere Programme (IGBP) definition. For this step, I essentially need to reclassify 1-100 values (in the forst cover 2000 layer) so that 61-100 are forest (1), everything else is non-forest (0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1: HANSEN COVER THRESHOLD\n",
    "\n",
    "# Store path to Hansen tree cover mosaic\n",
    "hansen_cover_mosaic = \"./processing/hansen_treecover2000_4326_DE.tif\"\n",
    "\n",
    "# Runs gdal_calc.py to define forest cover >60% as forest (1) and everything else as 0\n",
    "reclass_hansen_cover = subprocess.run(['python', \n",
    "                                       gdal_calc, \n",
    "                                       '-A', hansen_cover_mosaic, \n",
    "                                       '--outfile=./processing/hansen_treecover2000_60_4326_DE.tif', \n",
    "                                       '--calc=0*((A>=0)*(A<=60))+1*((A>=61)*(A<=100))', \n",
    "                                       '--co=COMPRESS=LZW', \n",
    "                                       '--co=BIGTIFF=YES', \n",
    "                                       '--NoDataValue=-9999'\n",
    "                                       ],\n",
    "                                       capture_output=True, \n",
    "                                       text=True)\n",
    "\n",
    "print(reclass_hansen_cover.stdout)\n",
    "print(reclass_hansen_cover.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Adjust for forest loss & gain\n",
    "\n",
    "To update the 2000 forest cover to 2018 (or as close as possible) the map needs to be updated using the loss and gain layers.\n",
    "\n",
    "For the loss, the data is stored as loss per year (2001-2023). To only consider the loss occurring up to 2018. I first need to reclassify the loss raster so that 1-18 has a value of 1 (loss), everything else is 0 (no loss). \n",
    "\n",
    "For the gain, the data is stored as a single raster with values of 1 if there was gain between 2001-2012 and 0 if there was no gain. This makes it more complicated as the gain can not be updated on a yearly basis like the loss, and I am missing gain for 2013-2018. To help contextualise and understand the relevance of the gain layer in the context of Germany and Germany's Natura 2000 areas, I gathered the following information by exploring the data in QGIS & Excel:\n",
    "\n",
    "| Area                       | Total Forest in 2000 (>60% tree cover) | \n",
    "| -------------------------- | -------------------------------------  | \n",
    "| Germany (CORINE footprint) |             140198 km2                 | \n",
    "| Germany Natura 2000 areas  |              46867 km2                 | \n",
    "\n",
    "\n",
    "| Area                       | Total Loss 2001-2018 | Average Loss Per Year |\n",
    "| -------------------------- | -------------------  | --------------------- |\n",
    "| Germany (CORINE footprint) |   8436 km2 (6.02%)   |    496 km2 (0.35%)    |\n",
    "| Germany Natura 2000 areas  |   2022 km2 (4.32%)   |    119 km2 (0.25%)    |\n",
    "\n",
    "\n",
    "| Area                       | Total Gain 2001-2012 | Average Gain Per Year |\n",
    "| -------------------------- | -------------------  | --------------------- |\n",
    "| Germany (CORINE footprint) |   3262 km2 (2.33%)   |    297 km2 (0.21%)    |\n",
    "| Germany Natura 2000 areas  |    719 km2 (1.53%)   |     65 km2 (0.14%)    |\n",
    "\n",
    "** Percentages are the proportion of loss/gain out of the total forest cover in 2000\n",
    "*** These calculations are estimates; they are based on multiplying by a 25m*25m pixel, which is an approximation of 0.00025 deg pixel size and the values are rounded to the nearest whole number. The rate of gain/loss per year is also an estimate which assumes a constant rate of change over time.\n",
    "\n",
    "I then generated the following estimates to see what the margin of error would be if gain was not included. Note the second value which takes loss and gain into account assumes that gain is constant over time AND also that it does NOT overlap with areas of loss. As the areas of loss and gain are likely spatially autocorrelation this latter assumptions means that the percentage difference here would be the *maximum* amount of difference, and *it is likely that the true total forest cover results are somewhere in between the two estimates*.\n",
    "\n",
    "| Area                       | Total Forest 2018 | Total Forest 2018 | Difference       |\n",
    "|                            | (w loss)          | (w loss & gain)   |                  |\n",
    "| -------------------------- | ----------------- | ----------------- | ---------------- |\n",
    "| Germany (CORINE footprint) |     131762 km2    |    136803 km2     | 5041 km2 (3.75%) |\n",
    "| Germany Natura 2000 areas  |      44845 km2    |     45956 km2     | 1111 km2 (2.45%) |\n",
    "\n",
    "\n",
    "\n",
    "In the end I decide to ?????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1: LOSS & GAIN - LOSS PREP\n",
    "\n",
    "# Store path to Hansen loss mosaic\n",
    "hansen_loss_mosaic = \"./processing/hansen_lossyear_4326_DE.tif\"\n",
    "\n",
    "# Runs gdal_calc.py in order to capture loss between 2001-2018\n",
    "hansen_loss_2018 = subprocess.run(['python', \n",
    "                                   gdal_calc, \n",
    "                                   '-A', hansen_loss_mosaic, \n",
    "                                   '--outfile=./processing/hansen_lossto2018_4326_DE.tif', \n",
    "                                   '--calc=0*(A==0)+1*((A>=1)*(A<=18))+0*(A>=19)', \n",
    "                                   '--co=COMPRESS=LZW', \n",
    "                                   '--co=BIGTIFF=YES', \n",
    "                                   '--NoDataValue=-9999'\n",
    "                                   ],\n",
    "                                   capture_output=True, \n",
    "                                   text=True)\n",
    "\n",
    "print(hansen_loss_2018.stdout)\n",
    "print(hansen_loss_2018.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1: LOSS & GAIN - UPDATE COVER\n",
    "# Gain is alread in 0 and 1 format\n",
    "\n",
    "# use raster calc to add gain and subtract loss\n",
    "# could make 3 versions - one with gain added first, one with gain added second, one without gain\n",
    "# compare outputs - percentage difference in total forest cover?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Extract layer from netcdf\n",
    "\n",
    "This is required for the ESA dataset only. The netcdf file includes several different layers of information - the one I want to use is called \"lccs_class\". The aim here is to simply extract the single layer and save it as a geotiff.\n",
    "\n",
    "Help with saving netcdf layer as geotiff: https://help.marine.copernicus.eu/en/articles/5029956-how-to-convert-netcdf-to-geotiff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: EXTRACT & CONVERT NETCDF DATA\n",
    "\n",
    "# Open the ESA netcdf for conversion\n",
    "esa_netcdf = xr.open_dataset(\"./rawdata/C3S-LC-L4-LCCS-Map-300m-P1Y-2018-v2.1.1.area-subset.56.1.46.16.nc\", engine=\"netcdf4\")\n",
    "#esa_netcdf\n",
    "\n",
    "# Extract the lccs_class variable\n",
    "esa_lccs_class = esa_netcdf['lccs_class']\n",
    "\n",
    "# Provide spatial axis & define the CRS\n",
    "esa_lccs_class = esa_lccs_class.rio.set_spatial_dims(x_dim='lon', y_dim='lat')\n",
    "esa_lccs_class.rio.crs\n",
    "esa_lccs_class.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "# Save the geotiff\n",
    "esa_lccs_class.rio.to_raster(\"./processing/esa_lccs_class_4326_DE.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Reproject\n",
    "\n",
    "The data needed to be projected to work in units of meters for calculating area. Three datasets are not in a projected CRS (they are WGS 1984 / EPSG:4326). The most common projection is ETRS89-extended / LAEA Europe (EPSG: 3035) which is used by the Natura 2000 areas and the CORINE dataset. \n",
    "\n",
    "So for this step, all datasets which are not already in this projection, will be (re)projected to 3035.\n",
    "\n",
    "Help with reprojecting using rioxarray: https://www.earthdatascience.org/courses/use-data-open-source-python/intro-raster-data-python/raster-data-processing/reproject-raster/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: REPROJECT RASTERS\n",
    "\n",
    "# A quick function for reprojecting individual rasters to EPSG:3035\n",
    "def reproject_raster_3035(input_path, output_path):\n",
    "    # Open the raster (NOTE: need to use rio.open_rasterio() here!)\n",
    "    input = rio.open_rasterio(input_path)\n",
    "    # Run the reprojection\n",
    "    output = input.rio.reproject(\"EPSG:3035\")\n",
    "    # Write the reprojected output raster\n",
    "    output.rio.to_raster(output_path, compress = \"LZW\")\n",
    "\n",
    "# Run this per file \n",
    "reproject_raster_3035(\"./processing/jaxa_FNF_4326_DE.tif\", \"./processing/jaxa_FNF_3035_DE.tif\")\n",
    "reproject_raster_3035(\"./processing/esa_lccs_class_4326_DE.tif\", \"./processing/esa_lccs_class_3035_DE.tif\")\n",
    "\n",
    "# TO DO: HANSEN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: REPROJECT SHPS\n",
    "\n",
    "# Store paths for shp zips in list\n",
    "ger_lulc_paths = glob.glob('./rawdata/clc5_class*.zip')\n",
    "\n",
    "# Create a function which reprojects the shp to 3035 (and saves to processing folder)\n",
    "def reproj_shp_3035(input_paths):\n",
    "    # Iterate through the shp paths \n",
    "    for path in input_paths:\n",
    "        # Open the shp for each path (excludes extra cols as they cause problems & are not needed)\n",
    "        shp = gpd.read_file(path, columns = [\"CLC18\"])\n",
    "        \n",
    "        # Reprojects to 3035\n",
    "        shp_3035  = shp.to_crs(\"EPSG:3035\")\n",
    "\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(path)[1] \n",
    "        # For output file naming: remove extension from input file name \n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # For output file naming: create the new name for reprojected shp\n",
    "        new_name = name_wo_ext + \"_3035_DE.shp\"\n",
    "\n",
    "        # Write the reprojected shp to the processing folder\n",
    "        shp_3035.to_file('./processing/' + new_name)\n",
    "\n",
    "# Run the function for the German LULC zipped shps\n",
    "reproj_shp_3035(ger_lulc_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: CLEAN UP\n",
    "\n",
    "# Create a list of the data paths for deletion (files with \"4326\" in their name)\n",
    "old_data =  glob.glob('./processing/*4326*')\n",
    "\n",
    "# Create a function which deletes the input paths\n",
    "def clean_up(input_paths):\n",
    "    for path in input_paths:\n",
    "        # Check that the paths exist\n",
    "        if os.path.exists(path):  \n",
    "           os.remove(path)\n",
    "        else:\n",
    "            print(\"Nothing to clean!\") \n",
    "\n",
    "# Run the function to remove any data with \"4326\" in the file name\n",
    "clean_up(old_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Rasterise or Upsample\n",
    "\n",
    "In this step, I rasterise the vector files (German LULC - Class 3 only & CORINE) and upsample the already exisiting rasters to 5m. \n",
    "\n",
    "5m was selected as is the commonly divisible unit across all datasets; so all pixels can be approximately divided by 5, meaning there is as little transformation as possible (little interpolation - no \"new information\" is created). It also means that a lot of the detail of the shapefiles can be retained during rasterisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: RASTERISE VECTORS (WITH ATTRIBUTE VALUE)\n",
    "# This is only required for GER LULC Class3 and CORINE\n",
    "\n",
    "# First some prep is needed for GER LULC Class 3\n",
    "# Read in shp\n",
    "ger_lulc_class3xx = gpd.read_file(\"./processing/clc5_class3xx_3035_DE.shp\")\n",
    "\n",
    "# Convert the CLC18 column to integer (stored as string in original file)\n",
    "ger_lulc_class3xx['CLC18'] = ger_lulc_class3xx['CLC18'].astype('int')\n",
    "\n",
    "# Save the output\n",
    "ger_lulc_class3xx.to_file('./processing/clc5_class3xx_3035_DE_int.shp')\n",
    "\n",
    "# And we need to unzip the CORINE data\n",
    "# Read in shp, Code_18 column only \n",
    "corine_shp = gpd.read_file(\"./rawdata/U2018_CLC2018_V2020_20u1.zip\", columns = [\"Code_18\"])\n",
    "\n",
    "# Save the output\n",
    "corine_shp.to_file('./processing/U2018_CLC2018_V2020_DE.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: RASTERISE VECTORS (WITH ATTRIBUTE VALUE) - CONTINUED\n",
    "\n",
    "# TAKES ABOUT 35 MIN TO RUN\n",
    "# Runs batch script which runs gdal_rasterize for GER LULC Class 3 & CORINE - outputs 5m tifs\n",
    "rasterise_5m = subprocess.run([\"rq1_step1_sub1_rasterise.bat\"], \n",
    "                                    capture_output=True, \n",
    "                                    text=True)\n",
    "\n",
    "print(rasterise_5m.stdout)\n",
    "print(rasterise_5m.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: UPSAMPLE RASTERS \n",
    "\n",
    "# TAKES ABOUT 30 MIN TO RUN\n",
    "# Runs batch script which runs gdal_translate to resample rasters (ESA & JAXA) to 5m - outputs tifs\n",
    "upsample_5m = subprocess.run([\"rq1_step1_sub2_upsample.bat\"], \n",
    "                             capture_output=True, \n",
    "                             text=True)\n",
    "\n",
    "print(upsample_5m.stdout)\n",
    "print(upsample_5m.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Convert all datasets to FNF\n",
    "\n",
    "To prepare the datasets for turning into a presence/absence consensus map, and also to prepare the JAXA and GER LULC maps for use in the workflow to create the FAO-aligned map, they need to be converted to Forest-Nonforest maps - i.e. maps where there are only two classes, 0 = Nonforest and 1 = Forest.\n",
    "\n",
    "As each dataset has its own set of classes, this conversion needs to be customisted for each map.\n",
    "\n",
    "Help with reclassifying (use gdal_calc): https://gis.stackexchange.com/questions/245170/reclassifying-raster-using-gdal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8.1: JAXA Reclassify\n",
    "\n",
    "For JAXA, the reclassification to true FNF is as follows:\n",
    "\n",
    "| Original Value | Original Label               | New Value | New Label  |\n",
    "| -------------- | ---------------------------- | --------- | ---------- |\n",
    "| 0              | NoData                       | -9999     | NoData     |\n",
    "| 1              | Forest (>90% canopy cover)   | 1         | Forest     |\n",
    "| 2              | Forest (10-90% canopy cover) | 1         | Forest     |\n",
    "| 3              | Non-Forest                   | 0         | Non-Forest |\n",
    "| 4              | Water                        | 0         | Non-Forest |\n",
    "\n",
    "Both forest categories are converted to forest here as this fits with the FAO canopy cover thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1: RECLASSIFY JAXA \n",
    "\n",
    "# Store path to 5m Jaxa dataset as the intput file\n",
    "jaxa_input = \"./processing/jaxa_FNF_3035_DE_5m.tif\"\n",
    "\n",
    "# TAKES ABOUT 30 MIN TO RUN\n",
    "# Runs gdal_calc.py in order to reclassify JAXA 5m raster \n",
    "reclass_jaxa = subprocess.run(['python', \n",
    "                               gdal_calc, \n",
    "                               '-A', jaxa_input, \n",
    "                               '--outfile=./processing/jaxa_FNF_3035_DE_5m_reclass.tif', \n",
    "                               '--calc=-9999*(A==0)+1*(A==1)+1*(A==2)+0*(A>=3)', \n",
    "                               '--co=COMPRESS=LZW', \n",
    "                               '--co=BIGTIFF=YES', \n",
    "                               '--NoDataValue=-9999'\n",
    "                               ],\n",
    "                              capture_output=True, \n",
    "                              text=True)\n",
    "\n",
    "print(reclass_jaxa.stdout)\n",
    "print(reclass_jaxa.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8.2: CORINE Reclassify\n",
    "\n",
    "For CORINE, the reclassification to FNF is as follows:\n",
    "\n",
    "| Original Value | Original Label                 | New Value | New Label  |\n",
    "| -------------- | ------------------------------ | --------- | ---------- |\n",
    "| 0              | NoData                         | -9999     | NoData     |\n",
    "| 1xx            | Urban classes                  | 0         | Non-Forest |\n",
    "| 2xx            | Agricultural classes           | 0         | Non-Forest |\n",
    "| 311            | Broad-leaved forest            | 1         | Forest     |\n",
    "| 312            | Coniferous forest              | 1         | Forest     |\n",
    "| 313            | Mixed forest                   | 1         | Forest     |\n",
    "| 321            | Natural grasslands             | 0         | Non-Forest |\n",
    "| 322            | Moors and heathland            | 0         | Non-Forest |\n",
    "| 323            | Sclerophyllous vegetation      | 1         | Forest     |\n",
    "| 324            | Transitional woodland-shrub    | 1         | Forest     |\n",
    "| 331            | Beaches - dunes - sands        | 0         | Non-Forest |\n",
    "| 332            | Bare rocks                     | 0         | Non-Forest |\n",
    "| 333            | Sparsely vegetated areas       | 0         | Non-Forest |\n",
    "| 334            | Burnt areas                    | 0         | Non-Forest |\n",
    "| 335            | Glaciers and perpetual snow    | 0         | Non-Forest |\n",
    "| 4xx            | Marsh, bog, intertidal classes | 0         | Non-Forest |\n",
    "| 5xx            | Water body classes             | 0         | Non-Forest |\n",
    "| >=600          | NoData                         | -9999     | NoData     |\n",
    "\n",
    "More simply: classes <311 = Non-Forest; classes 311-324 = Forest, classes >325 = Non-Forest.\n",
    "\n",
    "This is based on *Natura 2000 and forests. Part I-II* (European Commission, 2015) which describes how forest area calculations were performed with data from CORINE with \"CLC classes grouped as forests: 311 Broad-leaf forests; 312 Coniferous forests; 313 Mixed forests; 323 Sclerophyllous vegetation; 324 Transitional woodland-shrub.\" \n",
    "\n",
    "** Note the 323 class (Sclerophyllous vegetation) is not present in the Germany dataset.\n",
    "\n",
    "Whether class 324 (Transitional woodland-shrub) is included is a bit uncertain. In the definition above it is included, and in the *State of nature in the EU...* report (EEA, 2020) forest area tends to be reported as \"Forests and transitional woodland shrubbodies\" - so they are also sort of grouped together. \n",
    "\n",
    "For now, I will include class 324 in the definition of forest for CORINE. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2: RECLASSIFY CORINE\n",
    "\n",
    "# Store path to 5m Corine dataset as the intput file\n",
    "corine_input = \"./processing/U2018_CLC2018_V2020_3035_DE_5m.tif\"\n",
    "\n",
    "# TAKES ABOUT 10 MIN TO RUN\n",
    "# Runs gdal_calc.py in order to reclassify CORINE 5m raster \n",
    "reclass_corine = subprocess.run(['python', \n",
    "                                 gdal_calc, \n",
    "                                 '-A', corine_input, \n",
    "                                 '--outfile=./processing/U2018_CLC2018_V2020_3035_DE_5m_reclass.tif', \n",
    "                                 '--calc=-9999*(A==0)+0*(A<=310)+1*((A>=311)*(A<=324))+0*((A>=325)*(A<=599))+-9999*(A>=600)', \n",
    "                                 '--co=COMPRESS=LZW', \n",
    "                                 '--co=BIGTIFF=YES', \n",
    "                                 '--NoDataValue=-9999'\n",
    "                                 ],\n",
    "                              capture_output=True, \n",
    "                              text=True)\n",
    "\n",
    "print(reclass_corine.stdout)\n",
    "print(reclass_corine.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8.3: GER LULC Class 3 Reclassify\n",
    "\n",
    "The GER LULC class conversion is essentially the same as the CORINE one; I am applying the same definition of forest to both, but this dataset is a different operationalisation of that definition (and also how they apply the classes appears to be different?). Because the different main classes (1xx, 2xx, 3xx, 4xx and 5xx) come in separate shapefiles, the reclassification is simplier since I'm only dealing with class 3 for forests. \n",
    "\n",
    "For GER LULC Class 3, the reclassification to FNF is as follows:\n",
    "\n",
    "| Original Value | Original Label                 | New Value | New Label  |\n",
    "| -------------- | ------------------------------ | --------- | ---------- |\n",
    "| 0              | NoData                         | 0         | Non-Forest |\n",
    "| 311            | Broad-leaved forest            | 1         | Forest     |\n",
    "| 312            | Coniferous forest              | 1         | Forest     |\n",
    "| 313            | Mixed forest                   | 1         | Forest     |\n",
    "| 321            | Natural grasslands             | 0         | Non-Forest |\n",
    "| 322            | Moors and heathland            | 0         | Non-Forest |\n",
    "| 323            | Sclerophyllous vegetation      | 1         | Forest     |\n",
    "| 324            | Transitional woodland-shrub    | 1         | Forest     |\n",
    "| 331            | Beaches - dunes - sands        | 0         | Non-Forest |\n",
    "| 332            | Bare rocks                     | 0         | Non-Forest |\n",
    "| 333            | Sparsely vegetated areas       | 0         | Non-Forest |\n",
    "| 334            | Burnt areas                    | 0         | Non-Forest |\n",
    "| 335            | Glaciers and perpetual snow    | 0         | Non-Forest |\n",
    "| >=336          | NoData                         | -9999     | NoData     |\n",
    "\n",
    "More simply: classes <310 = Non-Forest; classes 311-324 = Forest, classes >325 = Non-Forest.\n",
    "\n",
    "See the CORINE reclassification for more explantion on the class conversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3: RECLASSIFY GER LULC Class 3\n",
    "\n",
    "# Store path to 5m GER LULC Class 3 as the intput file\n",
    "ger_lulc3_input = \"./processing/clc5_class3xx_3035_DE_5m.tif\"\n",
    "\n",
    "# TAKES ABOUT 15 MIN TO RUN\n",
    "# Runs gdal_calc.py in order to reclassify GER LULC Class 3 5m raster \n",
    "reclass_ger_lulc3 = subprocess.run(['python', \n",
    "                                    gdal_calc, \n",
    "                                    '-A', ger_lulc3_input, \n",
    "                                    '--outfile=./processing/clc5_class3xx_3035_DE_5m_reclass.tif', \n",
    "                                    '--calc=0*(A<=310)+1*((A>=311)*(A<=324))+0*((A>=325)*(A<=335))+-9999*(A>=336)', \n",
    "                                    '--co=COMPRESS=LZW', \n",
    "                                    '--co=BIGTIFF=YES', \n",
    "                                    '--NoDataValue=-9999'\n",
    "                                    ],\n",
    "                              capture_output=True, \n",
    "                              text=True)\n",
    "\n",
    "print(reclass_ger_lulc3.stdout)\n",
    "print(reclass_ger_lulc3.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8.4: ESA Reclassify\n",
    "\n",
    "For ESA, the reclassification to FNF is as follows:\n",
    "\n",
    "| Original Value          | Original Label                 | New Value | New Label  |\n",
    "| ----------------------- | ------------------------------ | --------- | ---------- |\n",
    "| 0                       | NoData                         | -9999     | NoData     |\n",
    "| 10, 11, 12, 20, 30, 40  | Agriculture classes            | 0         | Non-Forest |\n",
    "| 50                      | Broadleaf evergreen            | 1         | Forest     |\n",
    "| 60, 61, 62              | Broadleaf deciduous            | 1         | Forest     |\n",
    "| 70, 71, 72              | Needleleaf evergreen           | 1         | Forest     |\n",
    "| 80, 81, 82              | Needleleaf deciduous           | 1         | Forest     |\n",
    "| 90                      | Mixed (broad & needle leaf)    | 1         | Forest     |\n",
    "| 100                     | Mosaic tree & shrub / herb.    | 1         | Forest     |\n",
    "| 110                     | Mosaic herb. / tree & shrub    | 0         | Non-Forest |\n",
    "| 120, 121, 122           | Shrubland                      | 0         | Non-Forest |\n",
    "| 130                     | Grassland                      | 0         | Non-Forest |\n",
    "| 140, 150, 151, 152, 153 | Sparse vegetation              | 0         | Non-Forest |\n",
    "| 160, 170                | Tree cover, flooded            | 1         | Forest     |\n",
    "| 180                     | Wetland                        | 0         | Non-Forest |\n",
    "| 190                     | Urban                          | 0         | Non-Forest |\n",
    "| 200, 201, 202           | Bare Areas                     | 0         | Non-Forest |\n",
    "| 210, 220                | Water / Permanent Snow & Ice   | 0         | Non-Forest |\n",
    "\n",
    "\n",
    "This is based on how the producers of the data align their ESA classes with the IPCC land categories - see page 30 of the *Land Cover CCI Product User Guide*.\n",
    "\n",
    "Note that the classes 160, 170 (which correspond to mangroves and are included as forests) are not present in the Germany dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4: RECLASSIFY ESA\n",
    "\n",
    "# Store path to 5m ESA as the intput file\n",
    "esa_input = \"./processing/esa_lccs_class_3035_DE_5m.tif\"\n",
    "\n",
    "# TAKES ABOUT 60 MIN TO RUN \n",
    "# Runs gdal_calc.py in order to reclassify ESA 5m raster \n",
    "reclass_esa = subprocess.run(['python', \n",
    "                              gdal_calc, \n",
    "                              '-A', esa_input, \n",
    "                              '--outfile=./processing/esa_lccs_class_3035_DE_5m_reclass.tif', \n",
    "                              '--calc=-9999*(A==0)+0*((A>=10)*(A<=40))+1*((A>=50)*(A<=100))+0*((A>=110)*(A<=153))+1*((A>=160)*(A<=170))+0*(A>=180)', \n",
    "                              '--co=COMPRESS=LZW', \n",
    "                              '--co=BIGTIFF=YES', \n",
    "                              '--NoDataValue=-9999'\n",
    "                              ],\n",
    "                              capture_output=True, \n",
    "                              text=True)\n",
    "\n",
    "print(reclass_esa.stdout)\n",
    "print(reclass_esa.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8.5: Hansen Reclassify\n",
    "\n",
    "This might not be needed (I might already end up with a FNF for Hansen from Step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5: RECLASSIFY HANSEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Alginment Fixes\n",
    "\n",
    "When initally running the raster calculation for the consensus maps I got an error message saying the extents (width & height) of the CORINE FNF raster did not match the others. \n",
    "\n",
    "CORINE: (128212, 173469)\n",
    "others: (128211, 173468)\n",
    "\n",
    "This raised a broader issue of the rasters not being exactly aligned. I originally had only clipped the rasters to the CORINE footprint, and issues were visible at the raster edges in QGIS, particularly where there is forest, i.e. values of 1, with only CORINE snapped directly to the clipper boundary and the others either just inside or just outside.\n",
    "\n",
    "This is problematic both because the forest pixels should align for proper comparison, and also when the extents are different, the rasters cannot be used together in raster calculations. To fix this issue, I take 2 additional steps before clipping to the final boundary (step 10).\n",
    "\n",
    "1. Clip all rasters to a common rectangular area (in this case the bounding box for the CORINE dataset).\n",
    "2. Warp the extents to match the extent of the CORINE dataset. This forces the pixels to line up, but introduces some weird data values at the edges (this is okay because these values will be removed in the final clip in step 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1: CLIP TO CORINE BBOX\n",
    "# WHOLE PROCESS (4 DATASETS): ABOUT 155 MIN\n",
    "\n",
    "# Open reclassified CORINE tif\n",
    "corine_reclass = rasterio.open(\"./processing/U2018_CLC2018_V2020_3035_DE_5m_reclass.tif\")\n",
    "\n",
    "# Store the BoundingBox for the CORINE tif\n",
    "corine_bounds  = corine_reclass.bounds\n",
    "\n",
    "# Use shapely \"box\" to convert to geometry\n",
    "corine_bbox_geom = box(*corine_bounds)\n",
    "\n",
    "# Convert geometry to gpd df, set projection & save as shp\n",
    "corine_bbox_df = gpd.GeoDataFrame({\"id\":1,\"geometry\":[corine_bbox_geom]})\n",
    "corine_bbox_df = corine_bbox_df.set_crs(\"EPSG:3035\")\n",
    "corine_bbox_df.to_file(\"./processing/corine_reclass_bbox.shp\")\n",
    "\n",
    "# Store path to CORINE bbox shp\n",
    "corine_bbox_shp = \"./processing/corine_reclass_bbox.shp\"\n",
    "\n",
    "# Store paths for reclassed tifs in a list\n",
    "reclassed_paths = glob.glob('./processing/*_reclass.tif')\n",
    "\n",
    "# Define function that clips each reclassed tif to the CORINE bbox\n",
    "def bbox_clip(input_paths):\n",
    "    # Iterate through the paths \n",
    "    for path in input_paths:\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(path)[1] \n",
    "        # For output file naming: remove extension and last 12 letters (\"_reclass.tif\")\n",
    "        root_name = name_w_ext[:-12]\n",
    "        # For output file naming: assemble the new file path for the output\n",
    "        output_path = \"./processing/\" + root_name + \"_bboxclip.tif\"\n",
    "\n",
    "        # Run warp to crop to the CORINE bbox\n",
    "        clip_to_bbox = subprocess.run([gdalwarp, \n",
    "                                       '-crop_to_cutline', \n",
    "                                       '-cutline', corine_bbox_shp, \n",
    "                                       '-tr', '5', '5',\n",
    "                                       '-dstnodata', '-9999', \n",
    "                                       '-ot', 'Int16', \n",
    "                                       '-co', 'COMPRESS=LZW', \n",
    "                                       '-co', 'BIGTIFF=YES', \n",
    "                                       path, \n",
    "                                       output_path\n",
    "                                       ],\n",
    "                                       capture_output=True, \n",
    "                                       text=True)\n",
    "        print(clip_to_bbox.stdout)\n",
    "        print(clip_to_bbox.stderr)\n",
    "\n",
    "# Clip all reclassed tifs to the CORINE bbox\n",
    "bbox_clip(reclassed_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2: WARP EXTENTS\n",
    "# WHOLE PROCESS (4 DATASETS): ABOUT 85 MIN\n",
    "\n",
    "# First, extract the extents from the CORINE data \n",
    "corine_ref = rasterio.open(\"./processing/U2018_CLC2018_V2020_3035_DE_5m_bboxclip.tif\")\n",
    "corine_bounds  = corine_ref.bounds\n",
    "\n",
    "# Store the bounds in the format required for gdalwarp\n",
    "corine_xmin = str(corine_bounds[0])       # xmin = left\n",
    "corine_ymin = str(corine_bounds[1])       # ymin = bottom\n",
    "corine_xmax = str(corine_bounds[2])       # xmax = right\n",
    "corine_ymax = str(corine_bounds[3])       # ymax = top\n",
    "\n",
    "# Store paths for clipped (to bbox) tifs in a list\n",
    "bbox_clipped_paths = glob.glob('./processing/*_bboxclip.tif')\n",
    "\n",
    "# Define function that warps each raster to the CORINE extents\n",
    "def corine_warp(input_paths):\n",
    "    # Iterate through the paths \n",
    "    for path in input_paths:\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(path)[1] \n",
    "        # For output file naming: remove extension from input file name\n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # For output file naming: assemble the new file path for the output\n",
    "        output_path = \"./processing/\" + name_wo_ext + \"_warp_exts.tif\"\n",
    "        \n",
    "        # Run warp to match all rasters to CORINE extents\n",
    "        warp_extents = subprocess.run([gdalwarp, \n",
    "                                      '-t_srs', 'EPSG:3035', \n",
    "                                      #'-tr', '5', '5',\n",
    "                                      '-te', corine_xmin, corine_ymin, corine_xmax, corine_ymax,\n",
    "                                      #'-tap',\n",
    "                                      '-ot', 'Int16', \n",
    "                                      '-co', 'COMPRESS=LZW', \n",
    "                                      '-co', 'BIGTIFF=YES', \n",
    "                                      path, \n",
    "                                      output_path\n",
    "                                      ],\n",
    "                                      capture_output=True, \n",
    "                                      text=True)\n",
    "        print(warp_extents.stdout)\n",
    "        print(warp_extents.stderr)\n",
    "\n",
    "\n",
    "# Warp all \"bboxclip\" rasters to CORINE extents\n",
    "corine_warp(bbox_clipped_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Clip to Germany (CORINE footprint)\n",
    "\n",
    "After completing the alignment fixes, I now come back to my orginal clip step: clip all rasters again, this time to the CORINE footprint (i.e. to the outline of Germany). This gets rid of unwanted data outside of Germany (necessary for raster calculations) as well as the weird edge values introduced during the warping in step 9. \n",
    "\n",
    "I originally intended to clip the rasters to the German Natura2000 areas, however this proved to be too slow / create insanely large outputs (I was also trying to do this before converting to FNF which may have caused problems as these datasets are bigger). I have therefore adjusted my plans to clip to Germany instead. This provides a common extent and area of coverage across all rasters, and it allows me to calculate differences in forests for the whole country by summing the entire raster. \n",
    "\n",
    "Note that I decided to clip to the footprint of the CORINE data. This does mean that some data on the edges are lost from the GER LULC output. However, it was relatively little data and it makes sense to clip to a common region so that I am comparing the same areas across maps. \n",
    "\n",
    "I plan to use zonal statistics to extract pixel counts (which can then be summed and multiplied to convert to m2) for the Natura 2000 areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10: CLIP TO CORINE FOOTPRINT\n",
    "\n",
    "# WARNING! To produce 4 datasets, this runs for about 690 MIN (~11.5 hours)!\n",
    "# Runs batch script which runs gdalwarp for all vrt files ending in \"_reclass\" in the processing folder - outputs a tif for each\n",
    "clip_to_DE = subprocess.run([\"rq1_step1_sub3_clip.bat\"], \n",
    "                             capture_output=True, \n",
    "                             text=True)\n",
    "\n",
    "print(clip_to_DE.stdout)\n",
    "print(clip_to_DE.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Copy & Rename\n",
    "\n",
    "After visually checking the rasters in QGIS, the outputs from the last step seem to meet all the requirments! They are FNF maps with 2 values (1 = Forest, 0 = Non-Forest) and a NoData value = -9999. They are all clipped to Germany (CORINE footprint) and the rasters are aligned (pixels overlap completely and extents/bounds & origin are all the same). \n",
    "\n",
    "Since I'm happy with the results, I now copy over the rasters to the \"outputs\" folder and rename them to indicate they are the FNF outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./outputs/clc5_class3xx_3035_DE_5m_FNF.tif'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11: COPY & RENAME\n",
    "\n",
    "# Store the paths to the old version (to be copied & renamed)\n",
    "corine_old = \"./processing/U2018_CLC2018_V2020_3035_DE_5m_bboxclip_warp_exts_clipped.tif\"\n",
    "jaxa_old = \"./processing/jaxa_FNF_3035_DE_5m_bboxclip_warp_exts_clipped.tif\"\n",
    "esa_old = \"./processing/esa_lccs_class_3035_DE_5m_bboxclip_warp_exts_clipped.tif\" \n",
    "ger_lulc_old = \"./processing/clc5_class3xx_3035_DE_5m_bboxclip_warp_exts_clipped.tif\" \n",
    "\n",
    "# Copy & rename each raster\n",
    "shutil.copy(corine_old, \"./outputs/\" + os.path.split(corine_old)[1][:-30] + \"FNF.tif\")\n",
    "shutil.copy(jaxa_old, \"./outputs/\" + os.path.split(jaxa_old)[1][:-30] + \"FNF.tif\")\n",
    "shutil.copy(esa_old, \"./outputs/\" + os.path.split(esa_old)[1][:-30] + \"FNF.tif\")\n",
    "shutil.copy(ger_lulc_old, \"./outputs/\" + os.path.split(ger_lulc_old)[1][:-30] + \"FNF.tif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
