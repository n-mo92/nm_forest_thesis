{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "331c85f0",
   "metadata": {},
   "source": [
    "## RQ3 Full Process\n",
    "\n",
    "General plan:\n",
    "- Separate the main corpus into two separate ones: one where the dominant consensus class is Non-Consensus (max_class=3) and one where the dominant consensus is Full Conensus Forest (max_class=6).\n",
    "- Create tokens by performing the same text processing steps as in rq2_step2_text_analysis.ipynb (ie. cleaning, translating, tokenising, filtering)\n",
    "- Assign each token their cluster from rq2_step2_text_analysis.ipynb\n",
    "- Isolate the clusters I'm interested in \n",
    "- Compare those clusters (cluster membership metric?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5631f78",
   "metadata": {},
   "source": [
    "To do: think about how to handle the max class - should it be the max across all classes, the max across class 3, 4, 5, 6 or the max between 3 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fb618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "\n",
    "# Import packages\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import spacy\n",
    "from deep_translator import GoogleTranslator\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b58adc",
   "metadata": {},
   "source": [
    "### Step 1: Create separate corpora\n",
    "\n",
    "Separate the main corpus into two separate ones: one where the dominant consensus class is Non-Consensus (max_class=3) and one where the dominant consensus is Full Conensus Forest (max_class=6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c41fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: SEPARATE CORPORA\n",
    "\n",
    "# Load the master CSV from rq2_step1_data_collection\n",
    "master = pd.read_csv(\"./processing/master.csv\")\n",
    "\n",
    "# Separate the master corpus into 2 corpora for class 3 and 6\n",
    "corpus_3 = master[master[\"max_class\"] == 3]\n",
    "corpus_6 = master[master[\"max_class\"] == 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec94da7",
   "metadata": {},
   "source": [
    "### Step 2: Create Tokens\n",
    "\n",
    "Same steps as in RQ2\n",
    "\n",
    "for now I've just duplicated everything for class3 and class6, but ideally I will come back to this to make it all more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c4f1c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_22336\\1390292830.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corpus_3.fillna(\"['None']\", inplace=True)\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_22336\\1390292830.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corpus_6.fillna(\"['None']\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# STEP 2.1: CLEAN\n",
    "\n",
    "# Add ['None'] to any blank rows\n",
    "# this is necessary for the next step, but then they will be removed later\n",
    "corpus_3.fillna(\"['None']\", inplace=True)\n",
    "corpus_6.fillna(\"['None']\", inplace=True)\n",
    "\n",
    "# Extract the description and captitions and combine them into a single column\n",
    "raw_text_c3 = pd.DataFrame()\n",
    "raw_text_c3[\"desc_capt\"] = corpus_3[\"description text\"] + \" \" + corpus_3[\"photo_captions\"]\n",
    "raw_text_c6 = pd.DataFrame()\n",
    "raw_text_c6[\"desc_capt\"] = corpus_6[\"description text\"] + \" \" + corpus_6[\"photo_captions\"]\n",
    "\n",
    "# Now remove all the ['None'] text from both columns\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\['None'\\]\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\['None'\\]\", \"\", regex=True)\n",
    "\n",
    "# Remove certain special characters\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\[\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\]\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\'\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\|\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\\\\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\/\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\+\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"=\", \"\", regex=True)\n",
    "\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\[\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\]\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\'\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\|\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\\\\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\/\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\+\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"=\", \"\", regex=True)\n",
    "\n",
    "# This is to address a specific issue in one of the entries\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\n\", \" \", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\n\", \" \", regex=True)\n",
    "\n",
    "# Create a list from the column\n",
    "raw_text_c3_list = raw_text_c3[\"desc_capt\"].astype(str).values.tolist()\n",
    "raw_text_c6_list = raw_text_c6[\"desc_capt\"].astype(str).values.tolist()\n",
    "\n",
    "# Convert entries which are just a space (\" \") to be empty (\"\")\n",
    "raw_text_c3_list = [x.strip(' ') for x in raw_text_c3_list]\n",
    "raw_text_c6_list = [x.strip(' ') for x in raw_text_c6_list]\n",
    "\n",
    "# Remove all empty entries\n",
    "raw_text_c3_list = list(filter(None, raw_text_c3_list))\n",
    "raw_text_c6_list = list(filter(None, raw_text_c6_list))\n",
    "\n",
    "\n",
    "# Check\n",
    "#raw_text_c6_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e476b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Breite Forstwege und Pfade. Auf halber Strecke kann man im Bergwerkstüble einkehren (Montag Ruhetag)',\n",
       " 'Rinken - Feldberg-Ort Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto',\n",
       " 'Schauinsland hike Foto',\n",
       " 'Feldbergerhof - Feldberg-Ort Foto, Foto, Foto, Foto, Foto, Foto',\n",
       " 'Ruta circular apta para todo tipo de personas. El pico es redondeado y ofrece una buenas vistas de la selva negra y la vertiente Norte de los Alpes suizos, austriacos y montañas de Liechtenstein.  Feldberg (1493 m)., Techo de la Selva Negra (Alemania)',\n",
       " 'Bifurcación izquierda, Lago Feldsee, Lago. Banco, Cima, Monolito',\n",
       " 'Titisee Camping Bankenhof',\n",
       " 'Estany a Ruhestein',\n",
       " 'Von St. Bartholomä zunächst zur schönen Eiskapelle am Fuss der Watzmann Ostwand, dann auf gleichem Weg zurück nach St. Bartholomä. Dann entlang des Uferns Richtung Norden, bald beginnt der Rinnkendlsteig durch den Wald und wird oben zunehmend ausgesetzter. Wanderer mit Höhenangst sollten diesen Steig besser nicht gehen. Man erreicht schließlich den Aussichtspunkt Archenkanzel mit schöner Aussicht auf den Königssee und von dort in ca. 20 Minuten die schön gelegene Kührointalm mit prächtigem Blick auf den Watzmann.',\n",
       " 'this hike starts at the Altschönau parking lot and makes a big loop through the zoo. the path is accessible for strollers and wheelchairs and is cleared in winter. halfway we pass by the main entrance where you can sit down in the cafeteria for a hot drink and a snack.',\n",
       " 'Gondel als Aufstiegshilfe, danach aussichts- und gipfelreiche Wanderung am Spitzingsee.  Taubensteinbahn Bergstatiom, Rauhkopf (1.689m), Rauhkopf Gipfel (summit), Route 642 (easy) ... slippery when wet, View towards Benzingspitze and Jaegerkamp, Rauhkopf (front), Hochmiesing (background left), Rotwand (background right), Hut w Aiplspitz behind, Aiplspitze, X-ing Tracks 642643, Foto, Rauhkopf, Taubensteinbahn Bergstation, Rotwand von Benzingspitz, Foto, View from Jaegerkamp: Lake Schliersee, Jaegerkamp summit, Jaegerkamp (Background: Aiplspitze), X-ing 643642Jägerkamp, Foto, Foto, Foto, End of tarmack road, Decision time: Back to Bergstation or... down the road to Lake Spitzing... (each path 45m), don‘t follow the road, sharp right to the woods instead, Foto, Turn left to Taubensteinbahn Talstation, Foto, almost done, Time to clean your shoes and poles',\n",
       " 'Serbisch deutsch Geretsried',\n",
       " 'Passau Bratislava Prolog',\n",
       " 'From Vilde Rose lodging to Geretsried and back',\n",
       " 'Winkelmoosalm Wildalm Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto',\n",
       " 'Schloss Linderhof Graswang retour Schlosspark Linderhof , Schlosspark Linderhof , Schloß Linderhof',\n",
       " '39.09.18 Nymphenburger Park Foto, Foto, Foto, Foto',\n",
       " 'Good hikking Foto, Foto, Foto, Foto, Foto, Unterberg, Foto, Ettenberg',\n",
       " 'Kemptner Hütte - Memminger Hütte',\n",
       " 'St Bartholomä - Eiskapelle',\n",
       " '08.07.18 Nymphenburger Park Linde',\n",
       " '07.01.18 Nymphenburger Park Foto',\n",
       " '30.01.18 Deininger Weiher Foto',\n",
       " 'Silvan und Karlshöhe',\n",
       " 'An der Havel entlang zur Pfaueninsel Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto',\n",
       " 'Lange abenteuer Tour.',\n",
       " 'Heidschnuckenweg 1. Etappe',\n",
       " 'Wandertag der Wanderfreunde Frohnhausen 2018',\n",
       " 'Lahn Foto, Foto, Hose, Im Wohnzimmer die Jogginghose, Foto',\n",
       " 'Kurze Riunde durch den antionalpark Harz',\n",
       " 'Einfache Wanderung von Torfhaus zum Oderteich und über den Märchenweg zurück.',\n",
       " 'Schneverdingen: Pietzmoor & Osterheide Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto',\n",
       " 'Wandelen door moeilijk begaanbaar gebied, bij vloed mogelijk niet begaanbaar',\n",
       " 'Lengte : 2.87 km  \"53°36.164 6°45.997\"',\n",
       " 'Norderney Wrack on GPSies.com',\n",
       " 'Wanderung durch Moor. Man soll gute Schuhe haben welche nass werden können.',\n",
       " 'Rundgang Meyenburg Wassermühle',\n",
       " 'Heeseberg FEMO Rundweg, Start am Parkplatz und der Gaststätte, vorbei am Turm in Richtung Westen. Man besucht 3 Steinbrüche, erfährt viel über die Erdgeschichte. Auf einer aussichtsreichen Bank nimmt man im Laufe der kleinen Wanderung und genießt die schöne Sicht Richtung Süden und staunt über die Steilheit des Heeseberg Südhangs.',\n",
       " 'Very nice 10 km tour up on the Ems. Easy going, nice wildlife',\n",
       " 'Easy hike through the Eifel forest. We took the \"hard\" route at first but then took the route that is closer to the water. If we would have more time we would have gone further more. Great hike for mountain bikes too.',\n",
       " 'With train from Heimbach to Obermaubach and hike back partially over wilderness trail',\n",
       " 'Lahr - Bad Honnef',\n",
       " 'Durch die Länge ist die Strecke anspruchsvoll',\n",
       " 'Leuke korte wandeling. Makkelijk begaanbaar. Door heide en bos',\n",
       " 'Wanderung entlang des Laacher Sees nach Maria Laach und dann weiter nach Nickenich und zurück zum Campingplatz.',\n",
       " 'Rhine Loreley Walk Photo, Photo, Photo, Photo, Photo, Photo, Photo',\n",
       " 'Chemin des moulins',\n",
       " 'Dreimullerwasserfalle nohn Photo',\n",
       " 'Feierabend Runde Donnersberg Falkenstein',\n",
       " 'Vom Golfplatz zur Dreispitz im Rundweg zurück',\n",
       " 'Rheinsteig 1 28-1-18 Foto, Foto, Foto, Foto, Foto',\n",
       " 'Altenahr meets Kalenborn',\n",
       " 'Day 13 - Neuerburg to Grevenmacher',\n",
       " 'Leichtes Lauftraining Weg, Speicherbecken Lohsa',\n",
       " 'Die Schrammsteinaussicht ist ein beliebter Aussichtspunkt auf den Schrammsteinfelsen und bietet sagenhafte Blicke auf die Sandsteinfelsen und die Landschaft des Elbsandsteingebirges im Nationalpark Sächsiche Schweiz. Der Wanderweg führt vom Campingplatz Ostrauer Mühle im Kirnitschtal vorbei am Falkenstein, durch das Schrammtor und über den Wildschützensteig und dann hoch auf die Schrammsteine bis zur Schrammstein-Aussicht. Zurück gehe ich noch ein Stück entlang des Schrammsteingradweg und dann runter über den Mittelweg.',\n",
       " 'Honigsteine Lokomotive Schwedenhöhle',\n",
       " 'Stolberg Unterer Bandweg',\n",
       " 'Wanderung vom Hexentanzplatz nach Thale zur Talstation der Gondelbahn, mit der man wieder zurück zum Ausgangspunkt kommt.',\n",
       " 'Teilweise fehlt der Weg bzw ist zugewuchert... Naturschutzgebiet',\n",
       " 'Winterwandeling in sneeuw',\n",
       " 'Geltinger Birk, nature reserve, konik, free running cattle, Baltic sea, Denmark, Noor, P',\n",
       " 'Geltinger Birk Foto, Foto, Foto',\n",
       " 'Schöne Radtour auf dem Deich.  Foto',\n",
       " 'Rund um den Truppenübungsplatz Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto',\n",
       " 'List - Ellbogenberg Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto, Foto',\n",
       " 'Start an der Uwe Düne. Rotes Kliff von oben, Stärkung bei Gosch mit drei Canapés und einem Prosecco für 7,50. Foto',\n",
       " 'This walk was from Jena to the Hill top Napoleon Stone which commemorates the battle of 14th October 1806, then on to the 1806 museum and back to Jena Jena in the mist!, Jena in the mist, Photo, Map to Napoleon Stone , Napoleon Stone , Railway to house']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 2.2: FILTER OUT SHORT TEXTS\n",
    "\n",
    "# Load the spacy model\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Create an empty list to store the unique token counts for each trail\n",
    "unique_token_counts_c3 = []\n",
    "unique_token_counts_c6 = []\n",
    "\n",
    "# Tokenise the text for each trail & count the number of unique tokens for each trail\n",
    "for trail_text in raw_text_c3_list:\n",
    "    doc = nlp(trail_text)\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "    unique_tokens = set(tokens)\n",
    "    unique_token_counts_c3.append(len(unique_tokens))\n",
    "\n",
    "for trail_text in raw_text_c6_list:\n",
    "    doc = nlp(trail_text)\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "    unique_tokens = set(tokens)\n",
    "    unique_token_counts_c6.append(len(unique_tokens))\n",
    "\n",
    "# Combine results into a df\n",
    "raw_text_counts_c3 = pd.DataFrame()\n",
    "raw_text_counts_c3[\"text\"] = raw_text_c3_list\n",
    "raw_text_counts_c3[\"unique_tokens\"] = unique_token_counts_c3\n",
    "\n",
    "raw_text_counts_c6 = pd.DataFrame()\n",
    "raw_text_counts_c6[\"text\"] = raw_text_c6_list\n",
    "raw_text_counts_c6[\"unique_tokens\"] = unique_token_counts_c6\n",
    "\n",
    "# Filter df to only include rows where unique_tokens >= 3\n",
    "raw_text_counts_c3 = raw_text_counts_c3.loc[(raw_text_counts_c3[\"unique_tokens\"] >= 3)]\n",
    "raw_text_counts_c6 = raw_text_counts_c6.loc[(raw_text_counts_c6[\"unique_tokens\"] >= 3)]\n",
    "\n",
    "# Save the text column as a list for use in the next steps\n",
    "raw_text_3token_c3_list = raw_text_counts_c3[\"text\"].astype(str).values.tolist()\n",
    "raw_text_3token_c6_list = raw_text_counts_c6[\"text\"].astype(str).values.tolist()\n",
    "\n",
    "# Check\n",
    "raw_text_3token_c3_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6bf148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1325"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 2.3: DUPLICATE TEXT HANDLING\n",
    "\n",
    "# Check for duplicates and preserve order\n",
    "seen_c3 = set()\n",
    "raw_text_3token_c3_list_unq = []\n",
    "for item in raw_text_3token_c3_list:\n",
    "    if item not in seen_c3:\n",
    "        seen_c3.add(item)\n",
    "        raw_text_3token_c3_list_unq.append(item)\n",
    "\n",
    "len(raw_text_3token_c3_list_unq)\n",
    "\n",
    "seen_c6 = set()\n",
    "raw_text_3token_c6_list_unq = []\n",
    "for item in raw_text_3token_c6_list:\n",
    "    if item not in seen_c6:\n",
    "        seen_c6.add(item)\n",
    "        raw_text_3token_c6_list_unq.append(item)\n",
    "\n",
    "\n",
    "len(raw_text_3token_c6_list_unq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "838437c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "# STEP 2.4: COUNT >5000\n",
    "\n",
    "# Create starting point for count\n",
    "count_5000_c3 = 0\n",
    "count_5000_c6 = 0\n",
    "\n",
    "# Count trails with more than 5000 characters\n",
    "for trail_text in raw_text_3token_c3_list_unq:\n",
    "    if len(trail_text) > 5000:\n",
    "        count_5000_c3 += 1\n",
    "\n",
    "for trail_text in raw_text_3token_c6_list_unq:\n",
    "    if len(trail_text) > 5000:\n",
    "        count_5000_c6 += 1\n",
    "\n",
    "# Show the count\n",
    "print(count_5000_c3, count_5000_c6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5695f59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 2.5: TRUNCATE >5000 (ONLY NEEDED FOR C6)\n",
    "\n",
    "# Create a new list for the truncated trail text\n",
    "raw_text_3token_c6_list_unq_trunc = []\n",
    "\n",
    "# Truncate any text greater than 5000 characters\n",
    "# Also remove the last word in case it's only a partial word\n",
    "for trail_text in raw_text_3token_c6_list_unq:\n",
    "    if len(trail_text) > 5000:\n",
    "        trail_text_trunc = trail_text[:5000]\n",
    "        trail_text_trunc = trail_text_trunc.rsplit(' ', 1)[0]\n",
    "        raw_text_3token_c6_list_unq_trunc.append(trail_text_trunc)\n",
    "    else:\n",
    "        raw_text_3token_c6_list_unq_trunc.append(trail_text)\n",
    "\n",
    "# Check to make sure removal was successful\n",
    "# Create starting point for check count\n",
    "check_5000_c6 = 0\n",
    "\n",
    "# Count trails with more than 5000 characters in the new list\n",
    "for trail_text in raw_text_3token_c6_list_unq_trunc:\n",
    "    if len(trail_text) > 5000:\n",
    "        check_5000_c6 += 1\n",
    "\n",
    "# Show the check count\n",
    "check_5000_c6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4221def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated for C3: 28\n",
      "Skipped for C3 (already German or unchanged): 39\n",
      "Percent translated for C3: 41.79\n",
      "Translated for C6: 540\n",
      "Skipped for C6 (already German or unchanged): 785\n",
      "Percent translated for C6: 40.75\n"
     ]
    }
   ],
   "source": [
    "# STEP 2.6: TRANSLATE TO GERMAN (RUN ONCE)\n",
    "# TAKES ABOUT 10 MIN\n",
    "\n",
    "raw_text_de_c3 = []\n",
    "translated_count_c3 = 0\n",
    "skipped_count_c3 = 0\n",
    "\n",
    "raw_text_de_c6 = []\n",
    "translated_count_c6 = 0\n",
    "skipped_count_c6 = 0\n",
    "\n",
    "# Use deep translator to automatically detect language and translate to German\n",
    "# If German is detected it will skip the entry (ie. it will not try to translate)\n",
    "for trail_text in raw_text_3token_c3_list_unq:\n",
    "    translated_c3 = GoogleTranslator(source='auto', target='de').translate(text=trail_text)\n",
    "\n",
    "    # Check if translation changed anything\n",
    "    if translated_c3.strip() == trail_text.strip():\n",
    "        skipped_count_c3 += 1\n",
    "    else:\n",
    "        translated_count_c3 += 1\n",
    "\n",
    "    raw_text_de_c3.append(translated_c3)\n",
    "\n",
    "for trail_text in raw_text_3token_c6_list_unq_trunc:\n",
    "    translated_c6 = GoogleTranslator(source='auto', target='de').translate(text=trail_text)\n",
    "\n",
    "    # Check if translation changed anything\n",
    "    if translated_c6.strip() == trail_text.strip():\n",
    "        skipped_count_c6 += 1\n",
    "    else:\n",
    "        translated_count_c6 += 1\n",
    "\n",
    "    raw_text_de_c6.append(translated_c6)\n",
    "\n",
    "print(f\"Translated for C3: {translated_count_c3}\")\n",
    "print(f\"Skipped for C3 (already German or unchanged): {skipped_count_c3}\")\n",
    "print(f\"Percent translated for C3: {round(((translated_count_c3/(translated_count_c3+skipped_count_c3))*100),2)}\")\n",
    "\n",
    "print(f\"Translated for C6: {translated_count_c6}\")\n",
    "print(f\"Skipped for C6 (already German or unchanged): {skipped_count_c6}\")\n",
    "print(f\"Percent translated for C6: {round(((translated_count_c6/(translated_count_c6+skipped_count_c6))*100),2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "471e4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2.7: SAVE TRANSLATED TEXT\n",
    "\n",
    "pickle.dump(raw_text_de_c3, open(\"./processing/raw_text_de_c3.p\", \"wb\"))\n",
    "pickle.dump(raw_text_de_c6, open(\"./processing/raw_text_de_c6.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "920dab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2.8: LOAD TRANSLATED TEXT\n",
    "\n",
    "raw_text_de_c3 = pickle.load(open(\"./processing/raw_text_de_c3.p\", \"rb\"))\n",
    "raw_text_de_c6 = pickle.load(open(\"./processing/raw_text_de_c6.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64493fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2.9 TOKENISATION\n",
    "\n",
    "# Load the spacy model\n",
    "nlp_rq3 = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Create an empty list to store the tokens\n",
    "doc_c3 = []\n",
    "doc_c6 = []\n",
    "\n",
    "# Tokenise the raw_text input\n",
    "for string in raw_text_de_c3:\n",
    "    doc_c3.extend(nlp_rq3(string))\n",
    "\n",
    "for string in raw_text_de_c6:\n",
    "    doc_c6.extend(nlp_rq3(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ac41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2.10: FILTERING TOKENS (STOP WORDS ETC)\n",
    "\n",
    "# Add words to stop list\n",
    "nlp_rq3.vocab[\"Foto\"].is_stop = True\n",
    "nlp_rq3.vocab[\"foto\"].is_stop = True\n",
    "nlp_rq3.vocab[\"FOTO\"].is_stop = True\n",
    "nlp_rq3.vocab[\"Fotos\"].is_stop = True\n",
    "nlp_rq3.vocab[\"Photo\"].is_stop = True\n",
    "nlp_rq3.vocab[\"null\"].is_stop = True\n",
    "nlp_rq3.vocab[\"Waypoint\"].is_stop = True\n",
    "\n",
    "# Filter out tokens that are stop words (is_stop), puncutation (is_punct), \n",
    "# numbers (is_digit & like_num) OR spaces (is_space)\n",
    "# option use token.lemma_ to extract the lemma for the final tokens\n",
    "filtered_tokens_c3 = [token.text for token in doc_c3 if not token.is_stop | token.is_punct | \n",
    "                     token.is_digit | token.like_num | token.is_space]\n",
    "\n",
    "filtered_tokens_c6 = [token.text for token in doc_c6 if not token.is_stop | token.is_punct | \n",
    "                     token.is_digit | token.like_num | token.is_space]\n",
    "\n",
    "# Empty list for lower-case versions\n",
    "filtered_tokens_lc_c3 = []\n",
    "filtered_tokens_lc_c6 = []\n",
    "\n",
    "# Convert to lower-case \n",
    "for token in filtered_tokens_c3:\n",
    "    token_lc_c3 = token.lower()\n",
    "    filtered_tokens_c3.append(token_lc_c3)\n",
    "\n",
    "for token in filtered_tokens_c6:\n",
    "    token_lc_c6 = token.lower()\n",
    "    filtered_tokens_c6.append(token_lc_c6)\n",
    "\n",
    "# Check\n",
    "#print(filtered_tokens_c3, filtered_tokens_c6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21643832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2.11: CHECK SOME RESULTS :)\n",
    "\n",
    "word_freq_c3 = Counter(filtered_tokens_c3)\n",
    "common_words_c3 = word_freq_c3.most_common(20)\n",
    "\n",
    "word_freq_c6 = Counter(filtered_tokens_c6)\n",
    "common_words_c6 = word_freq_c6.most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca0c03",
   "metadata": {},
   "source": [
    "### Step 3: Pre-processing\n",
    "\n",
    "Assign each token their cluster from rq2_step2_text_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079aad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: LOAD TOKEN/CLUSTER LOOKUP FROM RQ2\n",
    "\n",
    "token_cluster_lookup = pickle.load(open(\"./processing/token_cluster_all.p\", \"rb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
