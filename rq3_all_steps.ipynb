{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "331c85f0",
   "metadata": {},
   "source": [
    "## RQ3 Full Process\n",
    "\n",
    "General plan:\n",
    "- Separate the main corpus into two separate ones: one where the dominant consensus class is Non-Consensus (max_class=3) and one where the dominant consensus is Full Conensus Forest (max_class=6).\n",
    "- Perform the same text processing as in rq2_step2_text_analysis.ipynb to generate the tokens\n",
    "- Assign each token their cluster from rq2_step2_text_analysis.ipynb\n",
    "- Isolate the clusters I'm interested in \n",
    "- Compare those clusters (cluster membership metric?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5631f78",
   "metadata": {},
   "source": [
    "To do: think about how to handle the max class - should it be the max across all classes, the max across class 3, 4, 5, 6 or the max between 3 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "\n",
    "# Import packages\n",
    "import pandas as pd\n",
    "\n",
    "import spacy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b58adc",
   "metadata": {},
   "source": [
    "### Step 1: Create separate corpora\n",
    "\n",
    "Separate the main corpus into two separate ones: one where the dominant consensus class is Non-Consensus (max_class=3) and one where the dominant consensus is Full Conensus Forest (max_class=6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: SEPARATE CORPORA\n",
    "\n",
    "# Load the master CSV from rq2_step1_data_collection\n",
    "master = pd.read_csv(\"./processing/master.csv\")\n",
    "\n",
    "# Separate the master corpus into 2 corpora for class 3 and 6\n",
    "corpus_3 = master[master[\"max_class\"] == 3]\n",
    "corpus_6 = master[master[\"max_class\"] == 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec94da7",
   "metadata": {},
   "source": [
    "### Step 2: Pre-processing\n",
    "\n",
    "Same steps as in RQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: CLEAN\n",
    "\n",
    "# Add ['None'] to any blank rows\n",
    "# this is necessary for the next step, but then they will be removed later\n",
    "corpus_3.fillna(\"['None']\", inplace=True)\n",
    "corpus_6.fillna(\"['None']\", inplace=True)\n",
    "\n",
    "# Extract the description and captitions and combine them into a single column\n",
    "raw_text_c3 = pd.DataFrame()\n",
    "raw_text_c3[\"desc_capt\"] = corpus_3[\"description text\"] + \" \" + corpus_3[\"photo_captions\"]\n",
    "raw_text_c6 = pd.DataFrame()\n",
    "raw_text_c6[\"desc_capt\"] = corpus_6[\"description text\"] + \" \" + corpus_6[\"photo_captions\"]\n",
    "\n",
    "# Now remove all the ['None'] text from both columns\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\['None'\\]\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\['None'\\]\", \"\", regex=True)\n",
    "\n",
    "# Remove certain special characters\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\[\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\]\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\'\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\|\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\\\\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\/\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\+\", \"\", regex=True)\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"=\", \"\", regex=True)\n",
    "\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\[\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\]\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\'\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\|\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\\\\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\/\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\+\", \"\", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"=\", \"\", regex=True)\n",
    "\n",
    "# This is to address a specific issue in one of the entries\n",
    "raw_text_c3[\"desc_capt\"] = raw_text_c3[\"desc_capt\"].str.replace(r\"\\n\", \" \", regex=True)\n",
    "raw_text_c6[\"desc_capt\"] = raw_text_c6[\"desc_capt\"].str.replace(r\"\\n\", \" \", regex=True)\n",
    "\n",
    "# Create a list from the column\n",
    "raw_text_c3_list = raw_text_c3[\"desc_capt\"].astype(str).values.tolist()\n",
    "raw_text_c6_list = raw_text_c6[\"desc_capt\"].astype(str).values.tolist()\n",
    "\n",
    "# Convert entries which are just a space (\" \") to be empty (\"\")\n",
    "raw_text_c3_list = [x.strip(' ') for x in raw_text_c3_list]\n",
    "raw_text_c6_list = [x.strip(' ') for x in raw_text_c6_list]\n",
    "\n",
    "# Remove all empty entries\n",
    "raw_text_c3_list = list(filter(None, raw_text_c3_list))\n",
    "raw_text_c6_list = list(filter(None, raw_text_c6_list))\n",
    "\n",
    "\n",
    "# Check\n",
    "#raw_text_c6_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e476b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: FILTER OUT SHORT TEXTS\n",
    "\n",
    "# Load the spacy model\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Create an empty list to store the unique token counts for each trail\n",
    "unique_token_counts = []\n",
    "\n",
    "# Tokenise the text for each trail & count the number of unique tokens for each trail\n",
    "for trail_text in raw_text_list:\n",
    "    doc = nlp(trail_text)\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "    unique_tokens = set(tokens)\n",
    "    unique_token_counts.append(len(unique_tokens))\n",
    "\n",
    "# Combine results into a df\n",
    "raw_text_counts = pd.DataFrame()\n",
    "raw_text_counts[\"text\"] = raw_text_list\n",
    "raw_text_counts[\"unique_tokens\"] = unique_token_counts\n",
    "\n",
    "# Filter df to only include rows where unique_tokens >= 3\n",
    "raw_text_counts = raw_text_counts.loc[(raw_text_counts[\"unique_tokens\"] >= 3)]\n",
    "\n",
    "# Save the text column as a list for use in the next steps\n",
    "raw_text_3token_list = raw_text_counts[\"text\"].astype(str).values.tolist()\n",
    "\n",
    "# Check\n",
    "raw_text_3token_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
