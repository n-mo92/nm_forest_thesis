{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2 Data Collection\n",
    "\n",
    "\n",
    "[Add description]\n",
    "\n",
    "Wikiloc data extraction for Germany\n",
    "\n",
    "Python code provided by Chai-Allah et al, 2023 through their GitHub repo: https://github.com/achaiallah-hub/Wiki4CES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-up\n",
    "\n",
    "To run scrapy you need to set up a scrapy \"project\". In Anaconda Prompt:\n",
    "\n",
    "1. cd C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\n",
    "2. conda activate C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda \n",
    "3. scrapy startproject wikiloc_scrapy\n",
    "\n",
    "Helpful scrapy information: https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    "\n",
    "Then you need to add the scrapy \"spiders\" (the processes which crawl and scrape information from the web) as .py scripts to the directory wikiloc_scrapy/wikiloc_scrapy/spiders. Basically, the .py scripts from the achaiallah-hub repo need to be saved in this directory. In Anaconda Prompt:\n",
    "\n",
    "1. cd wikiloc_scrapy\\wikiloc_scrapy\\spiders\n",
    "2. git init\n",
    "3. git remote add origin git@github.com:achaiallah-hub/Wiki4CES.git\n",
    "4. git pull origin main\n",
    "\n",
    "**NOTE** Instead of Steps 2-4 I first just tried the simple command git clone https://github.com/achaiallah-hub/Wiki4CES.git This works, but it stores the .py scripts inside a repo directory folder called Wiki4CES, and I think this causes problems when trying to use the spiders later. Steps 2-4 are a work-around: this way the python scripts are directly within the \"spiders\" directory without being inside another directory. In order to do steps 2-4 an **ssh-key** needs to be set up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what I understand, the spiders provided by achaiallah-hub do the following:\n",
    "1. **extract_link.py** Extracts the URLS for all the trails. You give it a starting point (an intital URL) and it goes through each city/town and extracts all the trail links (URLS) in the cities listing. It stores each link as {\"Link\": link}. Needs to be run first to get the URLS for steps 2 and 3. \n",
    "2. **wikiloc_track.py** Scrapes the trail details like track name, difficulty, distance, author, views and description. It loads the trail URLs from a file called link.csv (presumably created in step 1)\n",
    "3. **wikiloc_image.py** Scrapes image data from the trail pages, including URL, track name, user name, date, and location (latitude & longitude). It reads the trail pages from a file called link.csv (presumably created in step 1)\n",
    "4. **download_image.py** Downloads images from the URLS in a csv file called wikiloc_image.csv (presumably this would be created from step 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: extract_link.py\n",
    "\n",
    "Edit the extract_link.py to replace the staring_urls. Originally this contained https://www.wikiloc.com/trails/france/auvergne-rhone-alpes - this URL doesn't seem to exist anymore as it just redirects to https://www.wikiloc.com/trails/outdoor. \n",
    "\n",
    "My guess is that the URL format now needs to be https://www.wikiloc.com/trails/outdoor/ + *country_name* + *region_name* so for Germany I will try https://www.wikiloc.com/trails/outdoor/germany and then each of the regions within (I think I probably need to do each separately as that's the way the code was written for the original project).\n",
    "\n",
    "For Germany, there are the following regions:\n",
    "\n",
    "| Count | Region                 | Approx # Trails | URL ending              | \n",
    "| ----- | ---------------------- | --------------- | ----------------------- | \n",
    "| 1     | Baden-Wurttemberg      | 76500           | /baden-wurttemberg      | \n",
    "| 2     | Bavaria                | 77900           | /bavaria                |\n",
    "| 3     | Brandenburg            | 8240            | /brandenburg            |\n",
    "| 4     | Bremen                 | 1450            | /bremen                 |\n",
    "| -     | DE.16,11               | 5               | (don't use)             |\n",
    "| 5     | Hamburg                | 7160            | /hamburg                |\n",
    "| 6     | Hessen                 | 25800           | /hessen                 |\n",
    "| 7     | Mecklenburg-Vorpommern | 6250            | /mecklenburg-vorpommern |\n",
    "| 8     | Niedersachsen          | 30900           | /niedersachsen          |\n",
    "| 9     | Nordrhein-Westfalen    | 84500           | /nordrhein-westfalen    |\n",
    "| 10    | Rheinland-Pfalz        | 54600           | /rheinland-pfalz        |\n",
    "| 11    | Saarland               | 5790            | /saarland               |\n",
    "| 12    | Sachsen                | 20700           | /sachsen                |\n",
    "| 13    | Saxony-Anhalt          | 7070            | /saxony-anhalt          |\n",
    "| 14    | Schleswig-Holstein     | 9510            | /schleswig-holstein     |\n",
    "| 15    | Th√ºringen              | 5680            | /thuringen              |\n",
    "\n",
    "APPROX 434000 trails total (according to website - the sum here is 422050 but there's some rounding going on)\n",
    "\n",
    "DE.16,11 appears to be a few trails in Berlin - I don't think I need to bother with this as there is so few and in an urban area (and all the routes don't really look like anything to do with forests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Anaconda Prompt (with conda environment activated and from the scrapy project's top level directory):\n",
    "1. scrapy crawl wiki -o link.csv\n",
    "\n",
    "**NOTE:** For some reason, this seems to try to run all the spiders at once, and you end up getting error messages saying certain files don't exist (which makes sense as these files need to be created by certain spiders first). I tried looking for the solution for this, but for now I've just commented out the code within the other spiders.\n",
    "\n",
    "**PROBLEM** HTTP Status Code 403 - Forbidden / Access Denied - could this be anti-scraping measure?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
