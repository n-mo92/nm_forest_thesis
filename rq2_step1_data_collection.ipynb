{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2 Data Collection\n",
    "\n",
    "Wikiloc data extraction for Germany. **See scrapy_setup_info.md for setting up scrapy, including edits I made to the settings to fix 403 error messages and make the scraping more polite.**\n",
    "\n",
    "scrapy spiders are provided by Chai-Allah et al, 2023 through their GitHub repo: [Wiki4CES](https://github.com/achaiallah-hub/Wiki4CES)\n",
    "\n",
    "From what I understand, the spiders provided in the Wiki4CES repo do the following:\n",
    "1. **extract_link.py** Extracts the URLS for all the trails. You give it a starting region (an intital URL) and it goes through each city/town in that region and extracts all the trail links (URLS) in the cities listing. This spider neeeds to be run first to get the URLS for steps 2 and 3. \n",
    "2. **wikiloc_track.py** Scrapes the trail details like track name, difficulty, distance, author, views and description. It loads the trail URLs from a file called link.csv (presumably created in step 1)\n",
    "3. **wikiloc_image.py** Scrapes image data from the trail pages, including URL, track name, user name, date, and location (latitude & longitude). It reads the trail pages from a file called link.csv (presumably created in step 1)\n",
    "4. **download_image.py** Downloads images from the URLS in a csv file called wikiloc_image.csv (presumably this would be created from step 3). Not needed for my work, so I have removed this file\n",
    "\n",
    "**NOTE:** For some reason, running one spider seems to try to run all the spiders at once, and you end up getting error messages saying certain files don't exist (which makes sense as these files need to be created by certain spiders first). I tried looking for the solution for this, but for now I've just commented out the code within the other spiders. UPDATE: It seems to be okay once the errors have been resolved (it doesn't actually run the other spiders but seem to check for the correct files and the code being valid?), so I'm leaving finished scripts uncommented as I correct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs already exists\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "\n",
    "# Import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import shapely\n",
    "from shapely.geometry import box\n",
    "import geopandas as gpd\n",
    "\n",
    "# Create folders for storing scrapy outputs\n",
    "path_list = [\"./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs\"]\n",
    "\n",
    "for path in path_list:\n",
    "  if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print(\"Folder %s created!\" % path)\n",
    "  else:\n",
    "    print(\"Folder %s already exists\" % path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Updating extract_link.py\n",
    "\n",
    "**Step 1a: Update starting_urls (including Germany region info)** \n",
    "Edit the extract_link.py to replace the staring_urls. Originally this contained https://www.wikiloc.com/trails/france/auvergne-rhone-alpes - this URL doesn't seem to exist anymore as it just redirects to https://www.wikiloc.com/trails/outdoor. \n",
    "\n",
    "The URL format now needs to be https://www.wikiloc.com/trails/outdoor/ + *country_name* + *region_name* so for Germany I will try https://www.wikiloc.com/trails/outdoor/germany and then each of the regions within.\n",
    "\n",
    "For Germany, Wikiloc has trails for the following regions:\n",
    "\n",
    "| Count | Region                 | URL ending              |\n",
    "| ----- | ---------------------- | ----------------------- | \n",
    "| 1     | Baden-Wurttemberg      | /baden-wurttemberg      | \n",
    "| 2     | Bavaria                | /bavaria                |\n",
    "| 3     | Berlin                 | /berlin                 |\n",
    "| 4     | Brandenburg            | /brandenburg            |\n",
    "| 5     | Bremen                 | /bremen                 |\n",
    "| -     | DE.16,11               | (don't use)             |\n",
    "| 6     | Hamburg                | /hamburg                |\n",
    "| 7     | Hessen                 | /hessen                 |\n",
    "| 8     | Mecklenburg-Vorpommern | /mecklenburg-vorpommern |\n",
    "| 9     | Niedersachsen          | /niedersachsen          |\n",
    "| 10    | Nordrhein-Westfalen    | /nordrhein-westfalen    |\n",
    "| 11    | Rheinland-Pfalz        | /rheinland-pfalz        |\n",
    "| 12    | Saarland               | /saarland               |\n",
    "| 13    | Sachsen                | /sachsen                |\n",
    "| 14    | Saxony-Anhalt          | /saxony-anhalt          |\n",
    "| 15    | Schleswig-Holstein     | /schleswig-holstein     |\n",
    "| 16    | ThÃ¼ringen              | /thuringen              |\n",
    "\n",
    "*NOTE* The number of trails being added seem to be increasing steadily (for example, within a one week period, the total count for Germany went up by ~1000). I'll need to keep track of the number of expected trails on the day of download. Also check to make sure no new regions are added!\n",
    "\n",
    "DE.16,11 appears to be a few trails in Berlin - I don't think I need to bother with this as there is so few and in an urban area (and all the routes don't really look like anything to do with forests)\n",
    "\n",
    "**Step 1b: Update xpath expressions & other edits**\n",
    "After overcoming initial 403 error messages (see scrapy_setup_info.md), the spider seemed to correctly generate the urls for all the cities within the region, but still didn't return the trail URLs. I started looking into the xpath expressions in the extract_link.py as I wondered if the path structure has changed a bit over time (like the URLs).\n",
    "\n",
    "I found this video useful for understanding xpath https://www.youtube.com/watch?v=4EvxqTSzUkI \n",
    "I then went to https://www.wikiloc.com/trails/outdoor/germany/bremen and did rick click > Inspect to see the html (I selected Bremen as the testing region as it has the fewest trails). After a search for the components on the main Bremen page and then for one city (for example: https://www.wikiloc.com/trails/outdoor/germany/bremen/alte-neustadt) I made a couple changed to the xpaths in extract_link.py (see comments in script). I also made some changes to the pagination handling (see comments in script). ALSO, since the URLs saved initially were just the back half of the URL, without the beginning (eg. /cycling-trails/bremen-achim-18077390) I adjusted the code to add the beginning part as well. This ended up being required for the other spiders to work properly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Complete workflow for running extract_link.py\n",
    "\n",
    "*For now, this is just for the Bremen region.*\n",
    "\n",
    "**Step 2a**\n",
    "In extract_link.py:\n",
    "1. Update start_urls: 'https://www.wikiloc.com/trails/outdoor/germany/bremen' and save.\n",
    "\n",
    "**Step 2b**\n",
    "In Anaconda Prompt:\n",
    "1. conda activate C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\n",
    "2. cd C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\wikiloc_scrapy\\wikiloc_scrapy\\spiders\n",
    "3. scrapy crawl wiki -o crawling_outputs\\link-bremen.csv\n",
    "\n",
    "**Step 2c**\n",
    "Remove duplicates: I am not sure why duplicates are occuring, but the code below simply removes any duplicates.\n",
    "\n",
    "*NOTE:* For Bremen at the time of scraping (31 MARCH 2025), the website shows 1460 trails, however I get 1532 trails (after the duplicates are removed) - this means there are an extra 72 trails. I'm not sure why this is but I wonder if it has something to do with trails which cross borders (and therefore are in more than 1 region of Germany). It could be that these trails can be searched for in both regions but are only included in the count of 1 to avoid double-counting? **I should check for duplicates across regions to make sure all trails are unique.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.wikiloc.com/hiking-trails/rundwand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.wikiloc.com/offroading-trails/30-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.wikiloc.com/hiking-trails/rund-alt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.wikiloc.com/hiking-trails/alt-burl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.wikiloc.com/running-trails/alt-wol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30387</th>\n",
       "      <td>https://www.wikiloc.com/hiking-trails/altenau-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30388</th>\n",
       "      <td>https://www.wikiloc.com/hiking-trails/altenau-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30389</th>\n",
       "      <td>https://www.wikiloc.com/running-trails/altenau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30390</th>\n",
       "      <td>https://www.wikiloc.com/hiking-trails/oderteic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30391</th>\n",
       "      <td>https://www.wikiloc.com/hiking-trails/altenau-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30392 rows Ã 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Link\n",
       "0      https://www.wikiloc.com/hiking-trails/rundwand...\n",
       "1      https://www.wikiloc.com/offroading-trails/30-m...\n",
       "2      https://www.wikiloc.com/hiking-trails/rund-alt...\n",
       "3      https://www.wikiloc.com/hiking-trails/alt-burl...\n",
       "4      https://www.wikiloc.com/running-trails/alt-wol...\n",
       "...                                                  ...\n",
       "30387  https://www.wikiloc.com/hiking-trails/altenau-...\n",
       "30388  https://www.wikiloc.com/hiking-trails/altenau-...\n",
       "30389  https://www.wikiloc.com/running-trails/altenau...\n",
       "30390  https://www.wikiloc.com/hiking-trails/oderteic...\n",
       "30391  https://www.wikiloc.com/hiking-trails/altenau-...\n",
       "\n",
       "[30392 rows x 1 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 2C: Remove CSV duplicates \n",
    "\n",
    "# Create a list of the csv paths with all the scraped trail links\n",
    "link_csv_paths = glob.glob('./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs/link-*.csv')\n",
    "\n",
    "# Load csvs from list, remove duplicates and then write results to same file (overwrite)\n",
    "for csv_path in link_csv_paths:\n",
    "    loaded_csv = pd.read_csv(csv_path, sep=\"\\t\")\n",
    "    loaded_csv.drop_duplicates(inplace=True)\n",
    "    loaded_csv.to_csv(csv_path, index=False)\n",
    "\n",
    "# Check\n",
    "#link_bremen = pd.read_csv('./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs/link-bremen.csv', sep=\"\\t\")\n",
    "#link_bremen\n",
    "link_nieder = pd.read_csv('./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs/link-niedersachsen.csv', sep=\"\\t\")\n",
    "link_nieder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: updating wikiloc_track.py\n",
    "\n",
    "I edited wikiloc_track.py to update the xpaths (as with the extract_link.py - see comments in script). I also added code so that I could also scrape additional information: \n",
    "- date recorded\n",
    "- photo/waypoint captions (title and body)\n",
    "- comments\n",
    "- **photo/waypoint latitudes and longitudes**\n",
    "- **start point latitude and longitude** (unfortunately the end point is not stored in the html)\n",
    "\n",
    "Because I handled all the coordinate extraction in this script, I did not use or update the wikiloc_image.py script (and I since deleted it from my repo). I extracted all latitude values in one column, and all longitude values in another column. I can then extract the minimum and maximum values from each column in order to create a bounding box.\n",
    "\n",
    "Additionally, I removed the author extraction completely so that no personal information is collected. \n",
    "\n",
    "Although I updated the xapths for the following features, I commented them out as I don't think I'll need them for my analysis:\n",
    "- trail difficulty\n",
    "- view counts\n",
    "- download counts\n",
    "- trail length/distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Complete workflow for running wikiloc_track.py\n",
    "\n",
    "*For now, this is just for the Bremen region.*\n",
    "\n",
    "**Step 4a**\n",
    "In wikiloc_track.py:\n",
    "1. Change CSV name in start_urls to: crawling_outputs\\link-bremen.csv\n",
    "\n",
    "**Step 4b**\n",
    "In Anaconda Prompt:\n",
    "1. conda activate C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\n",
    "2. cd C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\wikiloc_scrapy\\wikiloc_scrapy\\spiders\n",
    "3. scrapy crawl wiki_track -o crawling_outputs\\track-bremen.json\n",
    "\n",
    "**Needs to be output as json** otherwise (as csv) the utf-8 encoding doesn't seem to work properly and the German special characters are not handled well. \n",
    "\n",
    "For Bremen (1532 trails), with download delays and autothrottle on, this stage takes about **65 minutes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Filter for 2018 & distance\n",
    "\n",
    "Some initial filtering can be applied to reduce the amount of data that goes through the generating geometries step.\n",
    "\n",
    "This function filters on two fields:\n",
    "1. Filter for 2018 only. Data from the year 2018 is needed so that the social media data matches the forest definition data (which is for 2018).\n",
    "2. Filter out very long trails (for now, >175km). These tend correspond to motorised transport (which, when covering large distances may be difficult to pin down to CES for forests) or unexpected use of the website/errors. For example, this trail https://www.wikiloc.com/hiking-trails/xabia-teulada-126272868 is recorded as a ~5 hour hike, but it goes from Germany to Spain. This trail already gets removed with the 2018 filter, but there may be others like it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: FILTER 2018 & DISTANCE\n",
    "\n",
    "# Create a list of the json paths with all scraped data\n",
    "track_json_paths = glob.glob('./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs/track-*.json')\n",
    "\n",
    "# Load jsons from list, select only 2018 data & trails less than certain distance, return new json\n",
    "# This outputs to the PROCESSING folder!\n",
    "def dist_year_filter(json_paths):\n",
    "    for json_path in json_paths:\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(json_path)[1] \n",
    "        # For output file naming: remove extension from input file name\n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # For output file naming: assemble the new file path for the output\n",
    "        output_path = \"./processing/\" + name_wo_ext + \"_2018_distfilter.json\" \n",
    "\n",
    "        # Load json as df\n",
    "        track_df = pd.read_json(json_path) \n",
    "\n",
    "        # Select rows where date_recorded includes \"2018\"\n",
    "        track_2018_df = track_df[track_df[\"date_recorded\"].str.contains(\"2018\")]\n",
    "\n",
    "        # Select rows where distance is less than 175 km\n",
    "        track_2018short_df = track_2018_df[track_2018_df[\"distance_km\"] < 175]\n",
    "        \n",
    "        # Save the gdf as a json\n",
    "        track_2018short_df.to_json(output_path)\n",
    "\n",
    "# Run the function\n",
    "dist_year_filter(track_json_paths)\n",
    "\n",
    "# Load the json and check\n",
    "#bremen_2018_df = pd.read_json(\"./processing/track-bremen_2018_distfilter.json\")\n",
    "#bremen_2018_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Generating geometries (bbox)\n",
    "\n",
    "In step 4, I extracted all latitude values in one column, and all longitude values in another column. I can then extract the minimum and maximum values from each column in order to create a bounding box. This bounding box contains all available coordinates (without downloading the actual gpx or kml file - which is likely more invasive to scrape from the website) - this includes the trail start coordinates and any available photo/waypoint coordinates (unfortunately the trail end coordinates were not available in the htmls).\n",
    "\n",
    "The code/function below generates a bounding box geometry for each trail within each json and outputs a shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 6: bbox geometries\n",
    "\n",
    "# Create a list of the json paths with all the filtered trails\n",
    "track2018_json_paths = glob.glob('./processing/track-*_2018_distfilter.json')\n",
    "\n",
    "# Load jsons from list, generate bbox geometries and save to processing folder as shp\n",
    "def bbox_generator(json_path_list):\n",
    "    for json_path in json_path_list:\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(json_path)[1] \n",
    "        # For output file naming: remove extension from input file name\n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # For output file naming: assemble the new file path for the output \n",
    "        output_path = \"./processing/\" + name_wo_ext + \"_bbox.shp\"\n",
    "\n",
    "        # Load json as df\n",
    "        track_df = pd.read_json(json_path)\n",
    "        \n",
    "        # Create column for xmin & xmax (lowest & highest longitude)\n",
    "        track_df[\"xmin\"] = [min(x) for x in track_df.longitudes]\n",
    "        track_df[\"xmax\"] = [max(x) for x in track_df.longitudes]\n",
    "\n",
    "        # Create column for ymin & ymax (lowest & highest latitude)\n",
    "        track_df[\"ymin\"] = [min(x) for x in track_df.latitudes]\n",
    "        track_df[\"ymax\"] = [max(x) for x in track_df.latitudes]\n",
    "\n",
    "        # Run shapely box function using new columns\n",
    "        # .apply(lambda row: ..., axis=1) runs the code after the : for each row in the df\n",
    "        # lambda simply indicates a function without a name is being used\n",
    "        track_df[\"geometry\"] = track_df.apply(lambda row: box(row[\"xmin\"], row[\"ymin\"], \n",
    "                                                              row[\"xmax\"], row[\"ymax\"]), axis=1)\n",
    "        \n",
    "        # Convert the df to a gdf\n",
    "        track_gdf = gpd.GeoDataFrame(track_df, geometry='geometry')\n",
    "\n",
    "        # Set the CRS and reproject to match other data\n",
    "        track_gdf.crs= \"EPSG:4326\"\n",
    "        track_gdf = track_gdf.to_crs(\"EPSG:3035\")\n",
    "\n",
    "        # Save the gdf as a shp\n",
    "        track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
    "\n",
    "# Run the function\n",
    "#bbox_generator(track2018_json_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: exporting the data as a shp truncates the long text fields (e.g. \"description\" \"photo_caption\", \"comments\", etc).** I also tried writing the file as a geojson, which works fine, but then when loading back in as a geodataframe, there are problems with the list structures in the \"photo_caption\" and \"comments\" fields. \n",
    "\n",
    "For now, I think the best work-around is to use the shapefile to do the spatial intersection steps, and then to **join the results back to the main json** (i.e. STEP 5 OUTPUTS, filtered for year and distance) for any text analsyis steps. I think the track URLs can be used for the join field as these are unique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- apply with axis= 1 applies the function to each row\n",
    "- lambda row sets up an anonymous function to do something for each row\n",
    "- zip pairs the items in each list by index (so the lats and longs get paired up according to their order in the list)\n",
    "- the list part converts the output from zip into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_11836\\638793300.py:31: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  bremen_2018_gdf.to_file(\"./processing/test.shp\", driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '['Pause am WÃ¼mme Deich', 'Am WÃ¼mme Deich', 'Weitere Picknick MÃ¶glichkeit mit Bank', 'Foto', 'Gastronomie mit WC', 'Gastronomie mit WC', 'Foto', 'Weitere Einkehr MÃ¶glichkeit mit ECHTErrr', 'Foto', 'Kirche St. JÃ¼rgen', 'MÃ¶glichkeit fÃ¼r Picknick auf der Wiese!', 'Eiscafe in Ritterhude', 'WÃ¼mme BrÃ¼cke', 'Dammsiel', 'Seitenwechsel Ã¼ber die WÃ¼mme']' of field photo_capt has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "### STEP 6 ALT: buffered line geometries\n",
    "\n",
    "from shapely.geometry import LineString, Point\n",
    "\n",
    "# Load tester json\n",
    "bremen_2018_df = pd.read_json(\"./processing/track-bremen_2018_distfilter.json\")\n",
    "\n",
    "# Pair-up the lats and longs into coordinates\n",
    "bremen_2018_df[\"coordinates\"] = bremen_2018_df.apply(lambda row: list(zip(row[\"longitudes\"], row[\"latitudes\"])), axis=1)\n",
    "\n",
    "# Function which creates point geom if only one coordinate pair available, otherwise line geom\n",
    "def make_geometry(coords):\n",
    "    if len(coords) == 1:\n",
    "        return Point(coords[0])\n",
    "    else:\n",
    "        return LineString(coords)\n",
    "\n",
    "# Run the make geom function on all rows\n",
    "bremen_2018_df['geometry'] = bremen_2018_df['coordinates'].apply(make_geometry)\n",
    "\n",
    "# Convert to geodataframe\n",
    "bremen_2018_gdf = gpd.GeoDataFrame(bremen_2018_df, geometry=\"geometry\")\n",
    "\n",
    "# Define projection and reproject\n",
    "bremen_2018_gdf.crs= \"EPSG:4326\"\n",
    "bremen_2018_gdf = bremen_2018_gdf.to_crs(\"EPSG:3035\")\n",
    "\n",
    "\n",
    "# Buffer all rows so that all geometries are now polygons\n",
    "# PLACE HOLDER BUFFER VALUE FOR NOW\n",
    "buffer_geoms = bremen_2018_gdf.buffer(100)\n",
    "\n",
    "# Replace geometries in gdf with buffered geometries\n",
    "bremen_2018_gdf = bremen_2018_gdf.set_geometry(buffer_geoms)\n",
    "\n",
    "# Write as shapefile for visualising\n",
    "bremen_2018_gdf.to_file(\"./processing/test_startupdate.shp\", driver=\"ESRI Shapefile\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Additional Filtering\n",
    "\n",
    "1. Decide how to handle trails with only one set of coordinates (start location) - buffer or delete? For now I have decided to buffer.\n",
    "2. Filter to only include areas which intersect with Natura 2000 areas. \n",
    "3. Filter for consensus forest and non-consensus forest\n",
    "\n",
    "\n",
    "More filtering to consider:\n",
    "1. Filter certain activity types?\n",
    "2. Remove trails without any associated text (this depends if I end up using text or just trail counts)\n",
    "3. Remove trails which have a bbox area below a certain value? (loop trails without photos/waypoints = to hard to tell where exactly the trail is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 212197253.54572135 of field area of feature 1317 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 163034282.73132238 of field area of feature 1324 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 148135562.45114201 of field area of feature 1330 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 108559250.78359126 of field area of feature 1331 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 102534729.23131719 of field area of feature 1338 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 536292765.05586529 of field area of feature 1342 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 130317477.94930455 of field area of feature 1348 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 547151440.02215445 of field area of feature 1353 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 202885488.60091645 of field area of feature 1369 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 202885488.60091645 of field area of feature 1377 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 463849393.95095873 of field area of feature 1383 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 331289476.48219168 of field area of feature 1401 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 176305889.22250551 of field area of feature 1414 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 116579076.15021797 of field area of feature 1415 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 501016728.14216536 of field area of feature 1423 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 870206000.50495434 of field area of feature 1435 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 202612766.14937976 of field area of feature 1439 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 120037342.9290338 of field area of feature 1440 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 729170324.1243825 of field area of feature 1447 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 114804211.73848741 of field area of feature 1448 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 782629547.26742852 of field area of feature 1453 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 169187146.58979112 of field area of feature 1475 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 104514174.89832713 of field area of feature 1490 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 170762790.51940772 of field area of feature 1493 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 464393751.40944386 of field area of feature 1502 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 148840615.80106822 of field area of feature 1509 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 173961073.14570695 of field area of feature 1512 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 288848887.34649658 of field area of feature 1518 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 106027093.40575466 of field area of feature 1523 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 5719795859.7616997 of field area of feature 1580 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 205602723.13571465 of field area of feature 1587 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 507151179.01741111 of field area of feature 1588 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 128395200.35232621 of field area of feature 1599 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 171702141.22789478 of field area of feature 1604 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 245929151.75869218 of field area of feature 1605 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 417859005.33040869 of field area of feature 1606 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 151504468.66905198 of field area of feature 1608 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 341212970.86092383 of field area of feature 1610 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 434379462.77697086 of field area of feature 1611 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 256895127.51745525 of field area of feature 1616 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 170933411.4063578 of field area of feature 1622 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 429805131.11696565 of field area of feature 1624 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 259262009.20256689 of field area of feature 1632 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 258473765.96019384 of field area of feature 1637 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 320135298.69060123 of field area of feature 1638 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 121167090.51411597 of field area of feature 1639 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 307128257.17252338 of field area of feature 1643 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 287231087.71534491 of field area of feature 1645 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 416494809.29698592 of field area of feature 1654 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 321769939.26398772 of field area of feature 1677 not successfully written. Possibly due to too larger number with respect to field width\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: BUFFER SINGLE COORDINATE GEOMETRIES\n",
    "\n",
    "# Create a list of the json paths with all the filtered trails\n",
    "track2018_bbox_paths = glob.glob(\"./processing/track-*_2018_distfilter_bbox.shp\")\n",
    "\n",
    "def single_coord_trails(shp_path_list):\n",
    "    for bboxshp in shp_path_list:\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(bboxshp)[1] \n",
    "        # For output file naming: remove extension from input file name\n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # For output file naming: assemble the new file path for the output \n",
    "        output_path = \"./processing/\" + name_wo_ext + \"+buffer.shp\"\n",
    "\n",
    "        # Load shp as df and calculate the area of all geoms\n",
    "        allgeoms = gpd.read_file(bboxshp)\n",
    "        allgeoms[\"area\"] = allgeoms.area\n",
    "\n",
    "        # Extract the rows where the area = 0 and where area > 0\n",
    "        single_coord = allgeoms[allgeoms[\"area\"] == 0.0]\n",
    "        bbox = allgeoms[allgeoms[\"area\"] > 0.0]\n",
    "        \n",
    "        # Generate new geometries for the single coordinate trails based on trail distance\n",
    "        single_coord_geoms = single_coord.buffer(single_coord[\"distance_k\"])\n",
    "\n",
    "        # Replace existing geometry with new geometry for single coordinate trails\n",
    "        single_coord = single_coord.set_geometry(single_coord_geoms)\n",
    "\n",
    "        # Combine the new buffered points with the bbox areas\n",
    "        allgeoms_new = pd.concat([single_coord, bbox])\n",
    "\n",
    "        # Save the gdf as a shp\n",
    "        allgeoms_new.to_file(output_path, driver=\"ESRI Shapefile\")\n",
    "\n",
    "# Run the function\n",
    "single_coord_trails(track2018_bbox_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: NATURA INTERSECT\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
