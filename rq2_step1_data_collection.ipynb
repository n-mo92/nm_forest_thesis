{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2 Data Collection\n",
    "\n",
    "\n",
    "[Add description]\n",
    "\n",
    "Wikiloc data extraction for Germany\n",
    "\n",
    "Python code provided by Chai-Allah et al, 2023 through their GitHub repo: https://github.com/achaiallah-hub/Wiki4CES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs created!\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "\n",
    "# Import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create folders for storing scrapy outputs\n",
    "path_list = [\"./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs\"]\n",
    "\n",
    "for path in path_list:\n",
    "  if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print(\"Folder %s created!\" % path)\n",
    "  else:\n",
    "    print(\"Folder %s already exists\" % path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy Set-up\n",
    "\n",
    "To run scrapy you need to set up a scrapy \"project\". In Anaconda Prompt:\n",
    "\n",
    "1. cd C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\n",
    "2. conda activate C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda \n",
    "3. scrapy startproject wikiloc_scrapy\n",
    "\n",
    "Helpful scrapy information: https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    "\n",
    "Then you need to add the scrapy \"spiders\" (the processes which crawl and scrape information from the web) as .py scripts to the directory wikiloc_scrapy/wikiloc_scrapy/spiders. Basically, the .py scripts from the achaiallah-hub repo need to be saved in this directory. In Anaconda Prompt:\n",
    "\n",
    "1. cd wikiloc_scrapy\\wikiloc_scrapy\\spiders\n",
    "2. git init\n",
    "3. git remote add origin git@github.com:achaiallah-hub/Wiki4CES.git\n",
    "4. git pull origin main\n",
    "\n",
    "5. Afterwards, delete the git files which are created in wikiloc_scrapy\\wikiloc_scrapy\\spiders (otherwise you end up working on the achiallah-hub Wiki4CES repo rather than your own repo!)\n",
    "\n",
    "**NOTE** Instead of Steps 2-4 I first just tried the simple command git clone https://github.com/achaiallah-hub/Wiki4CES.git This works, but it stores the .py scripts inside a repo directory folder called Wiki4CES, and I think this causes problems when trying to use the spiders later. Steps 2-4 are a work-around: this way the python scripts are directly within the \"spiders\" directory without being inside another directory. In order to do steps 2-4 an **ssh-key** needs to be set up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what I understand, the spiders provided in the Wiki4CES repo do the following:\n",
    "1. **extract_link.py** Extracts the URLS for all the trails. You give it a starting point (an intital URL) and it goes through each city/town and extracts all the trail links (URLS) in the cities listing. It stores each link as {\"Link\": link}. Needs to be run first to get the URLS for steps 2 and 3. \n",
    "2. **wikiloc_track.py** Scrapes the trail details like track name, difficulty, distance, author, views and description. It loads the trail URLs from a file called link.csv (presumably created in step 1)\n",
    "3. **wikiloc_image.py** Scrapes image data from the trail pages, including URL, track name, user name, date, and location (latitude & longitude). It reads the trail pages from a file called link.csv (presumably created in step 1)\n",
    "4. **download_image.py** Downloads images from the URLS in a csv file called wikiloc_image.csv (presumably this would be created from step 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: extract_link.py\n",
    "\n",
    "Edit the extract_link.py to replace the staring_urls. Originally this contained https://www.wikiloc.com/trails/france/auvergne-rhone-alpes - this URL doesn't seem to exist anymore as it just redirects to https://www.wikiloc.com/trails/outdoor. \n",
    "\n",
    "My guess is that the URL format now needs to be https://www.wikiloc.com/trails/outdoor/ + *country_name* + *region_name* so for Germany I will try https://www.wikiloc.com/trails/outdoor/germany and then each of the regions within (I think I probably need to do each separately as that's the way the code was written for the original project).\n",
    "\n",
    "For Germany, there are the following regions:\n",
    "\n",
    "| Count | Region                 | URL ending              |\n",
    "| ----- | ---------------------- | ----------------------- | \n",
    "| 1     | Baden-Wurttemberg      | /baden-wurttemberg      | \n",
    "| 2     | Bavaria                | /bavaria                |\n",
    "| 3     | Berlin                 | /berlin                 |\n",
    "| 4     | Brandenburg            | /brandenburg            |\n",
    "| 5     | Bremen                 | /bremen                 |\n",
    "| -     | DE.16,11               | (don't use)             |\n",
    "| 6     | Hamburg                | /hamburg                |\n",
    "| 7     | Hessen                 | /hessen                 |\n",
    "| 8     | Mecklenburg-Vorpommern | /mecklenburg-vorpommern |\n",
    "| 9     | Niedersachsen          | /niedersachsen          |\n",
    "| 10    | Nordrhein-Westfalen    | /nordrhein-westfalen    |\n",
    "| 11    | Rheinland-Pfalz        | /rheinland-pfalz        |\n",
    "| 12    | Saarland               | /saarland               |\n",
    "| 13    | Sachsen                | /sachsen                |\n",
    "| 14    | Saxony-Anhalt          | /saxony-anhalt          |\n",
    "| 15    | Schleswig-Holstein     | /schleswig-holstein     |\n",
    "| 16    | Thüringen              | /thuringen              |\n",
    "\n",
    "**NOTE** The number of trails being added seem to be increasing steadily (within one week, the total count for Germany went up by ~1000). I'll need to keep track of the number of expected trails on the day of download. Also check to make sure no new regions are added!\n",
    "\n",
    "DE.16,11 appears to be a few trails in Berlin - I don't think I need to bother with this as there is so few and in an urban area (and all the routes don't really look like anything to do with forests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Anaconda Prompt (with conda environment activated and from the scrapy project's top level directory):\n",
    "1. (if needed) conda activate C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda \n",
    "2. (if needed) cd C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\wikiloc_scrapy\\wikiloc_scrapy\\spiders\n",
    "3. scrapy crawl wiki -o link.csv\n",
    "\n",
    "**NOTE:** For some reason, this seems to try to run all the spiders at once, and you end up getting error messages saying certain files don't exist (which makes sense as these files need to be created by certain spiders first). I tried looking for the solution for this, but for now I've just commented out the code within the other spiders. UPDATE: It seems to be okay once the errors have been resolved, so I'm leaving finished scripts uncommented as I correct them.\n",
    "\n",
    "**PROBLEM** HTTP Status Code 403 - Forbidden / Access Denied - could this be anti-scraping measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution Attempt #1**\n",
    "\n",
    "1. pip install scrapy_cloudflare_middleware\n",
    "2. edit downloader middleware in settings.py according to: https://github.com/clemfromspace/scrapy-cloudflare-middleware?tab=readme-ov-file\n",
    "\n",
    "Error message, trying to install an older version of requests/urllib3 as per https://stackoverflow.com/questions/76414514/cannot-import-name-default-ciphers-from-urllib3-util-ssl-on-aws-lambda-us\n",
    "3. conda install requests==2.28.2\n",
    "4. Now try scrapy crawl command\n",
    "\n",
    "I got this to run, but it ended up back with the 403 error messages again. The solution maybe is too old? This source seems to suggest that the cloudflare middleware solution no longer works: https://www.zenrows.com/blog/scrapy-cloudflare#conclusion\n",
    "\n",
    "I reverted back to original set up by:\n",
    "1. Commenting out the downloader middlewware in settings.py\n",
    "2. ~~conda update requests~~ (just left requests as is for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution Attemp #2**\n",
    "\n",
    "1. Add default request headers according to https://www.zenrows.com/blog/scrapy-headers#most-important-ones\n",
    "2. Now try scrapy crawl command\n",
    "\n",
    "Now the spider seems to correctly generate the urls for all the cities within the region, but still doesn't return the trail URLs. I started looking into the xpath expressions in the extract_link.py as I wondered if the path structure has changed a bit over time (like the URLs).\n",
    "\n",
    "I found this video useful for understanding xpath https://www.youtube.com/watch?v=4EvxqTSzUkI \n",
    "I then went to https://www.wikiloc.com/trails/outdoor/germany/bremen and did rick click > Inspect to see the html. After a search for the components on the main Bremen page and then for one city (for example: https://www.wikiloc.com/trails/outdoor/germany/bremen/alte-neustadt) I made a couple changed to the xpaths.\n",
    "\n",
    "Now the spider generates URLs (or at least parts of URLs) for Bremen (which I'm using as my testing region)! \n",
    "\n",
    "**NOTE:** Since the URLs saved so far are just the back half of the URL, without the beginning (eg. /cycling-trails/bremen-achim-18077390) I adjusted the code to add the beginning part as well. I'm not 100% sure if this is needed or now (I think it might be), but I might need to remove this later. \n",
    "\n",
    "The next issue was that only 861 URLs were being saved for Bremen (website says there is 1450). This was an issue with the pagination handling, so I made some modifications to the extract_link.py for the next and next_page sections. Now I get 2061 URLs, but a check in excel showed there were many duplicates. After removing these I get 1458 which I think is correct (I think there is some rounding going on on the website as all trail counts at the city level are divisible by 10).\n",
    "\n",
    "Below I will record the workflow per region to make this clear now that things seem to be working!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Links Workflow Example: Bremen**\n",
    "\n",
    "In extract_link.py:\n",
    "1. Update start_urls: 'https://www.wikiloc.com/trails/outdoor/germany/bremen' and save.\n",
    "\n",
    "In Anaconda Prompt:\n",
    "1. conda activate C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\n",
    "2. cd C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\wikiloc_scrapy\\wikiloc_scrapy\\spiders\n",
    "3. scrapy crawl wiki -o crawling_outputs\\link-bremen.csv\n",
    "\n",
    "\n",
    "Remove duplicates:\n",
    "I am not sure why duplicates are occuring, but the step below simply removes any duplicates.\n",
    "(see code below)\n",
    "\n",
    "*NOTE:* For Bremen at the time of scraping (31 MARCH 2025), the website shows 1460 trails, however I get 1532 trails (after the duplicates are removed) - this means there are an extra 72 trails. I'm not sure why this is but I wonder if it has something to do with trails which cross borders (and therefore are in more than 1 region of Germany). It could be that these trails can be searched for in both regions but are only included in the count of 1 to avoid double-counting? **I should check for duplicates across regions to make sure all trails are unique.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.wikiloc.com/bicycle-touring-trails...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.wikiloc.com/hiking-trails/weser-ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.wikiloc.com/mountain-biking-trails...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.wikiloc.com/hiking-trails/weser-ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.wikiloc.com/hiking-trails/weser-ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>https://www.wikiloc.com/hiking-trails/bremen-4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>https://www.wikiloc.com/outdoor-trails/06bremh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>https://www.wikiloc.com/outdoor-trails/05lohnb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>https://www.wikiloc.com/bicycle-touring-trails...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>https://www.wikiloc.com/outdoor-trails/05diepb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1532 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Link\n",
       "0     https://www.wikiloc.com/bicycle-touring-trails...\n",
       "1     https://www.wikiloc.com/hiking-trails/weser-ra...\n",
       "2     https://www.wikiloc.com/mountain-biking-trails...\n",
       "3     https://www.wikiloc.com/hiking-trails/weser-ra...\n",
       "4     https://www.wikiloc.com/hiking-trails/weser-ra...\n",
       "...                                                 ...\n",
       "2021  https://www.wikiloc.com/hiking-trails/bremen-4...\n",
       "2023  https://www.wikiloc.com/outdoor-trails/06bremh...\n",
       "2027  https://www.wikiloc.com/outdoor-trails/05lohnb...\n",
       "2037  https://www.wikiloc.com/bicycle-touring-trails...\n",
       "2062  https://www.wikiloc.com/outdoor-trails/05diepb...\n",
       "\n",
       "[1532 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove CSV duplicates \n",
    "\n",
    "# Store scrapy spider path (where outputs are stored)\n",
    "scrapy_output = \"./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs/\"\n",
    "\n",
    "# Load the CSV as a df\n",
    "link_bremen = pd.read_csv(scrapy_output + \"link-bremen.csv\", sep=\"\\t\")\n",
    "\n",
    "#Remove duplicates\n",
    "link_bremen.drop_duplicates(inplace=True)\n",
    "\n",
    "# Write the results to same file (overwrite)\n",
    "link_bremen.to_csv(scrapy_output + \"link-bremen.csv\", index=False)\n",
    "\n",
    "# Check\n",
    "link_bremen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLAN\n",
    "- get everything working for Bremen\n",
    "- run \"formally\" by keeping track of date of download, number of trails listed on website, number actually downloaded, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: wikiloc_track.py\n",
    "\n",
    "I edited the wikiloc_track to update the xpaths and add extraction of: \n",
    "- date recorded\n",
    "- photo captions (title and body)\n",
    "- comments\n",
    "I removed the author extract so no personal information is collected. Although I updated the xapths for the following features, I commented them out as I don't think I'll need them for my analysis:\n",
    "- trail difficulty\n",
    "- view counts\n",
    "- download counts\n",
    "- trail length/distance\n",
    "\n",
    "To run the script:\n",
    "\n",
    "In wikiloc_track.py:\n",
    "1. Change CSV name in start_urls to crawling_outputs\\link-bremen.csv\n",
    "\n",
    "In Anaconda Prompt:\n",
    "1. conda activate C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\n",
    "2. cd C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\wikiloc_scrapy\\wikiloc_scrapy\\spiders\n",
    "3. scrapy crawl wiki_track -o crawling_outputs\\track-bremen.json\n",
    "\n",
    "**Needs to be output as json** otherwise the utf-8 encoding doesn't seem to work properly and the German special characters are not handled well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
