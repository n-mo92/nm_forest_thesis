{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2 Data Collection\n",
    "\n",
    "Wikiloc data extraction for Germany. **See scrapy_setup_info.md for setting up scrapy, including edits I made to the settings to fix 403 error messages and make the scraping more polite.**\n",
    "\n",
    "scrapy spiders are provided by Chai-Allah et al, 2023 through their GitHub repo: [Wiki4CES](https://github.com/achaiallah-hub/Wiki4CES)\n",
    "\n",
    "From what I understand, the spiders provided in the Wiki4CES repo do the following:\n",
    "1. **extract_link.py** Extracts the URLS for all the trails. You give it a starting region (an intital URL) and it goes through each city/town in that region and extracts all the trail links (URLS) in the cities listing. This spider neeeds to be run first to get the URLS for steps 2 and 3. \n",
    "2. **wikiloc_track.py** Scrapes the trail details like track name, difficulty, distance, author, views and description. It loads the trail URLs from a file called link.csv (presumably created in step 1)\n",
    "3. **wikiloc_image.py** Scrapes image data from the trail pages, including URL, track name, user name, date, and location (latitude & longitude). It reads the trail pages from a file called link.csv (presumably created in step 1)\n",
    "4. **download_image.py** Downloads images from the URLS in a csv file called wikiloc_image.csv (presumably this would be created from step 3). Not needed for my work, so I have removed this file\n",
    "\n",
    "**NOTE:** For some reason, running one spider seems to try to run all the spiders at once, and you end up getting error messages saying certain files don't exist (which makes sense as these files need to be created by certain spiders first). I tried looking for the solution for this, but for now I've just commented out the code within the other spiders. UPDATE: It seems to be okay once the errors have been resolved (it doesn't actually run the other spiders but seem to check for the correct files and the code being valid?), so I'm leaving finished scripts uncommented as I correct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs already exists\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "\n",
    "# Import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Point\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "# Create folders for storing scrapy outputs\n",
    "path_list = [\"./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs\"]\n",
    "\n",
    "for path in path_list:\n",
    "  if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    print(\"Folder %s created!\" % path)\n",
    "  else:\n",
    "    print(\"Folder %s already exists\" % path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Updating extract_link.py\n",
    "\n",
    "**Step 1a: Update starting_urls (including Germany region info)** \n",
    "Edit the extract_link.py to replace the staring_urls. Originally this contained https://www.wikiloc.com/trails/france/auvergne-rhone-alpes - this URL doesn't seem to exist anymore as it just redirects to https://www.wikiloc.com/trails/outdoor. \n",
    "\n",
    "The URL format now needs to be https://www.wikiloc.com/trails/outdoor/ + *country_name* + *region_name* so for Germany I will try https://www.wikiloc.com/trails/outdoor/germany and then each of the regions within.\n",
    "\n",
    "For Germany, Wikiloc has trails for the following regions:\n",
    "\n",
    "| Count | Region                 | URL ending              | Rough Count |\n",
    "| ----- | ---------------------- | ----------------------- | ----------- |\n",
    "| 1     | Baden-Wurttemberg      | /baden-wurttemberg      | 77,800      |\n",
    "| 2     | Bavaria                | /bavaria                | 79,400      |\n",
    "| 3     | Berlin                 | /berlin                 | 12,000      |\n",
    "| 4     | Brandenburg            | /brandenburg            | 8,340       |\n",
    "| 5     | Bremen                 | /bremen                 | 1,500       |\n",
    "| -     | DE.16,11               | (don't use)             | 5           |\n",
    "| 6     | Hamburg                | /hamburg                | 7,310       |\n",
    "| 7     | Hessen                 | /hessen                 | 26,300      |\n",
    "| 8     | Mecklenburg-Vorpommern | /mecklenburg-vorpommern | 6,320       |\n",
    "| 9     | Niedersachsen          | /niedersachsen          | 31,400      |\n",
    "| 10    | Nordrhein-Westfalen    | /nordrhein-westfalen    | 86,800      |\n",
    "| 11    | Rheinland-Pfalz        | /rheinland-pfalz        | 55,400      |\n",
    "| 12    | Saarland               | /saarland               | 5,960       |\n",
    "| 13    | Sachsen                | /sachsen                | 21,000      |\n",
    "| 14    | Saxony-Anhalt          | /saxony-anhalt          | 7,260       |\n",
    "| 15    | Schleswig-Holstein     | /schleswig-holstein     | 9,650       |\n",
    "| 16    | Thüringen              | /thuringen              | 5,650       |\n",
    "\n",
    "*NOTE* The number of trails being added seem to be increasing steadily (for example, within a one week period, the total count for Germany went up by ~1000). So the numbers shown here may not be up to date (taken 25 April 2025) and are just to give a sense of the size/time it will take to scrape.\n",
    "\n",
    "**Also check to make sure no new regions are added!**\n",
    "\n",
    "DE.16,11 appears to be a few trails in Berlin - I don't think I need to bother with this as there is so few and in an urban area (and all the routes don't really look like anything to do with forests)\n",
    "\n",
    "**Step 1b: Update xpath expressions & other edits**\n",
    "After overcoming initial 403 error messages (see scrapy_setup_info.md), the spider seemed to correctly generate the urls for all the cities within the region, but still didn't return the trail URLs. I started looking into the xpath expressions in the extract_link.py as I wondered if the path structure has changed a bit over time (like the URLs).\n",
    "\n",
    "I found this video useful for understanding xpath https://www.youtube.com/watch?v=4EvxqTSzUkI \n",
    "I then went to https://www.wikiloc.com/trails/outdoor/germany/bremen and did rick click > Inspect to see the html (I selected Bremen as the testing region as it has the fewest trails). After a search for the components on the main Bremen page and then for one city (for example: https://www.wikiloc.com/trails/outdoor/germany/bremen/alte-neustadt) I made a couple changed to the xpaths in extract_link.py (see comments in script). I also made some changes to the pagination handling (see comments in script). ALSO, since the URLs saved initially were just the back half of the URL, without the beginning (eg. /cycling-trails/bremen-achim-18077390) I adjusted the code to add the beginning part as well. This ended up being required for the other spiders to work properly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 ADDITION: extract_link_large.py\n",
    "\n",
    "For large/popular states like Bavaria and Nordrhein-Westfalen, I ran into problems where many trails links were missing after the link extraction. These were quite large amounts compared to small amounts missing for other regions - for example, for Bavaria I was expecting around 80,000 trails but only got ~10,000. I found out this is because Wikiloc uses a pagination cap of 1000 pages - so for especially popular activites, even if there were more than 1000 pages worth of trails, the URLS like https://www.wikiloc.com/trails/hiking/germany/bavaria?page=1001 simply don't work. Given that there are 10 trails per page, this means a maximum of 10,000 trails can be accessed through the 1000 pages. For most regions/activity combinations this is not a problem, but for hiking trails in Bavaria (for example), there are about 32,000 trails, meaning 22,000 trails are missed with this approach.\n",
    "\n",
    "To resolve this issue I created another scraper for handling large regions specifically. Instead of supplying a URL to the region and filtering all trails by activity, this approach uses a directory of places within a region, accessing each in turn. For each place within a region, the scraper checks whether further filtering is possible by activity. If available it filters the trails for the place by activity, and if not it extracts the trails directly. \n",
    "\n",
    "**NOTE** It seems like the best way is to run both the extract_link.py and the extract_link_large.py - this seems to cover all bases and get the most links. This could be because not all trails are linked to place beyond simply the region, and so the second script might not get everything either.Just make sure to remove duplicates AFTER merging the lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Workflow for running extract_link.py or extract_link_large.py\n",
    "\n",
    "*For now, this is just for the Bremen region.*\n",
    "\n",
    "**Step 2a**\n",
    "In extract_link.py:\n",
    "1. Update start_urls: 'https://www.wikiloc.com/trails/outdoor/germany/bremen' and save. If using extract_link_large the start URL looks more like this 'https://www.wikiloc.com/directory/bjnYAg' (this is the URL which shows a list of all the places within a region/state)\n",
    "\n",
    "**Step 2b**\n",
    "In Anaconda Prompt:\n",
    "1. conda activate C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\n",
    "2. cd C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\wikiloc_scrapy\\wikiloc_scrapy\\spiders\n",
    "3. scrapy crawl wiki -o crawling_outputs\\link-bremen.csv\n",
    "\n",
    "**Step 2c**\n",
    "Remove duplicates: I am not sure why duplicates are occuring, but the code below simply removes any duplicates.\n",
    "\n",
    "*NOTE:* For Bremen at the time of scraping (31 MARCH 2025), the website shows 1460 trails, however I get 1532 trails (after the duplicates are removed) - this means there are an extra 72 trails. I'm not sure why this is but I wonder if it has something to do with trails which cross borders (and therefore are in more than 1 region of Germany). It could be that these trails can be searched for in both regions but are only included in the count of 1 to avoid double-counting? **I should check for duplicates across regions to make sure all trails are unique.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2C: Remove CSV duplicates \n",
    "\n",
    "# Create a list of the csv paths with all the scraped trail links\n",
    "link_csv_paths = glob.glob('./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs/link-*.csv')\n",
    "\n",
    "# Load csvs from list, remove duplicates and then write results to same file (overwrite)\n",
    "for csv_path in link_csv_paths:\n",
    "    loaded_csv = pd.read_csv(csv_path, sep=\"\\t\")\n",
    "    loaded_csv.drop_duplicates(inplace=True)\n",
    "    loaded_csv.to_csv(csv_path, index=False)\n",
    "\n",
    "# Check\n",
    "#link_bremen = pd.read_csv('./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs/link-bremen.csv', sep=\"\\t\")\n",
    "#link_bremen\n",
    "#link_nieder = pd.read_csv('./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs/link-niedersachsen.csv', sep=\"\\t\")\n",
    "#link_nieder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: updating wikiloc_track.py\n",
    "\n",
    "I edited wikiloc_track.py to update the xpaths (as with the extract_link.py - see comments in script). I also added code so that I could also scrape additional information: \n",
    "- date recorded\n",
    "- photo/waypoint captions (title and body)\n",
    "- comments\n",
    "- **photo/waypoint latitudes and longitudes**\n",
    "- **start point latitude and longitude** (unfortunately the end point is not stored in the html)\n",
    "\n",
    "Because I handled all the coordinate extraction in this script, I did not use or update the wikiloc_image.py script (and I since deleted it from my repo). I extracted all latitude values in one column, and all longitude values in another column. I can then extract the minimum and maximum values from each column in order to create a bounding box.\n",
    "\n",
    "Additionally, I removed the author extraction completely so that no personal information is collected. \n",
    "\n",
    "Although I updated the xapths for the following features, I commented them out as I don't think I'll need them for my analysis:\n",
    "- trail difficulty\n",
    "- view counts\n",
    "- download counts\n",
    "- trail length/distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Workflow for running wikiloc_track.py\n",
    "\n",
    "*For now, this is just for the Bremen region.*\n",
    "\n",
    "**Step 4a**\n",
    "In wikiloc_track.py:\n",
    "1. Change CSV name in start_urls to: crawling_outputs\\link-bremen.csv\n",
    "\n",
    "**Step 4b**\n",
    "In Anaconda Prompt:\n",
    "1. conda activate C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\n",
    "2. cd C:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\wikiloc_scrapy\\wikiloc_scrapy\\spiders\n",
    "3. scrapy crawl wiki_track -o crawling_outputs\\track-bremen.json\n",
    "\n",
    "**Needs to be output as json** otherwise (as csv) the utf-8 encoding doesn't seem to work properly and the German special characters are not handled well. \n",
    "\n",
    "For Bremen (1532 trails), with download delays and autothrottle on, this stage takes about **65 minutes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Filter for 2018 & distance\n",
    "\n",
    "Some initial filtering can be applied to reduce the amount of data that goes through the generating geometries step.\n",
    "\n",
    "This function filters on two fields:\n",
    "1. Filter for window around 2018: 2017-2019 to roughly match date of forest data but expand the results a bit.\n",
    "2. Filter out very long trails (for now, >175km). These tend correspond to motorised transport (which, when covering large distances may be difficult to pin down to CES for forests) or unexpected use of the website/errors. For example, this trail https://www.wikiloc.com/hiking-trails/xabia-teulada-126272868 is recorded as a ~5 hour hike, but it goes from Germany to Spain. This trail already gets removed with the 2018 filter, but there may be others like it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: FILTER 2017-2019 & DISTANCE (ATTRIBUTE)\n",
    "\n",
    "# Create a list of the json paths with all scraped data\n",
    "track_json_paths = glob.glob('./wikiloc_scrapy/wikiloc_scrapy/spiders/crawling_outputs/track-*.json')\n",
    "\n",
    "# Load jsons from list, select only 2018 data & trails less than certain distance, return new json\n",
    "# This outputs to the PROCESSING folder!\n",
    "def dist_year_filter(json_paths):\n",
    "    for json_path in json_paths:\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(json_path)[1] \n",
    "        # For output file naming: remove extension from input file name\n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # For output file naming: assemble the new file path for the output\n",
    "        output_path = \"./processing/\" + name_wo_ext + \"_3year_distfilter.json\" \n",
    "\n",
    "        # Load json as df\n",
    "        track_df = pd.read_json(json_path) \n",
    "\n",
    "        # Select rows where date_recorded includes \"2017\", \"2018\" or \"2019\"\n",
    "        track_3year_df = track_df[track_df[\"date_recorded\"].str.contains(\"2017|2018|2019\")]\n",
    "\n",
    "        # Select rows where distance is less than 175 km\n",
    "        track_3year_short_df = track_3year_df[track_3year_df[\"distance_km\"] < 175]\n",
    "        \n",
    "        # Save the gdf as a json\n",
    "        track_3year_short_df.to_json(output_path)\n",
    "\n",
    "# Run the function\n",
    "dist_year_filter(track_json_paths)\n",
    "\n",
    "# Load the json and check\n",
    "#bw_3year_df = pd.read_json(\"./processing/track-badenwurttemberg_3year_distfilter.json\")\n",
    "#bw_3year_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Generating geometries (buffered lines/points)\n",
    "\n",
    "In step 4, I extracted all latitude values in one column, and all longitude values in another column. Since the coordinates are in order (first the start coordinate, then the waypoints in order) a line segment can be created for where multiple coordinates exist. Where only 1 coordinate pair exists (the start coordinates) a point can be created - then both points and lines can be buffered in order to generate polygons geometries for all trails.\n",
    "\n",
    "**IMPORTANT** The buffer is set to 30m (on each side of line, or the radius for points). This is roughly the average between a minimum sight distance of 15m used in Xiang, 1996 \"for hikers’ unobstructed forward and rear view of the surroundings\" and a buffer of 50m used in Torkko et al 2023 which was found to most accurately capture perceived greenery in urban areas. The 30m buffer is likely a small overestimation of sight in dense forest, but a large underestimation for high, open viewpoints where people can see much further. I opted for this more conservative approach with small buffers in an effort to increase the chance that the textual content is about forests (see forest masking step later).\n",
    "\n",
    "The code/function below generates a buffered line/point geometry for each trail within each json and outputs a shapefile.\n",
    "\n",
    "Technical notes for pairing up lat and long coordinates:\n",
    "- apply with axis= 1 applies the function to each row\n",
    "- lambda row sets up an anonymous function to do something for each row\n",
    "- zip pairs the items in each list by index (so the lats and longs get paired up according to their order in the list)\n",
    "- the list part converts the output from zip into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '['Zwingenberg Brücke', 'Zwingenberg Burg', 'Ziegen', 'Zwingenberg Burg', '1', 'Umgefallener Baum', '2', '3', 'Abzweigung', 'Abzweigung', 'Schlossblick', 'Bergbrunnen', 'Wildrosen', 'Abzweigung', 'Reihersee', 'Abzweigung', 'Ausblick auf den Neckar', 'Abzweigung rechts', 'Abzweigung', '4', 'Burg Stolzeneck', 'Burg Stolzeneck', 'Wegweiser', 'Bachlauf', 'Fingerhut', '5', 'Aussicht', 'Abzweigung Richtung Rockenau', 'Steinbruch', 'Abzweigung rechts', 'Blick auf Rockenau über Friedhof', 'Abzweigung', 'Blick auf Eberbach', 'Zur Stadt', 'Foto', 'Statue', 'Kirche']' of field photo_capt has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 'La Partnachklamm és un espectacular congost que obre el riu Partnach abans d'arribar a Garmisch. Usat tradicionalment per a baixar la fusta tallada aigües amunt a primeries del segle XX es va obrir un camí per la vora per a facilitar la tasca de conduir els troncs. Este camí s'ha convertit en tota una atracció turística visitada per milers de persones cada any i amb raó perquè és tot un espectacle dels sentits travessar el congost sentint l'aigua als peus i, alhora, l'aigua que cau del cel. ' of field descriptio has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 'Die Ringrollbahn (Taxiway) und die Versorgungsstraße im Südteil haben eine bessere Qualität als die beiden Start- und Landebahnen. Am Vormittag ist hier nicht ganz so viel los wie am Nachmittag. Am besten skatet es sich bei Westwind. Dann hat man ihn begab von vorn und bergauf von hinten. ' of field descriptio has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 'Die kleine Runde für zwischendurch geht entlang des Radweges nach Gandenitz ... und zurück. Der Weg ist außerdem nur für Forts- und Landwirtschaftsfahrzeuge erlaubt und daher wenig befahren und trotzdem meistens auch recht sauber. Zunächst flankiert von Kirschbäumen geht der Streckverlauf im Weiteren durch den Wald....also gemütliche Strecke, auf der man häufig auf Jogger und Radfahrer trifft.' of field descriptio has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '['Weserfähre Nordenham-Bremerhaven, Anleger Bremerhaven', 'Hotel Amaris', 'Weserfähre Lemwerder - Vegesack', 'Weserfähre Farge-Berne', 'Weserfähre Brake-Sandstedt', 'Hauptbahnhof Bremerhaven ', '2RADHAUS', 'Sportboothafen Dedesdorf-Eidewarden', \"Reisemobilplatz 'Am Sportboothafen'\", \"Wohnmobilstellplatz 'Doppelschleuse'\", \"Campingplatz 'Rechtenfleth'\", \"Campingplatz 'Sandstedt'\"]' of field photo_capt has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '[53.545167, 53.543665, 53.543574, 53.543235, 53.543189, 53.542975, 53.542558, 53.544271, 53.54414, 53.540137, 53.539546, 53.539948, 53.534811, 53.534621, 53.528853, 53.527258, 53.525932, 53.52781, 53.529489, 53.530009, 53.530251, 53.532852, 53.532842, 53.531422, 53.531215, 53.543439, 53.543512, 53.543546, 53.543991]' of field latitudes has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '['T2', 'Airport Terminal 2', 'Info T2 Level 2', 'Frankfurt Card', 'Shuttle Skyline T2-T1 Level 4', 'Shuttle Skyline T1-T2 Level 4', 'Info T1 Level 2', 'Frankfurt(M) Flughafen Regionalbf S8-S9 (to city center)', 'Frankfurt am Main - Stadion S8-S9', 'Frankfurt-Niederrad S8-S9', 'Frankfurt Hauptbahnhof S8-S9', 'Frankfurt(M)Taunusanlage S8-S9', 'Frankfurt(M)Hauptwache S8-S9', 'Hauptwache U1 U2 U3 U6 U7 U8', 'Dom/Römer U4 U5', 'Willy-Brandt-Platz U1 U2 U3 U4 U5 U8', 'Metro Frankfurt (Main) Hauptbahnhof U4 U5', 'Metro Festhalle / Messe U4', 'Metro Bockenheimer Warte U4 U6 U7', 'Metro Westend U6 U7', 'Metro Alte Oper U6 U7', 'Konstablerwache S8 S9', 'Metro Konstablerwache U4 U5 U6 U7', 'Senckenberg Museum (dinosaurs)', 'Experiminta Science Center', 'Alter Oper (Old Opera)', 'Maintower', 'St. Katherine (evangelic church)', 'Liebfrauen (catholic church)', 'Goethe house', 'St. Paul Church', 'Römer Town Hall', 'Nicholas old church', 'Arqueologic Museum', 'Iron bridge (pedestrian)', 'Niza park (river walk)', 'Frankfurt Cathedral', 'Domturm (wiewpoint)', 'Three kings (evangelic church)', 'Portikus Art gallery (free pass)', 'Tourist information Center', 'Tourist info', 'Tourist info', 'Botanical garden', 'T1', 'Airport Terminal 1']' of field photo_capt has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '['Kapitän Blaubär auf Tour', 'Schrauben-Teststrecke', 'Hier ist die Welt vernagelt', 'Einmal Öl nachschauen', 'Diesteln? Nee!', '?', 'HL in Griffnähe', 'Was ist das?', 'Tierra Santa', 'Grenzhaus=Museum', 'Grenzgeschichte', 'Einmal bitte Eis nachfüllen.']' of field photo_capt has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '[(8.564794, 53.694009), (8.681081, 53.891708), (8.681081, 53.891708), (8.681081, 53.891708), (8.682154, 53.884983), (8.71086, 53.872696), (8.710824, 53.872616), (8.710915, 53.872637), (8.71091, 53.872607), (8.71091, 53.872607), (8.71091, 53.872607), (8.710847, 53.872612), (8.710847, 53.872612), (8.710907, 53.872583), (8.711108, 53.872517), (8.711041999999999, 53.872546), (8.711128, 53.872473), (8.711128, 53.872473), (8.643661, 53.885444)]' of field coordinate has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value 'Start und Ziel ist die Gemeinde Adendorf mit ihren zahlreichen Töpferwerkstätten. Die Strecke führt durch die Grafschaft mit ihren ausgedehnten Obstplantagen bis hinauf ins Quellgebiet des Swistbaches bei Kalenborn. Bei klarem Wetter bieten sich eine Reihe hübscher Fernblicke in die Voreifel und über das Rheintal hinweg zum Siebengebirge. Der Weg verläuft zum größten Teil über gut befestigte Wirtschaftswege und ist damit auch bei feuchter Witterung gut zu begehen. Einen besonderen Reiz bietet die Runde zur Zeit der Apfelblüte.' of field descriptio has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '['Aussichtspunkt', 'Bahnhof Brohltalbahn', 'Bank 01', 'Bank 02', 'Bank 03', 'Bank 04, herrliche Aussicht', 'Bank 05, herrliche Aussicht', 'Bank 06', 'Bank 07, schöne Aussicht', 'Bank 08', 'Bank 09', 'Bank 10, schöne Aussicht', 'Burg Olbrück', 'Kastellaney Burg Olbrück', \"Lösch's Nück\", 'Neuer Maarhof, Niederdürenbach', 'Parkplatz', 'Rasthütte 01', 'Zum Bockshahn, Spessart']' of field photo_capt has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '[(6.538653, 49.502339), (6.536264, 49.499346), (6.536264, 49.499346), (6.539004, 49.495572), (6.535895, 49.492942), (6.545698, 49.497302), (6.545698, 49.497302), (6.547727, 49.496539), (6.548653, 49.496474), (6.547558, 49.497292), (6.547558, 49.497292), (6.580838, 49.491153), (6.580838, 49.491153), (6.57373, 49.492138), (6.57373, 49.492138), (6.569287, 49.496173), (6.569287, 49.496173)]' of field coordinate has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '['Englischer Pavillon (Pillnitz, Dresden)', 'Palaisteich (Grosser Garten, Dresden)', 'F14 Kleinzschachwitz', 'F14 Pillnitz', 'Pillnitzer Kamelie (Pillnitz, Dresden)', 'Flora (Pillnitz, Dresden)', 'Fliederhof (Pillnitz, Dresden)', 'Puerta-1 (Loschwitz, Dresden)', 'Kavaliershaus (Lochswitz, Dresden)', 'Puerta-2 (Loschwitz, Dresden)', 'Puerta-3 (Loschwitz, Dresden)', 'T-Lipsiusstrasse (Dresden)', 'T11-Lennéplatz (Dresden)', 'Kavaliershaus-S (Grosser Garten, Dresden)', 'Kavaliershaus-O (Grosser Garten, Dresden)', 'Englischer Teich (Pillnitz, Dresden)', 'Blaues Wunder brücker (Loschwitz, Dresden)', 'Wasserpalais (Pillnitz, Dresden)', 'Hofkirche (Dresden)', 'Georgentor (Dresden)', 'Fürstenzug (Dresden)', 'Tritonengondel (Pillnitz, Dresden)', 'Bergpalais (Pillnitz, Dresden)', 'Neues Palais (Pillnitz, Dresden)', 'Kavaliershaus-E (Grosser Garten, Dresden)', 'T11-Wihelminenstrasse (Dresden)', 'T2-Altmarkt (Dresden)', 'T12-Schillerplatz (Dresden)', 'Parktheater (Grosser Garten, Dresden)', 'Chinesischer Pavillon (Pillnitz, Dresden)', 'Kavaliershaus-N (Grosser Garten, Dresden)', 'Schloss Eckberg (Loschwitz, Dresden)', 'Palmenhaus (Pillnitz, Dresden)', 'Löwenkopfbastei (Pillnitz, Dresden)', 'Orangerie (Pillnitz, Dresden)', 'Kulturpalast (Dresden)', 'Neuer Teich (Grosser Garten, Dresden)', 'Villa Stockhausen (Loschwitz, Dresden)', 'Schloss Albrechtsberg (Loschwitz, Dresden)', 'Chinesischer Teich (Pillnitz, Dresden)', 'Palais (Grosser Garten, Dresden)', 'Mosaikbrunnen (Grosser Garten, Dresden)', 'Gänsediebbrunnen (Dresden)', 'Käitzbach (Grosser Garten, Dresden)', 'Martin Luther (Dresden)', 'Frauenkirche (Dresden)', 'Carolasee (Grosser Garten, Dresden)', 'Kreuzkirche (Dresden)', 'Star Inn Hotel (Dresden)', 'T2-Freystrasse (Dresden)']' of field photo_capt has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '[(12.209148, 51.844574), (12.258698, 51.85685), (12.258698, 51.85685), (12.281732, 51.862953), (12.281706, 51.862996), (12.284603, 51.874651), (12.284528, 51.874665), (12.321713, 51.86672), (12.327295, 51.864486), (12.36752, 51.844501), (12.371255, 51.843172), (12.416034, 51.844365), (12.394053, 51.842138), (12.308388, 51.866441), (12.279268, 51.862339)]' of field coordinate has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '['Schöner Spaziergang. Für Hunde allerdings ist Leinenzwang. Verständlich im Vogelschutzgebiet. Allerdings gibt es auch viele Kilometer Elektrozaun entlang der Wege. Für Hunde insofern nicht ganz “die große Freiheit“. Trotzdem ein sehr schöner Walk.']' of field comments has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_1152\\3284390648.py:47: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_published' to 'date_publi'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'description text' to 'descriptio'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_km' to 'distance_k'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'date_recorded' to 'date_recor'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'photo_captions' to 'photo_capt'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'coordinates' to 'coordinate'\n",
      "  ogr_write(\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Value '['Freienorla', 'Jena', 'Kahla', 'Kirche Kolkwitz', 'Kirche Kolkwitz (Kirche, Anbetungsort) \\n07407, Uhlstädt-Kirchhasel, Landkreis Saalfeld-Rudolstadt, DEU', 'Quelle Suppiche', 'Quelle (Quelle)', 'Rudolstadt Markt', 'Markt (Ortsbezeichnung)', 'Saalfeld', 'Rossmann (Drogerie)\\nMarkt 3 Rm \\n07318, Saalfeld/Saale, Landkreis Saalfeld-Rudolstadt, DEU\\n+49 3671 527530']' of field photo_capt has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "### STEP 6: buffered point/line geometries\n",
    "\n",
    "# Create a list of the json paths with all the filtered trails\n",
    "track3year_json_paths = glob.glob('./processing/track-*_3year_distfilter.json')\n",
    "\n",
    "# Function which creates point geom if only one coordinate pair available, otherwise line geom\n",
    "def geom_from_coords(coords):\n",
    "    if len(coords) == 1:\n",
    "        return Point(coords[0])\n",
    "    else:\n",
    "        return LineString(coords)\n",
    "\n",
    "# Load jsons from list, generate buffer geometries and saves to processing folder as shp\n",
    "def buffergeom_generator(json_path_list):\n",
    "    for json_path in json_path_list:\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(json_path)[1] \n",
    "        # For output file naming: remove extension from input file name\n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # For output file naming: assemble the new file path for the output \n",
    "        output_path = \"./processing/\" + name_wo_ext + \"_buffer.shp\"\n",
    "\n",
    "        # Load json as df\n",
    "        track_df = pd.read_json(json_path)\n",
    "\n",
    "        # Pair-up the lats and longs into coordinates\n",
    "        track_df[\"coordinates\"] = track_df.apply(lambda row: list(zip(row[\"longitudes\"], row[\"latitudes\"])), axis=1)\n",
    "\n",
    "        # Run the geom_from_coords function (defined above) on all rows\n",
    "        track_df['geometry'] = track_df['coordinates'].apply(geom_from_coords)\n",
    "\n",
    "        # Convert to geodataframe\n",
    "        track_gdf = gpd.GeoDataFrame(track_df, geometry=\"geometry\")\n",
    "\n",
    "        # Define projection and reproject\n",
    "        track_gdf.crs= \"EPSG:4326\"\n",
    "        track_gdf = track_gdf.to_crs(\"EPSG:3035\")\n",
    "\n",
    "        # Buffer all rows so that all geometries are now polygons\n",
    "        # SET BUFFER DISTANCE HERE (M)\n",
    "        buffer_geoms = track_gdf.buffer(30)\n",
    "\n",
    "        # Replace geometries in gdf with buffered geometries\n",
    "        track_gdf = track_gdf.set_geometry(buffer_geoms)\n",
    "\n",
    "        # Write to shapefile\n",
    "        track_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
    "\n",
    "# Run the buffer geometry generator\n",
    "buffergeom_generator(track3year_json_paths)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: exporting the data as a shp truncates the long text fields (e.g. \"description\" \"photo_caption\", \"comments\", etc).** I also tried writing the file as a geojson, which works fine, but then when loading back in as a geodataframe, there are problems with the list structures in the \"photo_caption\" and \"comments\" fields. \n",
    "\n",
    "For now, I think the best work-around is to use the shapefile to do the spatial intersection steps, and then to **join the results back to the main json** (i.e. STEP 5 OUTPUTS, filtered for year and distance) for any text analsyis steps. I think the track URLs can be used for the join field as these are unique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADDITIONAL AREA FILTER** Previously I ran a filter to remove very long trails based on the distance attribute entered by the user on Wikiloc. Unfortunately this does not catch every case as sometimes the distance is entered incorrectly. To accomodate for this, I added another filter based on the area of the buffered geometries. This is only meant to catch really extreme cases (such as flights and long distances outside Germany). I decided to remove everything over 65 km2 based on a manual assessment of the extreme cases in QGIS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: ADDITIONAL AREA FILTER \n",
    "\n",
    "# Create a list of the shp paths with all the buffer trail geometries\n",
    "track3year_buffer_paths = glob.glob('./processing/track-*_3year_distfilter_buffer.shp')\n",
    "\n",
    "# Load jsons from list, select only trails under 65km2, return new shp\n",
    "# This outputs to the PROCESSING folder!\n",
    "def area_filter(shp_paths):\n",
    "    for shp_path in shp_paths:\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(shp_path)[1] \n",
    "        # For output file naming: remove extension from input file name\n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # For output file naming: assemble the new file path for the output\n",
    "        output_path = \"./processing/\" + name_wo_ext + \"_areafilt.shp\" \n",
    "\n",
    "        # Load json as gdf\n",
    "        track_gdf = gpd.read_file(shp_path)\n",
    "\n",
    "        # Calculate the area\n",
    "        track_gdf[\"area\"] = track_gdf.geometry.area\n",
    "\n",
    "        # Select rows where area is less than 65km2 (65000000m2)\n",
    "        track_areafilt_gdf = track_gdf[track_gdf[\"area\"] < 65000000]\n",
    "        \n",
    "        # Write to shapefile\n",
    "        track_areafilt_gdf.to_file(output_path, driver=\"ESRI Shapefile\")\n",
    "\n",
    "# Run the function\n",
    "area_filter(track3year_buffer_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Natura & Forest Consensus Map Filtering\n",
    "\n",
    "Required steps:\n",
    "1. Filter to only include trails which intersect with Natura 2000 areas. \n",
    "2. Filter for forests - here I can actually calculate the zonal statistics using the buffered trail geometries (filtered to only those which intersect with Natura sites) with the consenus map. I can the calculate the max class for each trail geometry and only include the trails which have a max class of 3, 4, 5, or 6 (ie. classes where at least half of the forest definitions agree on forest presence). In other word, if the max class of a trail is a non-forest class (0,1,2), then I remove it from the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: NATURA INTERSECT\n",
    "\n",
    "# Create a list of the shp paths with all the buffered and filtered trail geometries\n",
    "track3year_bufferfilt_paths = glob.glob('./processing/track-*_3year_distfilter_buffer_areafilt.shp')\n",
    "\n",
    "# Load Germany Natura sites\n",
    "natura = gpd.read_file(\"./outputs/natura2000_3035_DE.shp\", \n",
    "                       columns=[\"SITECODE\", \"SITENAME\", \"MS\", \"SITETYPE\"])\n",
    "\n",
    "# Load shps from list, check for intersections, remove duplicates & save as shp\n",
    "def natura_intersects(shp_path_list):\n",
    "    for shp_path in shp_path_list:\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(shp_path)[1] \n",
    "        # For output file naming: remove extension from input file name\n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # For output file naming: assemble the new file path for the output \n",
    "        output_path = \"./processing/\" + name_wo_ext + \"_natura.shp\"\n",
    "\n",
    "        # Load shp as gdf\n",
    "        track_buffer_gdf = gpd.read_file(shp_path)\n",
    "\n",
    "        # Inner join means any non-intersecting geometries will be dropped\n",
    "        # Duplication: trail geoms are duplicated for every different Natura site they overlap with\n",
    "        inter_trails = track_buffer_gdf.sjoin(natura, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "        # Get rid of duplicate trail geometries\n",
    "        inter_trails = inter_trails.drop_duplicates(subset=[\"url_track\"])\n",
    "\n",
    "        # Also drop the Natura info (to avoid confusion as site listed may not be the only site)\n",
    "        inter_trails = inter_trails.drop(columns=[\"index_right\", \"SITECODE\", \n",
    "                                                  \"SITENAME\", \"MS\", \"SITETYPE\"])\n",
    "\n",
    "        # Write to shapefile\n",
    "        inter_trails.to_file(output_path, driver=\"ESRI Shapefile\")\n",
    "\n",
    "# Run intersecting function to remove trails which do not intersect \n",
    "natura_intersects(track3year_bufferfilt_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n",
      "C:\\Users\\ninam\\AppData\\Local\\Temp\\ipykernel_15564\\1651315801.py:78: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
      "c:\\Users\\ninam\\Documents\\UZH\\04_Thesis\\code\\nm_forest_thesis\\thesis_env_conda\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'total_area_ha' to 'total_area'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: CONSENUS MAP ZONAL STATISTICS (FOREST INTERSECTS)\n",
    "# TAKES ABOUT 70 MIN \n",
    "\n",
    "# Create a list of the shp paths with all the buffered trails which intersect Natura sites\n",
    "track3year_natura_paths = glob.glob('./processing/track-*_3year_distfilter_buffer_areafilt_natura.shp')\n",
    "\n",
    "# Path to forest consensus map\n",
    "consensus_map = \"./outputs/forest_consensus_3035_DE_5m_2018.tif\"\n",
    "\n",
    "# Load shps from list, calculate zonal stats for consensus map, remove trails which don't have\n",
    "# class 3, 4, 5, 6 coverage, find the dominant coverage class, save output as shp \n",
    "def forest_coverage(shp_path_list):\n",
    "    for shp_path in shp_path_list:\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(shp_path)[1] \n",
    "        # For output file naming: remove extension from input file name\n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # For output file naming: assemble the new file path for the output \n",
    "        output_path = \"./processing/\" + name_wo_ext + \"_forest.shp\"\n",
    "\n",
    "        # Load shp as gdf\n",
    "        track_gdf = gpd.read_file(shp_path)\n",
    "\n",
    "        # Calculate the consensus map zonal stats (count only) per class for each trail\n",
    "        zonalstats = zonal_stats(track_gdf, consensus_map, categorical=True, geojson_output=True)\n",
    "        \n",
    "        # Convert list of dictionaries to dataframe\n",
    "        zonalstats_df = pd.DataFrame(zonalstats)\n",
    "\n",
    "        # Force the columns in order (this is not guaranteed!)\n",
    "        zonalstats_df = zonalstats_df[[0, 1, 2, 3, 4, 5, 6]]\n",
    "\n",
    "        # Rename the columns for clarity\n",
    "        zonalstats_df.columns=[\"0_count\", \"1_count\", \"2_count\", \"3_count\",\n",
    "                               \"4_count\", \"5_count\", \"6_count\"]\n",
    "        \n",
    "        # Replace the NaN values with 0\n",
    "        zonalstats_df.fillna(0, inplace=True)\n",
    "\n",
    "        # Join stats with trails (join based on index)\n",
    "        trails_stats = track_gdf.join(zonalstats_df)\n",
    "\n",
    "        # Convert the counts per class to area (based on 25m2 pixel size converted to hectares)\n",
    "        trails_stats[\"0_area_ha\"] = trails_stats[\"0_count\"] * 0.0025\n",
    "        trails_stats[\"1_area_ha\"] = trails_stats[\"1_count\"] * 0.0025\n",
    "        trails_stats[\"2_area_ha\"] = trails_stats[\"2_count\"] * 0.0025\n",
    "        trails_stats[\"3_area_ha\"] = trails_stats[\"3_count\"] * 0.0025\n",
    "        trails_stats[\"4_area_ha\"] = trails_stats[\"4_count\"] * 0.0025\n",
    "        trails_stats[\"5_area_ha\"] = trails_stats[\"5_count\"] * 0.0025\n",
    "        trails_stats[\"6_area_ha\"] = trails_stats[\"6_count\"] * 0.0025\n",
    "\n",
    "        # Calculate the total area of each geometry (convert m2 to ha)\n",
    "        trails_stats[\"total_area_ha\"] = trails_stats.area * 0.0001\n",
    "\n",
    "        # Calculate the percentage coverage\n",
    "        trails_stats[\"0_percent\"] = (trails_stats[\"0_area_ha\"] / trails_stats[\"total_area_ha\"]) * 100\n",
    "        trails_stats[\"1_percent\"] = (trails_stats[\"1_area_ha\"] / trails_stats[\"total_area_ha\"]) * 100\n",
    "        trails_stats[\"2_percent\"] = (trails_stats[\"2_area_ha\"] / trails_stats[\"total_area_ha\"]) * 100\n",
    "        trails_stats[\"3_percent\"] = (trails_stats[\"3_area_ha\"] / trails_stats[\"total_area_ha\"]) * 100\n",
    "        trails_stats[\"4_percent\"] = (trails_stats[\"4_area_ha\"] / trails_stats[\"total_area_ha\"]) * 100\n",
    "        trails_stats[\"5_percent\"] = (trails_stats[\"5_area_ha\"] / trails_stats[\"total_area_ha\"]) * 100\n",
    "        trails_stats[\"6_percent\"] = (trails_stats[\"6_area_ha\"] / trails_stats[\"total_area_ha\"]) * 100\n",
    "        \n",
    "        # Add a column for the class with the highest percentage\n",
    "        trails_stats[\"max_class\"] = trails_stats[[\"0_percent\", \"1_percent\", \"2_percent\", \"3_percent\", \"4_percent\", \"5_percent\", \"6_percent\"]].idxmax(axis=1) \n",
    "\n",
    "        # Remove any trails which have max_class of 0,1 or 2\n",
    "        ind_to_drop = trails_stats[(trails_stats['max_class'] == \"0_percent\") | (trails_stats['max_class'] == \"1_percent\") | (trails_stats['max_class'] == \"2_percent\")].index\n",
    "        trails_stats_filter = trails_stats.drop(ind_to_drop)\n",
    "\n",
    "        # Replace the row values created in the last step to remove the word \"percent\"\n",
    "        trails_stats_filter.replace(\"3_percent\",\"3\", inplace=True)\n",
    "        trails_stats_filter.replace(\"4_percent\",\"4\", inplace=True)\n",
    "        trails_stats_filter.replace(\"5_percent\",\"5\", inplace=True)\n",
    "        trails_stats_filter.replace(\"6_percent\",\"6\", inplace=True)\n",
    "\n",
    "        # Write to shapefile\n",
    "        trails_stats_filter.to_file(output_path, driver=\"ESRI Shapefile\")\n",
    "\n",
    "# Run forest coverage function to find out which class has the most coverage for each trail (max class)\n",
    "# and remove trails which don't have a max class of 3, 4, 5, or 6 (i.e. remove the trails which have a non-forest class as their max class)\n",
    "forest_coverage(track3year_natura_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is only for RQ2 (to generate geometries to associate with the clusters) and so it only runs with the outputs from # STEP 7: NATURA INTERSECT **(ORIGINAL)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: EXPORT ALL GEOMS (ONLY NATURA TRAILS)\n",
    "\n",
    "# Create a list of the shp paths with all the final filtered trails per region\n",
    "filtered_paths = glob.glob('./processing/track-*_3year_distfilter_buffer_areafilt_natura_forest.shp')\n",
    "\n",
    "# Create a list for storing the gdfs (for reading in all the shp paths)\n",
    "gdf_list = []\n",
    "\n",
    "# Read in each shp, read as gdf and add to list\n",
    "for shp_path in filtered_paths:\n",
    "    filtered_track_gdf = gpd.read_file(shp_path)\n",
    "    gdf_list.append(filtered_track_gdf)\n",
    "\n",
    "# Combine all the gdfs to create one shp\n",
    "combined_shp = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True))\n",
    "\n",
    "# Write to shapefile\n",
    "combined_shp.to_file(\"./processing/master_geoms_3year_version.shp\", driver=\"ESRI Shapefile\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Finalise Text for Analysis\n",
    "\n",
    "At this stage I have the original data with the full text and the filtered data for the areas of interest, but without the full text (it was truncated during conversion to shp). At this stage I now need to do the following things to finalise the text that can be used for analysis:\n",
    "\n",
    "1. Link the full text back to the filtered trails. For ease I also drop the geometries at this stage so that outputs can be saved as a json. \n",
    "2. Combine the data from the different regions together.\n",
    "3. Remove any duplicates (trails which cross borders between regions may be listed in both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: REINCORPORATE FULL TEXT\n",
    "\n",
    "# Create a list of the shp paths with all the final filtered trails per region\n",
    "filtered_paths = glob.glob('./processing/track-*_3year_distfilter_buffer_areafilt_natura_forest.shp')\n",
    "\n",
    "# Load shps from list, drop geometries (for ease), load original json with full text and merge\n",
    "def full_text_merge(shp_path_list):\n",
    "    for shp_path in shp_path_list:\n",
    "        # For output file naming: extract the input file name (with extension)\n",
    "        name_w_ext = os.path.split(shp_path)[1] \n",
    "        # For output file naming: remove extension from input file name\n",
    "        name_wo_ext = os.path.splitext(name_w_ext)[0]\n",
    "        # Extract corresponsing file name for original json\n",
    "        original_name = name_wo_ext[:-41]\n",
    "        # For output file naming: assemble the new file path for the output \n",
    "        output_path = \"./processing/\" + original_name + \"_filtered_full_text.csv\"\n",
    "\n",
    "        # Load shp as gdf\n",
    "        filtered_track_gdf = gpd.read_file(shp_path)\n",
    "        # Drop geometries\n",
    "        filtered_track_df = filtered_track_gdf.drop(columns='geometry')\n",
    "\n",
    "\n",
    "        # Load the original json with full text\n",
    "        original_track_df = pd.read_json(\"./processing/\" + original_name + \"_distfilter.json\")\n",
    "\n",
    "        # Left join so that only filtered trails remain\n",
    "        filt_fulltxt = pd.merge(filtered_track_df, original_track_df, how=\"left\", \n",
    "                                    on=[\"url_track\", \"url_track\"], suffixes=[\"_filter\", \"_original\"])\n",
    "\n",
    "        # Clean up the columns\n",
    "        filt_fulltxt = filt_fulltxt.drop(columns=[\"date_publi\", \"descriptio\", \"distance_k\",  \"date_recor\", \n",
    "                                                \"photo_capt\",\"photo_capt\", \"comments_filter\", \n",
    "                                                \"latitudes_filter\", \"longitudes_filter\", \n",
    "                                                \"track_name_original\", \"latitudes_original\", \n",
    "                                                \"longitudes_original\", \"track_type_original\", \"coordinate\"\n",
    "                                                ])\n",
    "        filt_fulltxt.rename(columns={\"track_name_filter\":\"track_name\", \n",
    "                                    \"track_type_filter\":\"track_type\",\n",
    "                                    \"comments_original\":\"comments\"}, inplace=True)\n",
    "        \n",
    "        # Add a column for the region name (useful later)\n",
    "        track_region = name_wo_ext[:-46]\n",
    "        filt_fulltxt[\"region\"] = track_region[6:]\n",
    "\n",
    "        # Save as csv (utf-8-sig encoding seems to work for the special characters)\n",
    "        # Using csv for now as this will be easier some manual checking\n",
    "        filt_fulltxt.to_csv(output_path, index = False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Use the function to create csvs of full text for the filtered trails of each region\n",
    "full_text_merge(filtered_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: COMBINE REGIONAL TRAILS\n",
    "\n",
    "# Load in each region manually\n",
    "baden_wurt = pd.read_csv(\"./processing/track-badenwurttemberg_3year_filtered_full_text.csv\")\n",
    "bavaria = pd.read_csv(\"./processing/track-bavaria_3year_filtered_full_text.csv\")\n",
    "berlin = pd.read_csv(\"./processing/track-berlin_3year_filtered_full_text.csv\")\n",
    "branden = pd.read_csv(\"./processing/track-brandenburg_3year_filtered_full_text.csv\")\n",
    "bremen = pd.read_csv(\"./processing/track-bremen_3year_filtered_full_text.csv\")\n",
    "\n",
    "hamburg = pd.read_csv(\"./processing/track-hamburg_3year_filtered_full_text.csv\")\n",
    "hessen = pd.read_csv(\"./processing/track-hessen_3year_filtered_full_text.csv\")\n",
    "meck_vor = pd.read_csv(\"./processing/track-mecklenburgvorpommern_3year_filtered_full_text.csv\")\n",
    "nieder = pd.read_csv(\"./processing/track-niedersachsen_3year_filtered_full_text.csv\")\n",
    "nord_west = pd.read_csv(\"./processing/track-nordrheinwestfalen_3year_filtered_full_text.csv\")\n",
    "\n",
    "rhein_pfalz = pd.read_csv(\"./processing/track-rheinlandpfalz_3year_filtered_full_text.csv\")\n",
    "saar = pd.read_csv(\"./processing/track-saarland_3year_filtered_full_text.csv\")\n",
    "sachs = pd.read_csv(\"./processing/track-sachsen_3year_filtered_full_text.csv\")\n",
    "sax_anh = pd.read_csv(\"./processing/track-saxonyanhalt_3year_filtered_full_text.csv\")\n",
    "schles_hol = pd.read_csv(\"./processing/track-schleswigholstein_3year_filtered_full_text.csv\")\n",
    "thuri = pd.read_csv(\"./processing/track-thuringen_3year_filtered_full_text.csv\")\n",
    "\n",
    "# Combine regions by adding rows to master\n",
    "master = pd.concat([baden_wurt, bavaria, berlin, branden, bremen,  \n",
    "                    hamburg, hessen, meck_vor, nieder, nord_west,\n",
    "                    rhein_pfalz, saar, sachs, sax_anh, schles_hol, thuri], axis=0)\n",
    "\n",
    "# Remove duplicates (could be that trails which cross regional borders are duplicated?)\n",
    "master.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save master csv\n",
    "master.to_csv(\"./processing/master_3year_version.csv\", index = False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
